


```## Chapter 12 {#part0024_split_000.html#calibre_pb_0 .calibre11}

**System Initialization, Message Logging, and System Tuning**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Understand systemd, units, and
targets

[![](images/00001.jpeg){.c37}]{.c36}Analyze service and target unit
configuration files

[![](images/00001.jpeg){.c37}]{.c36}List and view status of running
units

[![](images/00001.jpeg){.c37}]{.c36}Manage service units and target
units

[![](images/00001.jpeg){.c37}]{.c36}Display and configure default system
boot target

[![](images/00001.jpeg){.c37}]{.c36}Switch into non-default targets

[![](images/00001.jpeg){.c37}]{.c36}Analyze system log configuration
file

[![](images/00001.jpeg){.c37}]{.c36}Examine log file rotation settings

[![](images/00001.jpeg){.c37}]{.c36}Review boot and system log files

[![](images/00001.jpeg){.c37}]{.c36}Record custom messages in system log
file

[![](images/00001.jpeg){.c37}]{.c36}Describe systemd journal service

[![](images/00001.jpeg){.c37}]{.c36}Retrieve and scrutinize messages
from journal

[![](images/00001.jpeg){.c37}]{.c36}Store journal information
persistently

[![](images/00001.jpeg){.c37}]{.c36}Know system tuning and apply tuning
profile

[RHCSA Objectives:]{.c39}

[17]{#part0024_split_000.html#id_783 .calibre10}. Boot, reboot, and shut
down a system normally

[18]{#part0024_split_000.html#id_784 .calibre10}. Boot systems into
different targets manually

[25]{#part0024_split_000.html#id_791 .calibre10}. Start, stop, and check
the status of network services

[41]{#part0024_split_000.html#id_807 .calibre10}. Start and stop
services and configure services to start automatically at boot

[42]{#part0024_split_000.html#id_808 .calibre10}. Configure systems to
boot into a specific target automatically

[49]{#part0024_split_000.html#id_814 .calibre10}. Configure network
services to start automatically at boot

[22]{#part0024_split_000.html#id_788 .calibre10}. Manage tuning profiles

[23]{#part0024_split_000.html#id_789 .calibre10}. Locate and interpret
system log files and journals

[24]{#part0024_split_000.html#id_790 .calibre10}. Preserve system
journals

::: {#part0024_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0024_split_001.html}

[ S]{.c42}[ystemd]{.c43} is the default system initialization and
service management scheme. It boots the system into one of several
predefined targets and it is used to handle operational states of
services. It employs the concepts of units and targets for
initialization, service administration, and state changes. A good grasp
of what unit and target configuration files store is key to
understanding how systemd operates.

Alerts and messages generated by system services and user activities are
forwarded to predefined locations for storage. These alerts and messages
include those that are produced during system boot time. The log data
may be analyzed for debugging or auditing purposes. Log files grow over
time and need to be rotated periodically to prevent the file system
space from filling up. There are configuration files that define the
default and custom locations to direct the log messages to and to
configure rotation settings. The system log file records custom messages
sent to it. systemd includes a service for viewing and managing system
logs in addition to the traditional logging service. This service
maintains a log of runtime activities for faster retrieval and can be
configured to store the information permanently.

System tuning service is employed to monitor connected devices and to
tweak their parameters to improve performance or conserve power. A
recommended tuning profile may be identified and activated for optimal
performance and power saving.

**[System]{#part0024_split_001.html#id_329 .calibre10} Initialization
and Service Management**

*systemd* (short for *system daemon*) is the system initialization and
service management mechanism. It has fast-tracked system initialization
and state transitioning by introducing features such as parallel
processing of startup scripts, improved handling of service
dependencies, and on-demand activation of services. Moreover, it
supports snapshotting of system states, tracks processes using control
groups, and automatically maintains mount points. systemd is the first
process with PID 1 that spawns at boot and it is the last process that
terminates at shutdown.

![](images/00002.jpeg){.image} systemd spawns several processes during a
service startup. It places the processes in a private hierarchy composed
of *control groups* (or *cgroups* for short) to organize processes for
the purposes of monitoring and controlling system resources such as
processor, memory, network bandwidth, and disk I/O. This includes
limiting, isolating, and prioritizing their usage of resources. This way
resources can be distributed among users, databases, and applications
based on need and priority, resulting in overall improved system
performance.

In order to benefit from parallelism, systemd initiates distinct
services concurrently, taking advantage of multiple CPU cores and other
compute resources. To that end, systemd creates sockets for all enabled
services that support socket-based activation instantaneously at the
very beginning of the initialization process. It passes them on to
service daemon processes as they attempt to start in parallel. This
approach lets systemd handle inter-service order dependencies and allows
services to start without any delays. With systemd, dependent daemons
need not be running; they only need the correct socket to be available.
systemd creates sockets first, starts daemons next, and caches any
client requests to daemons that have not yet started in the socket
buffer. It fills the pending client requests when the daemons they were
awaiting come online.

![](images/00002.jpeg){.image} Socket is a communication method that
allows a single process to talk to another process on the same or remote
system.

During the operational state, systemd maintains the sockets and uses
them to reconnect other daemons and services that were interacting with
an old instance of a daemon before that daemon was terminated or
restarted. Likewise, services that use activation based on D-Bus
(*Desktop Bus*) are started when a client application attempts to
communicate with them for the first time. Additional methods used by
systemd for activation are device-based and path-based, with the former
starting the service when a specific hardware type such as USB is
plugged in, and the latter starting the service when a particular file
or directory alters its state.

![](images/00002.jpeg){.image} D-Bus is another communication method
that allows multiple services running in parallel on a system to talk to
one another on the same or remote system.

With on-demand activation, systemd defers the startup of
services---Bluetooth and printing---until they are actually needed
during the boot process or during runtime. Together, parallelization and
on-demand activation save time and compute resources, and contribute to
expediting the boot process considerably.

Another major benefit of parallelism witnessed at system boot is when
systemd uses the autofs service to temporarily mount the configured file
systems. During the boot process, the file systems are checked that may
result in unnecessary delays. With autofs, the file systems are
temporarily mounted on their normal mount points, and as soon as the
checks on the file systems are finished, systemd remounts them using
their standard devices. Parallelism in file system mounts does not
affect the root and virtual file systems.

**[Units]{#part0024_split_001.html#id_330 .calibre10}**

*Units* are systemd objects used for organizing boot and maintenance
tasks, such as hardware initialization, socket creation, file system
mounts, and service startups. Unit configuration is stored in their
respective configuration files, which are auto-generated from other
configurations, created dynamically from the system state, produced at
runtime, or user-developed. Units are in one of several operational
states, including active, inactive, in the process of being activated or
deactivated, and failed. Units can be enabled or disabled. An enabled
unit can be started to an active state; a disabled unit cannot be
started.

Units have a name and a type, and they are encoded in files with names
in the form unitname.type. Some examples are tmp.mount, sshd.service,
syslog.socket, and umount.target. There are two types of unit
configuration files: (1) system unit files that are distributed with
installed packages and located in the **usr*lib/systemd/system*
directory, and (2) user unit files that are user-defined and stored in
the **etc*systemd/user* directory (run **ls -l** on both directories to
list their contents). This information can be vetted with the
*pkg-config* command:

![](images/00587.jpeg){.image2}

Furthermore, there are additional system units that are created at
runtime and destroyed when they are no longer needed. They are located
in the **run*systemd/system* directory. These runtime unit files take
precedence over the system unit files, and the user unit files take
priority over the runtime files.

![](images/00002.jpeg){.image} Unit configuration files are a direct
replacement of the initialization scripts found in the
**etc*rc.d/init.d* directory in older RHEL releases.

systemd includes 11 unit types, which are described in [Table
12-1](#part0024_split_001.html#id_734){.calibre5}.

::: c49
  --------------- ------------------------------------------------------------------------------------------------------------
  **Unit Type**   **Description**
  Automount       Offers automount capabilities for on-demand mounting of file systems
  Device          Exposes kernel devices in systemd and may be used to implement device-based activation
  Mount           Controls when and how to mount or unmount file systems
  Path            Activates a service when monitored files or directories are accessed
  Scope           Manages foreign processes instead of starting them
  Service         Starts, stops, restarts, or reloads service daemons and the processes they are made up of
  Slice           May be used to group units, which manage system processes in a tree-like structure for resource management
  Socket          Encapsulates local inter-process communication or network sockets for use by matching service units
  Swap            Encapsulates swap partitions
  Target          Defines logical grouping of units
  Timer           Useful for triggering activation of other units based on timers
  --------------- ------------------------------------------------------------------------------------------------------------

**[Table]{#part0024_split_001.html#id_734} 12-1 systemd Unit Types**
:::

Unit files contain common and specific configuration elements. Common
elements fall under the \[Unit\] and \[Install\] sections, and comprise
the description, documentation location, dependency information,
conflict information, and other options that are independent of the type
of unit. The unit-specific configuration data is located under the unit
type section: \[Service\] for the service unit type, \[Socket\] for the
socket unit type, and so forth. A sample unit file for *sshd.service* is
shown below from the **usr*lib/systemd/system* directory:

![](images/00588.jpeg){.image2}

Units can have dependency relationships among themselves based on a
sequence (ordering) or a requirement. A sequence outlines one or more
actions that need to be taken before or after the activation of a unit
(the Before and After directives). A requirement specifies what must
already be running (the Requires directive) or not running (the
Conflicts directive) in order for the successful launch of a unit. For
instance, the *graphical.target* unit file tells us that the system must
already be operating in the multi-user mode and not in rescue mode in
order for it to boot successfully into the graphical mode. Another
option, Wants, may be used instead of Requires in the \[Unit\] or
\[Install\] section so that the unit is not forced to fail activation if
a required unit fails to start. Run **man systemd.unit** for details on
systemd unit files.

There are a few other types of dependencies that you may see in other
unit configuration files. systemd generally sets and maintains
inter-service dependencies automatically; however, this can be done
manually if needed.

**[Targets]{#part0024_split_001.html#id_331 .calibre10}**

*Targets* are simply logical collections of units. They are a special
systemd unit type with the .target file extension. They share the
directory locations with other unit configuration files. Targets are
used to execute a series of units. This is typically true for booting
the system to a desired operational run level with all the required
services up and running. Some targets inherit services from other
targets and add their own to them. systemd includes several predefined
targets that are described in [Table
12-2](#part0024_split_001.html#id_735){.calibre5}.

::: c49
  ------------ ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Target**   **Description**
  halt         Shuts down and halts the system
  poweroff     Shuts down and powers off the system
  shutdown     Shuts down the system
  rescue       Single-user target for running administrative and recovery functions. All local file systems are mounted. Some essential services are started, but networking remains disabled.
  emergency    Runs an emergency shell. The root file system is mounted in read-only mode; other file systems are not mounted. Networking and other services remain disabled.
  multi-user   Multi-user target with full network support, but without GUI
  graphical    Multi-user target with full network support and GUI
  reboot       Shuts down and reboots the system
  default      A special soft link that points to the default system boot target (multi-user.target or graphical.target)
  hibernate    Puts the system into hibernation by saving the running state of the system on the hard disk and powering it off. When powered up, the system restores from its saved state rather than booting up.
  ------------ ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0024_split_001.html#id_735} 12-2 systemd Targets**
:::

Target unit files contain all information under the \[Unit\] section,
and it comprises the description, documentation location, and dependency
and conflict information. A sample file for the *graphical.target*
target is shown below from the **usr*lib/systemd/system* directory:

![](images/00589.jpeg){.image2}

The file shows four dependencies: Requires, Wants, Conflicts, and After.
It suggests that the system must have already accomplished the
rescue.service, rescue.target, multi-user.target, and
display-manager.service levels in order to be declared running in the
graphical target. Run **man systemd.target** for details on systemd
targets.

**[The]{#part0024_split_001.html#id_332 .calibre10} systemctl Command**

systemd comes with a set of management tools for querying and
controlling operations. The primary tool for interaction in this command
suite is *systemctl*. The *systemctl* command performs administrative
functions and supports plentiful subcommands and flags. [Table
12-3](#part0024_split_001.html#id_736){.calibre5} lists and describes
some common operations.

::: c49
  ----------------------------- -------------------------------------------------------------------------------------------------------------------
  **Subcommand**                **Description**
  daemon-reload                 Re-reads and reloads all unit configuration files and recreates the entire user dependency tree.
  enable (disable)              Activates (deactivates) a unit for autostart at system boot
  get-default (set-default)     Shows (sets) the default boot target
  get-property (set-property)   Returns (sets) the value of a property
  is-active                     Checks whether a unit is running
  is-enabled                    Displays whether a unit is set to autostart at system boot
  is-failed                     Checks whether a unit is in the failed state
  isolate                       Changes the running state of a system
  kill                          Terminates all processes for a unit
  list-dependencies             Lists dependency tree for a unit
  list-sockets                  Lists units of type socket
  list-unit-files               Lists installed unit files
  list-units                    Lists known units. This is the default behavior when systemctl is executed without any arguments.
  mask (unmask)                 Prohibits (permits) auto and manual activation of a unit to avoid potential conflict
  reload                        Forces a running unit to re-read its configuration file. This action does not change the PID of the running unit.
  restart                       Stops a running unit and restarts it
  show                          Shows unit properties
  start (stop)                  Starts (stops) a unit
  status                        Presents the unit status information
  ----------------------------- -------------------------------------------------------------------------------------------------------------------

**[Table]{#part0024_split_001.html#id_736} 12-3 systemctl Subcommands**
:::

You will use a majority of these subcommands with *systemctl* going
forward. Refer to the manual pages of the command for more details.

**[Listing]{#part0024_split_001.html#id_333 .calibre10} and Viewing
Units**

The *systemctl* command is used to view and manage all types of units.
The following examples demonstrate common operations pertaining to
viewing and querying units.

To list all units that are currently loaded in memory along with their
status and description, run the *systemctl* command without any options
or subcommands. A long output is generated. The graphic below shows a
few starting and concluding lines followed by a brief explanation.

![](images/00590.jpeg){.image2}

Here is a breakdown of the graphic above: the UNIT column shows the name
of the unit and its location in the tree, the LOAD column reflects
whether the unit configuration file was properly loaded (other
possibilities are not found, bad setting, error, and masked), the ACTIVE
column returns the high-level activation state (other possible states
are active, reloading, inactive, failed, activating, and deactivating),
the SUB column depicts the low-level unit activation state (reports
unit-specific information), and the DESCRIPTION column illustrates the
unit's content and functionality.

By default, the *systemctl* command lists only the active units. You can
use the \--all option to include the inactive units too.

To list all (\--all) active and inactive units of type (-t) socket:

![](images/00591.jpeg){.image2}

To list all units of type socket (column 2) currently loaded in memory
and the service they activate (column 3), sorted by the listening
address (column 1):

![](images/00592.jpeg){.image2}

To list all unit files (column 1) installed on the system and their
current state (column 2). This will generate a long list of units in the
output. The following only shows a selection.

![](images/00593.jpeg){.image2}

To list all units that failed (\--failed) to start at the last system
boot:

![](images/00594.jpeg){.image2}

To list the hierarchy of all dependencies (required and wanted units)
for the current default target:

![](images/00595.jpeg){.image2}

To list the hierarchy of all dependencies (required and wanted units)
for a specific unit such as *atd.service*:

![](images/00596.jpeg){.image2}

There are other listing subcommands and additional flags available that
can be used to produce a variety of reports.

**[Managing]{#part0024_split_001.html#id_334 .calibre10} Service Units**

The *systemctl* command offers several subcommands to manage service
units, including starting, stopping, restarting, and checking their
status. These and other management operations are summarized in [Table
12-3](#part0024_split_001.html#id_736){.calibre5}. The following
examples demonstrate their use on a service unit called *atd*.

To check the current operational status and other details for the *atd*
service:

![](images/00597.jpeg){.image2}

The above output reveals a lot of information about the *atd* service.
On line 1, it shows the service description (read from the
**usr*lib/systemd/system/atd.service* file). Line 2 illustrates the load
status, which reveals the current load status of the unit configuration
file in memory. Other possibilities for "Loaded" include "error" (if
there was a problem loading the file), \"not-found\" (if no file
associated with this unit was found), \"bad-setting\" (if a key setting
was missing), and \"masked\" (if the unit configuration file is masked).
Line 2 also tells us whether the service is set (enable or disable) for
autostart at system boot.

Line 3 exhibits the current activation status and the time the service
was started. An activation status designates the current state of the
service. Possible states include:

**Active (running)**: The service is running with one or more processes

**Active (exited):** Completed a one-time configuration

**Active (waiting)**: Running but waiting for an event

**Inactive:** Not running

**Activating:** In the process of being activated

**Deactivating**: In the process of being deactivated

**Failed:** If the service crashed or could not be started

The output also depicts the PID of the service process and other
information.

To disable the *atd* service from autostarting at the next system
reboot:

![](images/00598.jpeg){.image2}

To re-enable *atd* to autostart at the next system reboot:

![](images/00599.jpeg){.image2}

To check whether *atd* is set to autostart at the next system reboot:

![](images/00600.jpeg){.image2}

To check whether the *atd* service is running:

![](images/00601.jpeg){.image2}

To stop and restart *atd*, run either of the following:

![](images/00602.jpeg){.image2}

To show the details of the *atd* service:

![](images/00603.jpeg){.image2}

To prohibit *atd* from being enabled or disabled:

![](images/00604.jpeg){.image2}

Try disabling or enabling *atd* and observe the effect of the previous
command:

![](images/00605.jpeg){.image2}

Reverse the effect of the *mask* subcommand and try disable and enable
operations:

![](images/00606.jpeg){.image2}

Notice that the *unmask* subcommand has removed the restriction that was
placed on the *atd* service.

**[Managing]{#part0024_split_001.html#id_335 .calibre10} Target Units**

The *systemctl* command is also used to manage the target units. It can
be used to view or change the default boot target, switch from one
running target into another, and so on. These operations are briefed in
[Table 12-3](#part0024_split_001.html#id_736){.calibre5}. Let's look at
some examples.

To view what units of type (-t) target are currently loaded and active:

![](images/00607.jpeg){.image2}

For each target unit, the above output returns the target unit's name,
load state, high-level and low-level activation states, and a short
description. Add the \--all option to the above to see all loaded
targets in either active or inactive state.

**Viewing and Setting Default Boot Target**

The *systemctl* command is used to view the current default boot target
and to set it. Let's use the *get-default* and *set-default* subcommands
with *systemctl* to perform these operations.

To check the current default boot target:

![](images/00608.jpeg){.image2}

[**EXAM TIP:**]{.c56} You may have to modify the default boot target
persistently.

To change the current default boot target from graphical.target to
multi-user.target:

![](images/00609.jpeg){.image2}

The command simply removes the existing symlink (*default.target*)
pointing to the old boot target and replaces it with the new target file
path.

Execute **sudo systemctl set-default graphical** to revert the default
boot target to graphical.

**Switching into Specific Targets**

The *systemctl* command can be used to transition the running system
from one target state into another. There are a variety of potential
targets available to switch into as listed in [Table
12-2](#part0024_split_001.html#id_735){.calibre5}; however, only a few
of them---graphical, multi-user, reboot, shutdown---are typically used.
The rescue and emergency targets are for troubleshooting and system
recovery purposes, poweroff and halt are similar to shutdown, and
hibernate is suitable for mobile devices. Consider the following
examples that demonstrate switching targets.

The current default target on *server1* is graphical. To switch into
multi-user, use the *isolate* subcommand with *systemctl*:

![](images/00610.jpeg){.image2}

This should stop the graphical service on the system and display the
text-based console login screen, as shown below:

![](images/00611.jpeg){.image2}

Type in a username such as *user1* and enter the password to log in:

![](images/00612.jpeg){.image2}

To return to the graphical target:

![](images/00613.jpeg){.image2}

The graphical login screen should appear shortly and you should be able
to log back in.

To shut down the system and power it off, use the following or simply
run the *poweroff* command:

![](images/00614.jpeg){.image2}

To shut down and reboot the system, use the following or simply run the
*reboot* command:

![](images/00615.jpeg){.image2}

The *halt*, *poweroff*, and *reboot* commands are mere symbolic links to
the *systemctl* command, as the following long listing suggests:

![](images/00616.jpeg){.image2}

The *halt*, *poweroff*, and *reboot* commands are available in RHEL 8
for compatibility reasons only. It is recommended to use the *systemctl*
command instead when switching system states.

The three commands, without any arguments, perform the same action that
the *shutdown* command would with the "-H now", "-P now", and "-r now"
arguments, respectively. In addition, it also broadcasts a warning
message to all logged-in users, blocks new user login attempts, waits
for the specified amount of time for users to save their work and log
off, stops the services, and eventually shut the system down to the
specified target state.

**[System]{#part0024_split_001.html#id_336 .calibre10} Logging**

*System logging* (*syslog* for short) is one of the most rudimentary
elements of an operating system. Its purpose is to capture messages
generated by the kernel, daemons, commands, user activities,
applications, and other events, and forwarded them to various log files,
which store them for security auditing, service malfunctioning, system
troubleshooting, or informational purposes.

The daemon that is responsible for system logging is called *rsyslogd*
(*rocket-fast system for log processing*). This service daemon is
multi-threaded, with support for enhanced filtering,
encryption-protected message relaying, and a variety of configuration
options. The *rsyslogd* daemon reads its configuration file
**etc*rsyslog.conf* and the configuration files located in the
**etc*rsyslog.d* directory at startup. The default depository for most
system log files is the **var*log* directory. Other services such as
audit, Apache, and GNOME desktop manager have subdirectories under
**var*log* for storing their respective log files.

The *rsyslog* service is modular, allowing the modules listed in its
configuration file to be dynamically loaded in the kernel as and when
needed. Each module brings a new functionality to the system upon
loading.

The *rsyslogd* daemon can be stopped manually using **systemctl stop
rsyslog**. Replace stop with start, restart, reload, and status as
appropriate.

A PID is assigned to the daemon at startup and a file by the name
*rsyslogd.pid* is created in the */run* directory to save the PID. The
reason this file is created and stores the PID is to prevent the
initiation of multiple instances of this daemon.

**[The]{#part0024_split_001.html#id_337 .calibre10} Syslog Configuration
File**

The *rsyslog.conf* is the primary syslog configuration file located in
the */etc* directory . The default uncommented line entries from the
file are shown below and explained thereafter. Section headings have
been added to separate the directives in each section.

![](images/00617.jpeg){.image2}

As depicted, the syslog configuration file contains three sections:
Modules, Global Directives, and Rules. The Modules section defines two
modules---*imuxsock* and *imjournal*---and they are loaded on demand.
The *imuxsock* module furnishes support for local system logging via the
*logger* command, and the *imjournal* module allows access to the
systemd journal.

The Global Directives section contains three active directives. The
definitions in this section influence the overall functionality of the
*rsyslog* service. The first directive sets the location for the storage
of auxiliary files (**var*lib/rsyslog*). The second directive instructs
the *rsyslog* service to save captured messages using traditional file
formatting. The third directive directs the service to load additional
configuration from files located in the **etc*rsyslogd.d/* directory.

The Rules section has many two-field line entries. The left field is
called *selector*, and the right field is referred to as *action*. The
selector field is further divided into two period-separated sub-fields
called *facility* (left) and *priority* (right), with the former
representing one or more system process categories that generate
messages, and the latter identifying the severity associated with the
messages. The semicolon (;) character is used as a distinction mark if
multiple facility.priority groups are present. The action field
determines the destination to send the messages to.

There are numerous supported facilities such as auth, authpriv, cron,
daemon, kern, lpr, mail, news, syslog, user, uucp, and local0 through
local7. The asterisk (\*) character represents all of them.

Similarly, there are several supported priorities, and they include
emerg, alert, crit, error, warning, notice, info, debug, and none. This
sequence is in the descending criticality order. The asterisk (\*)
represents all of them. If a lower priority is selected, the daemon logs
all messages of the service at that and higher levels.

Line 1 under the Rules section instructs the syslog daemon to catch and
store informational messages from all services to the
**var*log/messages* file and ignore all messages generated by mail,
authentication, and cron services. Lines 2, 3, and 4 command the daemon
to collect and log all messages produced by authentication, mail, and
cron to the *secure*, *maillog*, and *cron* files, respectively. Line 5
orders the daemon to display emergency messages (omusrmsg stands for
*user message output module*) on the terminals of all logged-in users.
Line 6 shows two comma-separated facilities that are set at a common
priority. These facilities tell the daemon to gather critical messages
from uucp and news facilities and log them to the **var*log/spooler*
file. Line 7 (the last line) is for recording the boot-time service
startup status to the **var*log/boot.log* file.

If you have made any modifications to the syslog configuration file, you
need to run the *rsyslogd* command with the -N switch and specify a
numeric verbosity level to inspect the file for any syntax or typing
issues:

![](images/00618.jpeg){.image2}

The validation returns the version of the command, verbosity level used,
and the configuration file path. With no issues reported, the *rsyslog*
service can be restarted (or reloaded) in order for the changes to take
effect.

**[Rotating]{#part0024_split_001.html#id_338 .calibre10} Log Files**

RHEL records all system activities in log files that are stored in a
central location under the **var*log* directory, as defined in the
rsyslog configuration file. A long listing of this directory reveals the
files along with subdirectories that may have multiple service-specific
logs. Here is a listing from *server1*:

![](images/00619.jpeg){.image2}

The output shows log files for various services. Depending on the usage
and the number of events generated and captured, log files may quickly
fill up the */var* file system, resulting in unpredictable system
behavior. Also, they may grow to an extent that would make it difficult
to load, read, send, or analyze them. To avoid getting into any unwanted
situation, it's important to ensure that they're rotated on a regular
basis and their archives are removed automatically.

To that end, a script called *logrotate* under **etc*cron.daily/*
invokes the *logrotate* command on a daily basis. Via the Anacron
service, the command runs a rotation as per the schedule defined in the
**etc*logrotate.conf* file and the configuration files for various
services located in the **etc*logrotate.d* directory. The configuration
files may be modified to alter the schedule or include additional tasks
such as removing, compressing, and emailing selected log files.

Here is what the **etc*cron.daily/logrotate* script contains:

![](images/00620.jpeg){.image2}

The following returns the default content of the **etc*logrotate.conf*
file:

![](images/00621.jpeg){.image2}

The file content shows the default log rotation frequency (weekly). It
indicates the period of time (4 weeks) to retain the rotated logs before
deleting them. Each time a log file is rotated, an empty replacement
file is created with the date as a suffix to its name, and the *rsyslog*
service is restarted. The script presents the option of compressing the
rotated files using the *gzip* utility. During the script execution, the
*logrotate* command checks for the presence of additional log
configuration files in the **etc*logrotate.d* directory and includes
them as necessary. The directives defined in the *logrotate.conf* file
have a global effect on all log files. You can define custom settings
for a specific log file in *logrotate.conf* or create a separate file in
the **etc*logrotate.d* directory. Any settings defined in user-defined
files overrides the global settings.

The **etc*logrotate.d* directory includes additional configuration files
for other service logs, as shown below:

![](images/00622.jpeg){.image2}

Here there are log configuration files for a number of
services---*chrony*, *cups*, *dnf*, and *samba*---all with their own
rules defined. The following shows the file content for *btmp* (records
of failed user login attempts) that is used to control the rotation
behavior for the **var*log/btmp* file:

![](images/00623.jpeg){.image2}

The rotation is once a month. The replacement file created will get
read/write permission bits for the owner (*root*), the owning group will
be set to *utmp*, and the *rsyslog* service will maintain one rotated
copy of the *btmp* log file.

**[The]{#part0024_split_001.html#id_339 .calibre10} Boot Log File**

Logs generated during the system startup display the service startup
sequence with a status showing whether the service was started
successfully. This information may help in any post-boot troubleshooting
if required. Boot logs are stored in the *boot.log* file under
**var*log*. Here are a few lines from the beginning of the file:

![](images/00624.jpeg){.image2}

OK or FAILED within the square brackets indicates if the service was
started successfully or not.

**[The]{#part0024_split_001.html#id_340 .calibre10} System Log File**

The default location for storing most system activities, as defined in
the *rsyslog.conf* file, is the **var*log/messages* file. This file
saves log information in plain text format and may be viewed with any
file display utility, such as *cat*, *more*, *pg*, *less*, *head*, or
*tail*. This file may be observed in real time using the *tail* command
with the -f switch. The *messages* file captures the date and time of
the activity, hostname of the system, name and PID of the service, and a
short description of the event being logged.

[**EXAM TIP:**]{.c56} It is helpful to "tail" the messages file when
starting or restarting a system service or during testing to identify
any issues encountered.

The following illustrates some recent entries from this file:

![](images/00625.jpeg){.image2}

Each line entry represents the detail for one event.

**[Logging]{#part0024_split_001.html#id_341 .calibre10} Custom
Messages**

Many times it is worthwhile to add a manual note to the system log file
to mark the start or end of an activity for future reference. This is
especially important when you run a script to carry out certain tasks
and you want to record the status or add comments at various stages
throughout its execution. This is also beneficial in debugging the
startup of an application to know where exactly it is failing.

The Modules section in the *rsyslog.conf* file provides the support via
the imuxsock module to record custom messages to the *messages* file
using the *logger* command. This command may be run by normal users or
the *root* user. The following example shows how to add a note
indicating the calling user has rebooted the system:

![](images/00626.jpeg){.image2}

*tail* the last line from the *messages* file and you'll observe the
message recorded along with the timestamp, hostname, and PID:

![](images/00627.jpeg){.image2}

You may add the -p option and specify a priority level either as a
numerical value or in the facility.priority format. The default priority
at which the events are recorded is user.notice. See the manual pages
for the *logger* command for more details.

**[The]{#part0024_split_001.html#id_342 .calibre10} systemd Journal**

In addition to the *rsyslog* service, RHEL offers a systemd-based
logging service for the collection and storage of logging data. This
service is implemented via the *systemd-journald* daemon. The function
of this service is to gather, store, and display logging events from a
variety of sources such as the kernel, *rsyslog* and other services,
initial RAM disk, and alerts generated during the early boot stage. It
stores these messages in the binary format in files called *journals*
that are located in the **run*log/journal* directory. These files are
structured and indexed for faster and easier searches, and may be viewed
and managed using the *journalctl* command. As you know, */run* is a
virtual file system that is created in memory at system boot, maintained
during system runtime, and destroyed at shutdown. Therefore, the data
stored therein is non-persistent, but you can enable persistent storage
for the logs if desired.

RHEL runs both *rsyslogd* and *systemd-journald* concurrently. In fact,
the data gathered by *systemd-journald* may be forwarded to *rsyslogd*
for further processing and persistent storage in text format.

The main configuration file for this service is
**etc*systemd/journald.conf*, which contains numerous default settings
that affect the overall functionality of the service. These settings may
be modified as required.

**[Retrieving]{#part0024_split_001.html#id_343 .calibre10} and Viewing
Messages**

RHEL provides the *journalctl* command to retrieve messages from the
journal for viewing in a variety of ways using different options. One
common usage is to run the command without any options to see all the
messages generated since the last system reboot. The following shows a
few initial entries from the journal:

![](images/00628.jpeg){.image2}

Notice that the format of the messages is similar to that of the events
logged to the **var*log/messages* file that you saw earlier. Each line
begins with a timestamp followed by the system hostname, process name
with or without a PID, and the actual message.

Let's run the *journalctl* command with different options to produce
various reports.

To display detailed output for each entry, use the -o verbose option:

![](images/00629.jpeg){.image2}

To view all events since the last system reboot, use the -b options:

![](images/00630.jpeg){.image2}

You may specify -0 (default, since the last system reboot), -1 (the
previous system reboot), -2 (two reboots before), and so on to view
messages from previous system reboots.

To view only kernel-generated alerts since the last system reboot:

![](images/00631.jpeg){.image2}

To limit the output to view a specific number of entries only (3 in the
example below), use the -n option:

![](images/00632.jpeg){.image2}

To show all alerts generated by a particular service, such as *crond*:

![](images/00633.jpeg){.image2}

To retrieve all messages logged for a certain process, such as the PID
associated with the *chronyd* service:

![](images/00634.jpeg){.image2}

To reveal all messages for a particular system unit, such as
sshd.service:

![](images/00635.jpeg){.image2}

To view all error messages logged between a date range, such as October
10, 2019 and October 16, 2019:

![](images/00636.jpeg){.image2}

To get all warning messages that have appeared today and display them in
reverse chronological order:

![](images/00637.jpeg){.image2}

You can specify the time range in hh:mm:ss format, or yesterday, today,
or tomorrow instead.

Similar to the -f (follow) option that is used with the *tail* command
for real-time viewing of a log file, you can use the same switch with
*journalctl* as well. Press Ctrl+c to terminate.

![](images/00638.jpeg){.image2}

Check the manual pages of the *journalctl* command and the
*systemd-journald* service for more details.

**[Preserving]{#part0024_split_001.html#id_344 .calibre10} Journal
Information**

By default, journals are stored in the **run*log/journal* directory for
the duration of system runtime. This data is transient and it does not
survive across reboots. The *journalctl* command examples demonstrated
earlier retrieve journal information from this temporary location. The
*rsyslogd* daemon, by default, reads the temporary journals and stores
messages in the **var*log/messages* file. You can enable a separate
storage location for the journal to save all its messages there
persistently. The default is under the **var*log/journal* directory.
This will make the journal information available for future reference.

The *systemd-journald* service supports four options with the Storage
directive in its configuration file *journald.conf* to control how the
logging data is handled. These options are described in [Table
12-4](#part0024_split_001.html#id_737){.calibre5}.

::: c49
  ------------ -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Option**   **Description**
  volatile     Stores data in memory only
  persistent   Stores data permanently under *var*log/journal and falls back to memory-only option if this directory does not exist or has a permission or other issue. The service creates *var*log/journal in case of its non-existence.
  auto         Similar to "persistent" but does not create *var*log/journal if it does not exist. This is the default option.
  none         Disables both volatile and persistent storage options. Not recommended.
  ------------ -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0024_split_001.html#id_737} 12-4 Journal Data Storage
Options**
:::

The default (auto) option appears more suitable as it stores data in
both volatile and on-disk storage; however, you need to create the
**var*log/journal* directory manually. This option provides two
fundamental benefits: faster query responses from in-memory storage and
access to historical log data from on-disk storage.

**Exercise 12-1: Configure Persistent Storage for Journal Information**

This exercise should be done on *server1* as *user1* with *sudo* where
required.

In this exercise, you will run the necessary steps to enable and confirm
persistent storage for the journals.

1[.]{.c19}Create a subdirectory called *journal* under the **var*log*
directory and confirm:

![](images/00639.jpeg){.image2}

2[.]{.c19}Restart the *systemd-journald* service and confirm:

![](images/00640.jpeg){.image2}

3[.]{.c19}List the new directory and observe a subdirectory matching the
machine ID of the system as defined in the **etc*machine-id* file is
created:

![](images/00641.jpeg){.image2}

Compare the name of the subdirectory with the ID stored in the
**etc*machine-id* file. They are identical.

![](images/00002.jpeg){.image} This log file is rotated automatically
once a month based on the settings in the *journald.conf* file. Check
the manual pages of the configuration file for details and relevant
directives.

This concludes the exercise.

**[System]{#part0024_split_001.html#id_345 .calibre10} Tuning**

RHEL uses a system tuning service called *tuned* to monitor storage,
networking, processor, audio, video, and a variety of other connected
devices, and adjusts their parameters for better performance or power
saving based on a chosen profile. There are several predefined tuning
profiles for common use cases shipped with RHEL that may be activated
either statically or dynamically.

The *tuned* service activates a selected profile at service startup and
continues to use it until it is switched to a different profile. This is
the static behavior and it is enabled by default.

The dynamic alternative adjusts the system settings based on the live
activity data received from monitored system components to ensure
optimal performance. In most cases, the utilization of system components
vary throughout the day. For example, a disk and processor's utilization
increases during a program startup and the network connection use goes
up during a large file transfer. A surge in a system component activity
results in heightened power consumption.

**[Tuning]{#part0024_split_001.html#id_346 .calibre10} Profiles**

*tuned* includes nine predefined profiles to support a variety of use
cases. In addition, you can create custom profiles from nothing or by
using one of the existing profiles as a template. In either case, you
need to store the custom profile under the **etc*tuned* directory in
order to be recognized by the *tuned* service.

Tuning profiles may be separated into three groups: (1) optimized for
better performance, (2) geared more towards power consumption, and (3)
that offers a balance between the other two and the maximum
performance/power combination. [Table
12-5](#part0024_split_001.html#id_738){.calibre5} lists and describes
these profiles.

::: c49
  ----------------------------------------------- -------------------------------------------------------------------------------------------------------------
  **Profile**                                     **Description**
  **Profiles Optimized for Better Performance**   
  Desktop                                         Based on the balanced profile for desktop systems. Offers improved throughput for interactive applications.
  Latency-performance                             For low-latency requirements
  Network-latency                                 Based on the latency-performance for faster network throughput
  Network-throughput                              Based on the throughput-performance profile for maximum network throughput
  Virtual-guest                                   Optimized for virtual machines
  Virtual-host                                    Optimized for virtualized hosts
  **Profiles Optimized for Power Saving**         
  Powersave                                       Saves maximum power at the cost of performance
  **Balanced/Max Profiles**                       
  Balanced                                        Preferred choice for systems that require a balance between performance and power saving
  Throughput-performance                          Provides maximum performance and consumes maximum power
  ----------------------------------------------- -------------------------------------------------------------------------------------------------------------

**[Table]{#part0024_split_001.html#id_738} 12-5 Tuning Profiles**
:::

Predefined profiles are located in the **usr*lib/tuned* directory in
subdirectories matching their names. The following shows a long listing
of the directory:

![](images/00642.jpeg){.image2}

The default active profile set on *server1* and *server2* is the
*virtual-guest* profile, as the two systems are hosted in a VirtualBox
virtualized environment.

**[The]{#part0024_split_001.html#id_347 .calibre10} tuned-adm Command**

*tuned* comes with a single profile management command called
*tuned-adm*. This tool can list active and available profiles, query
current settings, switch between profiles, and turn the tuning off. This
command can also recommend the best profile for the system based on many
system attributes. Refer to the manual pages of the command for more
details.

The following exercise demonstrates the use of most of the management
operations listed above.

**[Exercise]{#part0024_split_001.html#id_348 .calibre10} 12-2: Manage
Tuning Profiles**

This exercise should be done on *server1* as *user1* with *sudo* where
required.

In this exercise, you will install the *tuned* service, start it now,
and enable it for auto-restart upon future system reboots. You will
display all available profiles and the current active profile. You will
switch to one of the available profiles and confirm. You will determine
the recommended profile for the system and switch to it. Finally, you
will deactivate tuning and reactivate it. You will confirm the
activation to conclude the exercise.

1[.]{.c19}Install the *tuned* package if it is not already installed:

![](images/00643.jpeg){.image2}

The output indicates that the software is already installed.

2[.]{.c19}Start the *tuned* service and set it to autostart at reboots:

![](images/00644.jpeg){.image2}

3[.]{.c19}Confirm the startup:

![](images/00645.jpeg){.image2}

4[.]{.c19}Display the list of available tuning profiles:

![](images/00646.jpeg){.image2}

The output shows the nine predefined profiles. It also exhibits the
current active profile.

5[.]{.c19}List only the current active profile:

![](images/00647.jpeg){.image2}

6[.]{.c19}Switch to the *powersave* profile and confirm:

![](images/00648.jpeg){.image2}

The active profile is now *powersave*.

7[.]{.c19}Determine the recommended profile for *server1* and switch to
it:

![](images/00649.jpeg){.image2}

The first instance of the command shows the best recommended profile for
*server1* based on its characteristics, the second command instance
switched to the recommended profile, and the third instance confirmed
the switching.

8[.]{.c19}Turn off tuning:

![](images/00650.jpeg){.image2}

The service will not perform any tuning until it is reactivated.

9[.]{.c19}Reactivate tuning and confirm:

![](images/00651.jpeg){.image2}

The tuning is re-enabled and the *virtual-guest* profile is in effect.
This concludes the exercise.

**[Chapter]{#part0024_split_001.html#id_349 .calibre10} Summary**

This chapter started with a discussion of systemd, the default service
management and system initialization scheme used in RHEL. We explored
key components of systemd, its key directories, and examined unit and
target configuration files. We utilized the lone systemd administration
command to switch system operational states, identify and set default
boot targets, and manage service start, stop, and status checking.

Next, we looked at the traditional system logging and newer systemd
journaling services. Both mechanisms have similarities and differences
in how they capture log data and where they direct it for storage and
retrieval. We examined the system log configuration file and the
configuration file that controls the log file rotation settings. The log
subsystem proves valuable when records are needed for monitoring,
troubleshooting, auditing, or reporting purpose.

Finally, we explored preconfigured tuning profiles and analyzed pros and
cons associated with each one of them. We demonstrated how to determine
a recommended profile for the system and how to set and activate it.

**[Review]{#part0024_split_001.html#id_350 .calibre10} Questions**

1[.]{.c19}The *systemd* command may be used to rebuild a new kernel.
True or False?

2[.]{.c19}Which command is used to manage system services?

3[.]{.c19}Which configuration file must be modified to ensure journal
log entries are stored persistently?

4[.]{.c19}What is the PID of the *systemd* process?

5[.]{.c19}What is a target in *systemd*?

6[.]{.c19}You need to append a text string "Hello world" to the system
log file. What would be the command to achieve this?

7[.]{.c19}What is the recommended location to store custom log
configuration files?

8[.]{.c19}What would the command *systemctl list-dependencies crond* do?

9[.]{.c19}Name the two directory paths where *systemd* unit files are
stored.

10[.]{.c23}What would you run to identify the recommended tuning profile
for the system?

11[.]{.c23}What would the command *systemctl get-default* do?

12[.]{.c23}*systemd* starts multiple services concurrently during system
boot. True or False?

13[.]{.c23}What is the name of the boot log file?

14[.]{.c23}Which *systemctl* subcommand is executed after a unit
configuration file has been modified to apply the changes?

15[.]{.c23}Which other logging service complements the *rsyslog*
service?

16[.]{.c23}A RHEL system is booted up. You want to view all messages
that were generated during the boot process. Which log file would you
look at?

17[.]{.c23}What would the command *systemctl restart rsyslog* do?

18[.]{.c23}What are the two common *systemd* targets production RHEL
servers are typically configured to run at?

19[.]{.c23}By default, log files are rotated automatically every week.
True or False?

**[Answers]{#part0024_split_001.html#id_351 .calibre10} to Review
Questions**

1[.]{.c19}False.

2[.]{.c19}The *systemctl* command.

3[.]{.c19}The *journald.conf* file under the **etc*systemd* directory.

4[.]{.c19}The PID of the *systemd* process is 1.

5[.]{.c19}A target is a collection of units.

6[.]{.c19}The command to accomplish the desired result would be *logger
-i "Hello world"*.

7[.]{.c19}The recommended location to store custom log configuration
files is **etc*rsyslog.d* directory.

8[.]{.c19}The command provided will display all dependent units
associated with the specified service.

9[.]{.c19}The directory locations are **etc*systemd/system* and
**usr*lib/systemd/system*.

10[.]{.c23}The *tuned-adm recommend* command.

11[.]{.c23}The command provided will reveal the current default boot
target.

12[.]{.c23}True.

13[.]{.c23}The *boot.log* file in the **var*log* directory.

14[.]{.c23}The *daemon-reload* subcommand.

15[.]{.c23}The *systemd-journald* service.

16[.]{.c23}The **var*log/boot.log* file.

17[.]{.c23}The command provided will restart the *rsyslog* service.

18[.]{.c23}The two common *systemd* boot targets are multi-user and
graphical.

19[.]{.c23}True.

**[Do-]{#part0024_split_001.html#id_352 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

**Lab 12-1: Modify Default Boot Target**

As *user1* with *sudo* on *server1*, modify the default boot target from
graphical to multi-user, and reboot the system to test it. Run the
*systemctl* and *who* commands after the reboot for validation. Restore
the default boot target back to graphical and reboot to verify. (Hint:
System Initialization and Service Management).

**[Lab]{#part0024_split_001.html#id_353 .calibre10} 12-2: Record Custom
Alerts**

As *user1* with *sudo* on *server1*, write the message "This is
\$LOGNAME adding this marker on \$(date)" to **var*log/messages* file.
Ensure that variable and command expansions work. Verify the entry in
the file. (Hint: System Logging).

**[Lab]{#part0024_split_001.html#id_354 .calibre10} 12-3: Apply Tuning
Profile**

As *user1* with *sudo* on *server1*, identify the current system tuning
profile with the *tuned-adm* command. List all available profiles. List
the recommended profile for *server1*. Apply the "balanced" profile and
verify with *tuned-adm*. (Hint: System Tuning).

[]{#part0025_split_000.html}

## Chapter 13 {#part0025_split_000.html#calibre_pb_0 .calibre11}

**Basic Storage Partitioning**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Master Boot Record vs. GUID
Partition Table

[![](images/00001.jpeg){.c37}]{.c36}Identify and understand disk
partitions

[![](images/00001.jpeg){.c37}]{.c36}The concept of thin provisioning,
and its benefits

[![](images/00001.jpeg){.c37}]{.c36}Create and delete partition on MBR
disk

[![](images/00001.jpeg){.c37}]{.c36}Create and delete partition on GPT
disk

[![](images/00001.jpeg){.c37}]{.c36}Overview of Virtual Data Optimizer
and how it conserves storage

[![](images/00001.jpeg){.c37}]{.c36}Create and delete a Virtual Data
Optimizer volume

[RHCSA Objectives:]{.c39}

[27]{#part0025_split_000.html#id_793 .calibre10}. List, create, and
delete partitions on MBR and GPT disks

[37]{#part0025_split_000.html#id_803 .calibre10}. Configure disk
compression

::: {#part0025_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0025_split_001.html}

[ D]{.c42}[ata]{.c43} is stored on disks that are logically divided into
partitions. A partition can exist on a portion of a disk, on an entire
disk, or it may span multiple disks. Each partition is accessed and
managed independent of other partitions and may contain a file system or
swap space. Partitioning information is stored at special disk locations
that the system references at boot time. RHEL offers a number of tools
for partition management. Partitions created with a combination of most
of these tools can coexist on a single disk.

Thin provisioning is a powerful feature that guarantees an efficient use
of storage space by allocating only what is needed and by storing data
at adjacent locations. Many storage management solutions such as those
we discuss later in this chapter and in the next incorporate thin
provisioning technology in their core configuration.

Virtual Disk Optimizer is one of the newer storage management solutions
incorporated in RHEL. It capitalizes on thin provisioning,
de-duplication, and compression technologies to conserve storage space,
improve data throughput, and save money.

This is the first of the three chapters that shed light on storage
solutions. The next two chapters ([Chapters
14](#part0026_split_000.html#page_321){.calibre5} and
[15](#part0027_split_000.html#page_345){.calibre5}) discuss advanced
concepts and management tools.

**[Storage]{#part0025_split_001.html#id_355 .calibre10} Management
Overview**

A disk in RHEL can be carved up into several partitions. This partition
information is stored on the disk in a small region, which is read by
the operating system at boot time. This region is referred to as the
*Master Boot Record* (MBR) on the BIOS-based systems, and *GUID
Partition Table* (GPT) on the UEFI-based systems. At system boot, the
BIOS/UEFI scans all storage devices, detects the presence of MBR/GPT
areas, identifies the boot disks, loads the bootloader program in memory
from the default boot disk, executes the boot code to read the partition
table and identify the */boot* partition, loads the kernel in memory,
and passes control over to it. Though MBR and GPT are designed for
different PC firmware types, their job is essentially the same: to store
disk partition information and the boot code.

**[Master]{#part0025_split_001.html#id_356 .calibre10} Boot Record
(MBR)**

The MBR resides on the first sector of the boot disk. MBR was the
preferred choice for saving partition table information on x86-based
computers. However, with the arrival of bigger and larger hard drives, a
newer firmware specification (UEFI) was introduced. MBR is still widely
used, but its use is diminishing in favor of UEFI.

MBR allows the creation of three types of partition---*primary*,
*extended*, and *logical*---on a single disk. Of these, only primary and
logical can be used for data storage; the extended is a mere enclosure
for holding the logical partitions and it is not meant for data storage.
MBR supports the creation of up to four primary partitions numbered 1
through 4 at a time. In case additional partitions are required, one of
the primary partitions must be deleted and replaced with an extended
partition to be able to add logical partitions (up to 11) within that
extended partition. Numbering for logical partitions begins at 5. MBR
supports a maximum of 14 usable partitions (3 primary and 11 logical) on
a single disk.

MBR cannot address storage space beyond 2TB. This is due to its 32-bit
nature and its 512-byte disk sector size. The MBR is non-redundant; the
record it contains is not replicated, resulting in an unbootable system
in the event of corruption. If your disk is smaller than 2TB and you
don't intend to build more than 14 usable partitions, you can use MBR
without issues. For more information on MBR, refer to [Chapter
11](#part0023_split_000.html#page_249){.calibre5} "Boot Process, GRUB2,
and the Linux Kernel".

**[GUID]{#part0025_split_001.html#id_357 .calibre10} Partition Table
(GPT)**

With the increasing use of disks larger than 2TB on x86 computers, a new
64-bit partitioning standard called *Globally Unique Identifiers* (GUID)
*Partition Table* (GPT) was developed and integrated into the UEFI
firmware. This new standard introduced plenty of enhancements, including
the ability to construct up to 128 partitions (no concept of extended or
logical partitions), utilize disks larger than 2TB, use 4KB sector size,
and store a copy of the partition information before the end of the disk
for redundancy.

Moreover, this standard allows a BIOS-based system to boot from a GPT
disk using the bootloader program stored in a protective MBR at the
first disk sector. In addition, the UEFI firmware also supports the
secure boot feature, which only allows signed binaries to boot. For more
information on UEFI and GPT, refer to [Chapter
11](#part0023_split_000.html#page_249){.calibre5} "Boot Process, GRUB2,
and the Linux Kernel".

**[Disk]{#part0025_split_001.html#id_358 .calibre10} Partitions**

The space on a storage device can be sliced into partitions. Care must
be taken when adding a new partition to elude data corruption with
overlapping an extant partition or wasting storage by leaving unused
space between adjacent partitions. On *server1*, the disk that was
allocated at the time of installation is recognized as *sda* (**s** for
**S**ATA, SAS, or SCSI device) **d**isk **a**, with the first partition
identified as *sda1* and the second partition as *sda2*. Any subsequent
disks added to the system will be known as *sdb*, *sdc*, *sdd*, and so
on, and will use 1, 2, 3, *etc.* for partition numbering.

RHEL offers a command called *lsblk* to list disk and partition
information. The following graphic illustrates the current storage
status on *server1*:

![](images/00652.jpeg){.image2}

It reveals the presence of one 10GB disk, *sda*, with two partitions:
*sda1* and *sda2*. The first partition holds */boot*, and the second one
is an LVM object encapsulating *root* and *swap* logical volumes within
it. Both *sda1* and *sda2* partitions occupy the entire disk capacity.
The *sr0* represents the ISO image mounted as an optical medium.

![](images/00002.jpeg){.image} LVM is discussed at length in [Chapter
14](#part0026_split_000.html#page_321){.calibre5} "Advanced Storage
Partitioning".

There are additional tools such as *fdisk* and *parted* available that
can be used to expose disk and partitioning information. Let's run
*fdisk* with -l and see what it reveals:

![](images/00653.jpeg){.image2}

The output depicts the size of *sda* in GBs, bytes, and sectors, the
type of disk label (dos) the disk has, and the disk's geometry in the
top block. The second block shows the two disk partitions: *sda1* as the
bootable partition marked with an asterisk (\*) and *sda2* as an LVM
partition. It also exposes the starting and ending sector numbers, size
in 1KB blocks, and type of each partition. The identifiers 83 and 8e are
hexadecimal values for the partition types. The last two blocks are
specific to the LVM logical volumes that exist within the *sda2*
partition. A detailed coverage on LVM is provided in [Chapter
14](#part0026_split_000.html#page_321){.calibre5} "Advanced Storage
Partitioning".

**[Storage]{#part0025_split_001.html#id_359 .calibre10} Management
Tools**

RHEL offers numerous tools and toolsets for storage management, and they
include *parted*, *gdisk*, VDO, LVM, and Stratis. There are other native
tools available in the OS, but their discussion is beyond the scope of
this book. Partitions created with a combination of most of these tools
and toolsets can coexist on the same disk. We look at *parted*, *gdisk*,
and VDO in this chapter and LVM and Stratis in [Chapter
14](#part0026_split_000.html#page_321){.calibre5} "Advanced Storage
Partitioning".

*parted* is a simple tool that understands both MBR and GPT formats.
*gdisk* is designed to support the GPT format only, and it may be used
as a replacement of *parted*. VDO is a disk optimizer software that
takes advantage of certain technologies to minimize the overall data
footprint on storage devices. LVM is a feature-rich logical volume
management solution that gives flexibility in storage management.
Stratis capitalizes on thin provisioning to create volumes much larger
in size than the underlying storage devices they are built upon.

**[Thin]{#part0025_split_001.html#id_360 .calibre10} Provisioning**

*Thin provisioning* technology allows for an economical allocation and
utilization of storage space by moving arbitrary data blocks to
contiguous locations, which results in empty block elimination. With
thin provisioning support in VDO, LVM, and Stratis, you can create a
*thin pool* of storage space and assign volumes much larger storage
space than the physical capacity of the pool. Workloads begin consuming
the actual allocated space for data writing. When a preset custom
threshold (80%, for instance) on the actual consumption of the physical
storage in the pool is reached, expand the pool dynamically by adding
more physical storage to it. The volumes will automatically start
exploiting the new space right away. The thin provisioning technique
helps prevent spending more money upfront.

**[Adding]{#part0025_split_001.html#id_361 .calibre10} Storage for
Practice**

This and the next two chapters have a considerable number of exercises
that require block storage devices for practice. In [Chapter
01](#part0013_split_000.html#page_1){.calibre5} "Local Installation"
under "Lab Infrastructure for Practice", we mentioned that *server2*
will have 4x250MB, 1x4GB, and 2x1GB virtual disks for storage exercises.
We presume that *server2* was built as part of Lab 1-1 and it is now
available for use.

**[Exercise]{#part0025_split_001.html#id_362 .calibre10} 13-1: Add
Required Storage to server2**

This exercise will add the required storage disks to *server2*
(*RHEL8-VM2*) using VirtualBox.

In this exercise, you will start VirtualBox and add 4x250MB, 1x4GB, and
2x1GB disks to *server2* in preparation for exercises in this chapter
and [Chapters 14](#part0026_split_000.html#page_321){.calibre5} and
[15](#part0027_split_000.html#page_345){.calibre5}.

1[.]{.c19}Start VirtualBox on your Windows/Mac computer and highlight
the *RHEL8-VM2* virtual machine that you created in Lab 1-1. See [Figure
13-1](#part0025_split_001.html#id_675){.calibre5}.

::: c49
::: width_
![](images/00654.jpeg){.calibre13}
:::

**Figure 13-1 VirtualBox Interface**
:::

2[.]{.c19}Click Settings at the top and then Storage on the window that
pops up. Click on "Controller: SATA" to select it. [Figure
13-2](#part0025_split_001.html#page_306){.calibre5}.

::: c49
::: width_
![](images/00655.jpeg){.calibre13}
:::

**Figure 13-2 VirtualBox -- Add Storage**
:::

3[.]{.c19}Click on the right-side icon next to "Controller: SATA" to add
a hard disk.

4[.]{.c19}Follow this sequence to add a 250MB disk: Click "Create new
disk", "VDI (Virtualization Disk Image)", "Dynamically allocated", and
adjust the size to 250MB. Assign the disk a unique name. [Figure
13-3](#part0025_split_001.html#id_676){.calibre5}.

::: c49
::: width_
![](images/00656.jpeg){.calibre13}
:::

**Figure 13-3 VirtualBox -- Adjust Disk Name and Size**
:::

5[.]{.c19}Click Create to create and attach the disk to the VM.

6[.]{.c19}Repeat steps 3 through 5 three more times to add disks of the
same size to the VM.

7[.]{.c19}Repeat steps 3 through 5 one time to add a disk of size 4GB to
the VM.

8[.]{.c19}Repeat steps 3 through 5 two times to add two disks of size
1GB to the VM.

9[.]{.c19}The final list of disks should look similar to what is shown
in [Figure 13-4](#part0025_split_001.html#id_677){.calibre5} after the
addition of all seven drives. Disk names may vary.

::: c49
::: width_
![](images/00657.jpeg){.calibre13}
:::

**Figure 13-4 VirtualBox -- 7 New Disks Added**
:::

10[.]{.c23}Click OK to return to the main VirtualBox interface.

11[.]{.c23}Power on *RHEL8-VM2* to boot RHEL 8 in it.

12[.]{.c23}When the server is booted up, log on as *user1* and run the
*lsblk* command to verify the new storage:

![](images/00658.jpeg){.image2}

13[.]{.c23}The seven new disks added to *server2* are 250MB (*sdb*,
*sdc*, *sdd*, and *sde*), 4GB (*sdf*), and 1GB (*sdg* and *sdh*).

This concludes the exercise for storage addition to *server2*.

**MBR Storage Management with parted**

*parted* (*partition editor*) is a popular tool in RHEL that can be used
to partition disks. This program may be run interactively or directly
from the command prompt. It understands and supports both MBR and GPT
schemes, and can be used to create up to 128 partitions on a single GPT
disk. *parted* provides an abundance of subcommands to perform disk
management operations such as viewing, labeling, adding, naming, and
deleting partitions. [Table
13-1](#part0025_split_001.html#id_739){.calibre5} describes these
subcommands in that sequence.

::: c49
  ---------------- -------------------------------------------------------------------------------------------------------------------------------------------------
  **Subcommand**   **Description**
  print            Displays the partition table that includes disk geometry and partition number, start and end, size, type, file system type, and relevant flags.
  mklabel          Applies a label to the disk. Common labels are gpt and msdos.
  mkpart           Makes a new partition
  name             Assigns a name to a partition
  rm               Removes the specified partition
  ---------------- -------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0025_split_001.html#id_739} 13-1 Common parted
Subcommands**
:::

For the basic partition creation and deletion operations, Exercises 13-2
and 13-3 will show the use of this tool by directly invoking it from the
command prompt. You will use the **dev*sdb* disk for these exercises.
After making a partition, use the *print* subcommand to ensure you
created what you wanted. The **proc*partitions* file is also updated to
reflect the results of partition management operations.

**[Exercise]{#part0025_split_001.html#id_363 .calibre10} 13-2: Create an
MBR Partition**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will assign partition type "msdos" to **dev*sdb*
for using it as an MBR disk. You will create and confirm a 100MB primary
partition on the disk.

1[.]{.c19}Execute *parted* on **dev*sdb* to view the current partition
information:

![](images/00659.jpeg){.image2}

There is an error on line 1 of the output, indicating an unrecognized
label. This disk must be labeled before it can be partitioned.

2[.]{.c19}Assign disk label "msdos" to the disk with *mklabel*. This
operation is performed only once on a disk.

![](images/00660.jpeg){.image2}

The *print* subcommand confirms the successful application of the label.

![](images/00002.jpeg){.image} To use the GPT partition table type, run
"**sudo parted *dev*sdb mklabel gpt**" instead.

3[.]{.c19}Create a 100MB primary partition starting at 1MB (beginning of
the disk) using *mkpart*:

![](images/00661.jpeg){.image2}

4[.]{.c19}Verify the new partition with *print*:

![](images/00662.jpeg){.image2}

Partition numbering begins at 1 by default.

5[.]{.c19}Confirm the new partition with the *lsblk* command:

![](images/00663.jpeg){.image2}

The device file for the first partition on the *sdb* disk is *sdb1* as
identified on the bottom line. The partition size is 95MB.

![](images/00002.jpeg){.image} Different tools will have variance in
reporting partition sizes. You should ignore minor differences.

6[.]{.c19}Check the **proc*partitions* file also:

![](images/00664.jpeg){.image2}

The virtual file is also updated with the new partition information.
This completes the steps for creating and verifying an MBR partition
using the *parted* command.

**[Exercise]{#part0025_split_001.html#id_364 .calibre10} 13-3: Delete an
MBR Partition**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will delete the *sdb1* partition that was created
in [Exercise 13-2](#part0025_split_001.html#id_363){.calibre5} and
confirm the deletion.

1[.]{.c19}Execute *parted* on **dev*sdb* with the *rm* subcommand to
remove partition number 1:

![](images/00665.jpeg){.image2}

2[.]{.c19}Confirm the partition deletion with *print*:

![](images/00666.jpeg){.image2}

The partition no longer exists.

3[.]{.c19}Check the **proc*partitions* file:

![](images/00667.jpeg){.image2}

The virtual file has the partition entry deleted as well. You can also
run the *lsblk* command for further verification. The partition has been
removed successfully.

[**EXAM TIP:**]{.c56} Knowing either parted or gdisk for the exam is
enough.

We will recreate partitions for use in LVM in [Chapter
14](#part0026_split_000.html#page_321){.calibre5} and then again in
[Chapter 15](#part0027_split_000.html#page_345){.calibre5} to construct
file system and swap structures.

**[GPT]{#part0025_split_001.html#id_365 .calibre10} Storage Management
with gdisk**

The *gdisk* (*GPT disk*) utility partitions disks using the GPT format.
This text-based, menu-driven program can show, add, verify, modify, and
delete partitions among other operations. *gdisk* can create up to 128
partitions on a single disk on systems with UEFI firmware.

The main interface of *gdisk* can be invoked by specifying a disk device
name such as **dev*sdc* with the command. Type *help* or *?* (question
mark) at the prompt to view available subcommands.

![](images/00668.jpeg){.image2}

The output illustrates that there is no partition table defined on the
disk at the moment. There are several subcommands in the main menu
followed by a short description. Refer to the screenshot above for a
list of subcommands.

**[Exercise]{#part0025_split_001.html#id_366 .calibre10} 13-4: Create a
GPT Partition**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will assign partition type "gpt" to **dev*sdc* for
using it as a GPT disk. You will create and confirm a 200MB partition on
the disk.

1[.]{.c19}Execute *gdisk* on **dev*sdc* to view the current partition
information:

![](images/00669.jpeg){.image2}

The disk currently does not have any partition table on it.

2[.]{.c19}Assign "gpt" as the partition table type to the disk using the
*o* subcommand. Enter "y" for confirmation to proceed. This operation is
performed only once on a disk.

![](images/00670.jpeg){.image2}

3[.]{.c19}Run the *p* subcommand to view disk information and confirm
the GUID partition table creation:

![](images/00671.jpeg){.image2}

The output returns the assigned GUID and states that the partition table
can hold up to 128 partition entries.

4[.]{.c19}Create the first partition of size 200MB starting at the
default sector with default type "Linux filesystem" using the *n*
subcommand:

![](images/00672.jpeg){.image2}

5[.]{.c19}Verify the new partition with *p*:

Command (? For help): **p**

![](images/00673.jpeg){.image2}

6[.]{.c19}Run *w* to write the partition information to the partition
table and exit out of the interface. Enter "y" to confirm when prompted.

![](images/00674.jpeg){.image2}

![](images/00002.jpeg){.image} You may need to run the partprobe command
after exiting the gdisk utility to update the kernel of the changes.

7[.]{.c19}Verify the new partition by issuing either of the following at
the command prompt:

![](images/00675.jpeg){.image2}

The device file for the first partition on the *sdc* disk is *sdc1* and
it is 200MB in size as reported in the above outputs. This completes the
steps for creating and verifying a GPT partition using the *gdisk*
command.

**[Exercise]{#part0025_split_001.html#id_367 .calibre10} 13-5: Delete a
GPT Partition**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will delete the *sdc1* partition that was created
in [Exercise 13-4](#part0025_split_001.html#id_366){.calibre5} and
confirm the removal.

1[.]{.c19}Execute *gdisk* on **dev*sdc* and run *d1* at the utility's
prompt to delete partition number 1:

![](images/00676.jpeg){.image2}

2[.]{.c19}Confirm the partition deletion with *p*:

![](images/00677.jpeg){.image2}

The partition no longer exists.

3[.]{.c19}Write the updated partition information to the disk with *w*
and quit *gdisk*:

![](images/00678.jpeg){.image2}

4[.]{.c19}Verify the partition deletion by issuing either of the
following at the command prompt:

![](images/00679.jpeg){.image2}

Both commands confirm the successful partition removal.

**[Storage]{#part0025_split_001.html#id_368 .calibre10} Optimization
with Virtual Data Optimizer (VDO)**

One of the new features recently introduced in RHEL is a device driver
layer that sits between the operating system kernel and the physical
storage devices. The goals are to conserve disk space, improve data
throughput, and save on storage cost. This feature is referred to as
*Virtual Data Optimizer* (VDO). VDO employs thin provisioning,
de-duplication, and compression technologies to help realize the goals.

**[How]{#part0025_split_001.html#id_369 .calibre10} VDO Conserves
Storage Space**

VDO makes use of the thin provisioning technology to identify and
eliminate empty (zero-byte) data blocks. This is referred to as
*zero-block elimination*. VDO removes randomization of data blocks by
moving in-use data blocks to contiguous locations on the storage device.
This is the initial stage in the process.

Next, VDO keeps an eye on data being written to the disk. If it detects
that the new data is an identical copy of some existing data, it makes
an internal note of it but does not actually write the redundant data to
the disk. VDO uses the technique called *de-duplication* to this end.
This technique is implemented in RHEL with the inclusion of a kernel
module called *UDS* (*Universal Deduplication Service*). This is the
second stage in the process.

In the third and final stage, VDO calls upon another kernel module
called *kvdo*, which compresses the residual data blocks and
consolidates them on a lower number of blocks. This results in a further
drop in storage space utilization.

VDO runs in the background and processes inbound data through the three
stages on VDO-enabled volumes. VDO is not a CPU-or memory-intensive
process; it consumes a low amount of system resources.

**Creating and Managing VDO Volumes**

The concept of VDO volumes is similar to that of disk partitions, which
you created in Exercises 13-1 and 13-3 using *parted* and *gdisk*. VDO
volumes can be initialized for use just like disk partitions, or they
can be used as LVM physical volumes.

VDO offers a set of commands to create, manage, and monitor volumes. Of
these *vdo* and *vdostats* commands are discussed and used in this
section. The *vdo* command is used to create and perform essential
operations on VDO volumes, and the *vdostats* command is employed to
monitor usage statistics of the underlying physical storage device.

[Table 13-2](#part0025_split_001.html#id_740){.calibre5} summarizes the
subcommands available with *vdo*.

::: c49
  ---------------- -----------------------------------------------------
  **Subcommand**   **Description**
  create           Adds a new VDO volume on the specified block device
  status           Returns the status and attributes of VDO volumes
  list             Lists the names of all started VDO volumes
  start            Starts a VDO volume
  stop             Stops a VDO volume
  ---------------- -----------------------------------------------------

**[Table]{#part0025_split_001.html#id_740} 13-2 vdo Subcommands**
:::

The *vdostats* command has a couple of interesting options that you will
use shortly.

**[Exercise]{#part0025_split_001.html#id_370 .calibre10} 13-6: Install
Software and Activate VDO**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will install the VDO software packages, start the
VDO service, and mark it for autostart on subsequent system reboots.

1[.]{.c19}Install packages *vdo* and *kmod-kvdo*:

![](images/00680.jpeg){.image2}

2[.]{.c19}Start the service and enable it to start automatically on
future system reboots:

![](images/00681.jpeg){.image2}

3[.]{.c19}Check the operational status of the service:

![](images/00682.jpeg){.image2}

The relevant packages for VDO are installed, and the VDO service is
started and activated. This concludes the exercise.

**Exercise 13-7: Create a VDO Volume**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create a volume called *vdo-vol1* of logical
size 16GB on **dev*sdf* disk (the actual size of **dev*sdf* is 4GB). You
will list the volume and display its status information. You will also
show the activation status of the compression and de-duplication
features.

1[.]{.c19}Create a volume called *vdo-vol1* (\--name) on the **dev*sdf*
device (\--device) with a logical size of 16GB (\--vdoLogicalSize) and
slab size of 128MB (\--vdoSlabSize):

![](images/00683.jpeg){.image2}

![](images/00002.jpeg){.image} Increase the amount of memory allocated
to the virtual machine to 2GB if the output complains about insufficient
memory.

![](images/00002.jpeg){.image} If the logical size is not specified, the
VDO volume will have the same size as the underlying disk (*dev*sdf in
this case).

![](images/00002.jpeg){.image} The slab size is the size of the
increment by which VDO volumes grow. This value must be a power of two
between 128MB and 32GB; the default is 2GB. The default unit of size
specification is MB.

2[.]{.c19}List the new volume using the *vdo* and *lsblk* commands:

![](images/00684.jpeg){.image2}

As indicated, the major number for the VDO volume is 253, which is
associated with the device mapper kernel driver. The output also shows
the logical volume size (16GB) and type (vdo). It also depicts the disk
(*sdf*) that houses the volume, along with its actual size (4GB).

3[.]{.c19}Display the usage status of the volume:

![](images/00685.jpeg){.image2}

The size of the actual disk is 4GB. Due to thin provisioning, the system
allowed you to create the VDO volume much larger in size (4 times) than
the physical disk capacity.

4[.]{.c19}Show detailed statistics for the volume including
configuration information:

![](images/00686.jpeg){.image2}

The output will expose over one hundred different settings for the
volume.

5[.]{.c19}Display detailed statistics for the volume including
configuration information:

![](images/00687.jpeg){.image2}

The status includes volume, kernel module, and configuration
information. It also provides a detailed look at volume-specific
elements.

6[.]{.c19}Show the activation status of the compression and
de-duplication features:

![](images/00688.jpeg){.image2}

Both compression and de-duplication features are activated by default on
new VDO volumes. This concludes the exercise.

**Exercise 13-8: Delete a VDO Volume**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will delete the *vdo-vol1* volume that was created
in [Exercise 13-7](#part0025_split_000.html#page_316){.calibre5} and
confirm the removal.

1[.]{.c19}Verify that the volume still exists with the *vdo* and *lsblk*
commands:

![](images/00689.jpeg){.image2}

2[.]{.c19}Specify the name (\--name) of the volume with the *vdo*
command to delete:

![](images/00690.jpeg){.image2}

3[.]{.c19}Confirm the removal with the *vdo* and *lsblk* commands:

![](images/00691.jpeg){.image2}

The volume has been deleted successfully as reported in the above
outputs.

[**EXAM TIP:**]{.c56} Make sure that you run the lsblk command before
attempting any storage task. The VDO task is usually performed on the
biggest sized available disk.

You will recreate VDO volumes in [Chapter
15](#part0027_split_000.html#page_345){.calibre5} "Local File Systems
and Swap" for use as file systems and swap areas.

**[Chapter]{#part0025_split_001.html#id_371 .calibre10} Summary**

This chapter started with an overview of how and where disk partitioning
information is stored. It presented a comparison between the two common
schemes and explained which one to use and in which situation. A little
later, we touched briefly on the common storage management solutions
available in RHEL.

We examined the concept of thin provisioning and realized the benefits
associated with this technology.

Next, we carved up available disk devices using both MBR and GPT
partitioning schemes. We demonstrated the partition creation, display,
and delete operations by running one command directly at the command
prompt and launching the other in interactive mode.

The last topic of the chapter focused on a storage management solution
that was introduced in RHEL not too long ago. This solution exploits the
underlying thin provisioning, de-duplication, and compression
technologies to save cost, ensure efficient use of storage space, and
improve data throughput. We performed a couple of exercises in the end
to demonstrate the creation and deletion of volumes using this solution.

**[Review]{#part0025_split_001.html#id_372 .calibre10} Questions**

1[.]{.c19}What is missing in the command *vdo create \--name vdo1
\--vdoLogicalSize 16GB \--vdoSlabSize 128MB*?

2[.]{.c19}What is the maximum number of usable partitions that can be
created on a GPT disk?

3[.]{.c19}Which kernel module is responsible for data block compression
in VDO volumes?

4[.]{.c19}What are the three techniques VDO volumes employ for storage
conservation?

5[.]{.c19}What would the command *parted *dev*sdc mkpart pri 1 200m* do?

6[.]{.c19}Where is the partition table information stored on BIOS-based
systems?

7[.]{.c19}What is the name of the technology that VDO employs to remove
randomization of data blocks?

8[.]{.c19}What would *sdd3* represent?

9[.]{.c19}Which command can be used to view the usage statistics of VDO
volumes?

10[.]{.c23}Thin provisioning technology allows us to create logical
volumes of sizes larger than the actual physical storage size. True or
False?

11[.]{.c23}You have an unused VDO volume called *vdo1* on **dev*sdf*
disk and you try to delete it. You run *vdo remove \--device *dev*sdf*,
but the removal fails. What are you doing wrong?

12[.]{.c23}What is the maximum number of usable partitions that can be
created on an MBR disk?

13[.]{.c23}What would the command *systemctl \--now enable vdo* do?

14[.]{.c23}The *gdisk* utility can be used to store partition
information in MBR format. True or False?

15[.]{.c23}What would the command *parted *dev*sdd mklabel msdos* do?

16[.]{.c23}VDO is a memory-intensive storage optimization solution and
should not be used. True or False?

17[.]{.c23}Which file in the */proc* file system stores the in-memory
partitioning information?

18[.]{.c23}You have created a VDO volume and you want to check whether
compression is enabled. Which command would you use?

19[.]{.c23}Deduplication is the process of zero-block elimination. True
or False?

**[Answers]{#part0025_split_001.html#id_373 .calibre10} to Review
Questions**

1[.]{.c19}The storage device (\--device) name is missing from the
command provided.

2[.]{.c19}128.

3[.]{.c19}The *kvdo* module is responsible for compressing data blocks
in VDO volumes.

4[.]{.c19}VDO volumes use thin provisioning, de-duplication, and
compression techniques for storage conservation.

5[.]{.c19}The command provided will create a primary partition of size
200MB on the *sdc* disk starting at the beginning of the disk.

6[.]{.c19}The partition table information is stored on the Master Boot
Record.

7[.]{.c19}VDO employs thin provisioning technology to remove data block
randomization.

8[.]{.c19}*sdd3* represents the third partition on the fourth disk.

9[.]{.c19}The *vdostats* command can be used to view VDO volume usage
statistics.

10[.]{.c23}True.

11[.]{.c23}The correct command would be *vdo remove \--name vdo1*.

12[.]{.c23}14.

13[.]{.c23}The command provided will start the VDO service and enable it
to autostart on system reboots.

14[.]{.c23}False. The *gdisk* tool is only for GPT type tables.

15[.]{.c23}The command provided will apply msdos label to the *sdd*
disk.

16[.]{.c23}False. VDO uses low memory and other compute resources.

17[.]{.c23}The *partitions* file.

18[.]{.c23}You can issue the *vdo status* command and pipe the output to
*grep* for the pattern "compression".

19[.]{.c23}False. Deduplication is the process of removing blocks of
identical data.

**[Do-]{#part0025_split_001.html#id_374 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

Add more storage to *server2* if required.

**[Lab]{#part0025_split_001.html#id_375 .calibre10} 13-1: Create and
Remove Partitions with parted**

As *user1* with *sudo* on *server2*, create a 100MB primary partition on
one of the available 250MB disks (*lsblk*) by invoking the *parted*
utility directly at the command prompt. Apply label "msdos" if the disk
is new. Create another 100MB partition by running *parted* interactively
while ensuring that the second partition won't overlap the first. Verify
the label and the partitions. Remove both partitions from the command
prompt. (Hint: MBR Storage Management with parted).

**[Lab]{#part0025_split_001.html#id_376 .calibre10} 13-2: Create and
Remove Partitions with gdisk**

As *user1* with *sudo* on *server2*, create two 80MB partitions on one
of the 250MB disks (*lsblk*) using the *gdisk* utility. Make sure the
partitions won't overlap. Verify the partitions. You may delete the
partitions if you want. (Hint: GPT Storage Management with gdisk).

**[Lab]{#part0025_split_001.html#id_377 .calibre10} 13-3: Create and
Delete VDO Volumes**

As *user1* with *sudo* on *server2*, check to see if VDO software is
installed and the VDO service is enabled and started. Identify the 4GB
disk with the *lsblk* command, and make sure that it is not in use.
Create a volume *testvdo* with a logical size 16GB on the 4GB disk using
the *vdo* command. Select an appropriate slab size for the volume.
Verify the volume creation with the *vdo*, *lsblk*, and *vdostats*
commands. (Hint: Storage Optimization with Virtual Data Optimizer).

**[Lab]{#part0025_split_001.html#id_378 .calibre10} 13-4: Disable and
Enable VDO Volume Features**

As *user1* with *sudo* on *server2*, use the *vdostats* command to check
whether compression and de-duplication are enabled for the volume
created in Lab 13-3. Use the *vdo* command with disableCompression and
disableDeduplication subcommands to disable compression and
de-duplication, and verify with *vdostats*. Reactivate both features and
confirm activation. You may delete the volume if you want. (Hint:
Storage Optimization with Virtual Data Optimizer).

[]{#part0026_split_000.html}

## Chapter 14 {#part0026_split_000.html#calibre_pb_0 .calibre11}

**Advanced Storage Partitioning**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Describe Logical Volume Manager and
its components

[![](images/00001.jpeg){.c37}]{.c36}Understand various Logical Volume
Manager management operations

[![](images/00001.jpeg){.c37}]{.c36}Know Logical Volume Manager
administration commands

[![](images/00001.jpeg){.c37}]{.c36}Create and confirm physical volumes,
volume groups, and logical volumes

[![](images/00001.jpeg){.c37}]{.c36}Rename, reduce, extend, and remove
logical volumes

[![](images/00001.jpeg){.c37}]{.c36}Extend, reduce, and remove volume
groups

[![](images/00001.jpeg){.c37}]{.c36}Remove physical volumes

[![](images/00001.jpeg){.c37}]{.c36}Overview of Stratis (a
volume-managing file system solution in RHEL) service and how it works

[![](images/00001.jpeg){.c37}]{.c36}Understand various Stratis
management operations and the command

[![](images/00001.jpeg){.c37}]{.c36}Create, confirm, expand, rename, and
destroy pools and file systems

[RHCSA Objectives:]{.c39}

[28]{#part0026_split_000.html#id_794 .calibre10}[.]{.c29}Create and
remove physical volumes

[29]{#part0026_split_000.html#id_795 .calibre10}[.]{.c29}Assign physical
volumes to volume groups

[30]{#part0026_split_000.html#id_796 .calibre10}[.]{.c29}Create and
delete logical volumes

32[.]{.c29}Add new partitions and logical volumes, and swap to a system
non-destructively (the swap portion of this objective is covered in
Chapter 15)

[35]{#part0026_split_000.html#id_801 .calibre10}[.]{.c29}Extend existing
logical volumes (additional coverage on this objective is available in
Chapter 15)

[38]{#part0026_split_000.html#id_804 .calibre10}[.]{.c29}Manage layered
volumes

::: {#part0026_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0026_split_001.html}

[ T]{.c42}[his]{.c43} chapter is the second of the three chapters (the
other two being [Chapter
13](#part0025_split_000.html#page_301){.calibre5} (previous) and
[Chapter 15](#part0027_split_000.html#page_345){.calibre5} (next)) that
expounds upon storage management concepts and solutions available in
RHEL. We discussed partitioning and thinly provisioned volumes in the
previous chapter. This chapter presents a detailed coverage on Logical
Volume Manager solution. LVM sets up an abstraction layer between the
operating system and the storage hardware. It utilizes virtual objects
for storage pooling and allocation, and offers a whole slew of
management commands, each of which carries out a particular operation.

The other advanced storage management solution discussed in this chapter
is a volume-managing file system that capitalizes on the proven features
of LVM and the kernel device driver software. This solution dynamically
adjusts the size of the underlying volume, eliminating the need for
manual expansion.

**[Logical]{#part0026_split_001.html#id_379 .calibre10} Volume Manager
(LVM)**

The *Logical Volume Manager* (LVM) solution is widely used for managing
block storage in Linux. LVM provides an abstraction layer between the
physical storage and the file system, enabling the file system to be
resized, span across multiple physical disks, use arbitrary disk space,
*etc.* LVM accumulates spaces taken from partitions or entire disks
(called *Physical Volumes*) to form a logical container (called *Volume
Group*), which is then divided into logical partitions (called *Logical
Volumes*). The other key benefits of LVM include online resizing of
volume groups and logical volumes, online data migration between logical
volumes and between physical volumes, user-defined naming for volume
groups and logical volumes, mirroring and striping across multiple
physical disks, and snapshotting of logical volumes. [Figure
14-1](#part0026_split_001.html#id_678){.calibre5} depicts the LVM
components.

::: c49
::: width_
![](images/00692.jpeg){.calibre13}
:::

**Figure 14-1 LVM Structure**
:::

As noted above, the LVM structure is made up of three key objects called
physical volume, volume group, and logical volume. These objects are
further virtually broken down into *Physical Extents* (PEs) and *Logical
Extents* (LEs). The LVM components are explained in the following
subsections.

**Physical Volume**

A *Physical Volume* (PV) is created when a block storage device such as
a partition or an entire disk is initialized and brought under LVM
control. This process constructs LVM data structures on the device,
including a label on the second sector and metadata shortly thereafter.
The label includes the UUID, size, and pointers to the locations of data
and metadata areas. Given the criticality of metadata, LVM stores a copy
of it at the end of the physical volume as well. The rest of the device
space is available for use.

You can use an LVM command called *pvs* (*physical volume scan* or
*summary*) to scan and list available physical volumes on *server2*:

![](images/00693.jpeg){.image2}

The output shows one physical volume (PV) **dev*sda2* of size 9GB in
*rhel* volume group (VG). Additional information displays the metadata
format (Fmt) used, status of the physical volume under the Attr column
(a for allocatable), and the amount of free space available on the
physical volume (PFree).

Try running this command again with the -v flag to view more information
about the physical volume.

**[Volume]{#part0026_split_001.html#id_380 .calibre10} Group**

A *Volume Group* (VG) is created when at least one physical volume is
added to it. The space from all physical volumes in a volume group is
aggregated to form one large pool of storage, which is then used to
build logical volumes. The physical volumes added to a volume group may
be of varying sizes. LVM writes volume group metadata on each physical
volume that is added to it. The volume group metadata contains its name,
date and time of creation, how it was created, the extent size used, a
list of physical and logical volumes, a mapping of physical and logical
extents, *etc.* A volume group can have a custom name assigned to it at
the time of its creation. For example, it may be called *vg01*, *vgora*,
or *vgweb* that identifies the type of information it is constructed to
store. A copy of the volume group metadata is stored and maintained at
two distinct locations on each physical volume within the volume group.

You can use an LVM command called *vgs* (*volume group scan* or
*summary*) to scan and list available volume groups on *server2*:

![](images/00694.jpeg){.image2}

The output shows one volume group (VG) *rhel* on *server2* containing
one physical volume (#PV). Additional information displays the number of
logical volumes (#LV) and snapshots (#SN) in the volume group, status of
the volume group under the Attr column (w for writeable, z for
resizable, and n for normal), size of the volume group (VSize), and the
amount of free space available in the volume group (VFree).

Try running this command again with the -v flag to view more information
about the volume group.

**Physical Extent**

A physical volume is divided into several smaller logical pieces when it
is added to a volume group. These logical pieces are known as *Physical
Extents* (PE). An extent is the smallest allocatable unit of space in
LVM. At the time of volume group creation, you can either define the
size of the PE or leave it to the default value of 4MB. This implies
that a 20GB physical volume would have approximately 5,000 PEs. Any
physical volumes added to this volume group thereafter will use the same
PE size.

You can use an LVM command called *vgdisplay* (*volume group display*)
on *server2* and *grep* for 'PE Size' to view the PE size used in the
*rhel* volume group:

![](images/00695.jpeg){.image2}

The output reveals the PE size used for the *rhel* VG.

**[Logical]{#part0026_split_001.html#id_381 .calibre10} Volume**

A volume group consists of a pool of storage taken from one or more
physical volumes. This volume group space is used to create one or more
*Logical Volumes* (LVs). A logical volume can be created or weeded out
online, expanded or shrunk online, and can use space taken from one or
multiple physical volumes inside the volume group.

The default naming convention used for logical volumes is *lvol0, lvol1,
lvol2*, and so on; however, you may assign custom names to them. For
example, a logical volume may be called *system*, *undo*, or *webdata1*
so as to establish the type of information it is constructed to store.

You can use an LVM command called *lvs* (*logical volume scan* or
*summary*) to scan and list available logical volumes on *server2*:

![](images/00696.jpeg){.image2}

The output shows two logical volumes *root* and *swap* in *rhel* volume
group. Additional information displays the status of the logical volumes
under the Attr column (w for writeable, i for inherited allocation
policy, a for active, and o for open) and their sizes.

Try running this command again with the -v flag to view more information
about the logical volumes.

**[Logical]{#part0026_split_001.html#id_382 .calibre10} Extent**

A logical volume is made up of *Logical Extents* (LE). Logical extents
point to physical extents, and they may be random or contiguous. The
larger a logical volume is, the more logical extents it will have.
Logical extents are a set of physical extents allocated to the logical
volume.

The PE and LE sizes are normally kept the same within a volume group;
however, a logical extent can be smaller or larger than a physical
extent. The default LE size is 4MB, which corresponds to the default PE
size.

You can use an LVM command called *lvdisplay* (*logical volume display*)
on *server2* to view information about the *root* logical volume in the
*rhel* volume group.

![](images/00697.jpeg){.image2}

The output does not disclose the LE size; however, you can convert the
LV size in MBs (8,000) and then divide the result by the Current LE
count (2,047) to get the LE size (which comes close to 4MB).

**[LVM]{#part0026_split_001.html#id_383 .calibre10} Operations and
Commands**

The LVM toolset offers a multitude of administrative commands to carry
out various disk and volume management operations. These operations
include creating and removing a physical volume, volume group, and
logical volume; extending and reducing a volume group and logical
volume; renaming a volume group and logical volume; and listing and
displaying physical volume, volume group, and logical volume
information.

[Table 14-1](#part0026_split_001.html#id_741){.calibre5} summarizes the
common LVM tasks and the commands that are employed to accomplish them.

::: c49
  ---------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Command**                        **Description**
  **Create and Remove Operations**   
  pvcreate/pvremove                  Initializes/uninitializes a disk or partition for LVM use
  vgcreate/vgremove                  Creates/removes a volume group
  lvcreate/lvremove                  Creates/removes a logical volume
  **Extend and Reduce Operations**   
  vgextend/vgreduce                  Adds/removes a physical volume to/from a volume group
  lvextend/lvreduce                  Extends/reduces the size of a logical volume
  lvresize                           Resizes a logical volume. With the -r option, this command calls the resize2fs command and resizes the underlying file system as well. Applies to Ext2/Ext3/Ext4 file system types only.
  **Rename Operations**              
  vgrename                           Renames a volume group
  lvrename                           Renames a logical volume
  **Command**                        **Description**
  **List and Display Operations**    
  pvs/pvdisplay                      Lists/displays physical volume information
  vgs/vgdisplay                      Lists/displays volume group information
  lvs/lvdisplay                      Lists/displays logical volume information
  ---------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0026_split_001.html#id_741} 14-1 Common LVM Operations
and Commands**
:::

All the tools accept the -v switch to support verbosity. Refer to the
manual pages of the commands for usage and additional details.

As noted earlier, there are seven disks available on *server2* for
practice. Issue the *lsblk* command to confirm:

![](images/00698.jpeg){.image2}

You will use the *sdd* and *sde* disks for LVM activities in the
following exercises.

**[Exercise]{#part0026_split_001.html#id_384 .calibre10} 14-1: Create a
Physical Volume and Volume Group**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will initialize one partition *sdd1* (90MB) and
one disk *sde* (250MB) for use in LVM. You will create a volume group
called *vgbook* and add both physical volumes to it. You will use the PE
size of 16MB and list and display the volume group and the physical
volumes.

1[.]{.c19}Create a partition of size 90MB on *sdd* using the *parted*
command and confirm. You need to label the disk first, as it is a new
disk.

![](images/00699.jpeg){.image2}

The *print* subcommand confirms the creation of the partition. It is the
first partition on the disk.

2[.]{.c19}Set (*set*) the flag on the partition (1) to "lvm" using the
*parted* command:

![](images/00700.jpeg){.image2}

3[.]{.c19}Verify flag activation using the *print* subcommand with
*parted*:

![](images/00701.jpeg){.image2}

The flag is applied and enabled on the partition as indicated under the
Flags column.

4[.]{.c19}Initialize the *sdd1* partition and the *sde* disk using the
*pvcreate* command. Note that there is no need to apply a disk label on
*sde* with *parted* as LVM does not require it.

![](images/00702.jpeg){.image2}

The command generated a verbose output. You now have two physical
volumes available for use.

5[.]{.c19}Create *vgbook* volume group using the *vgcreate* command and
add the two physical volumes to it. Use the -s option to specify the PE
size in MBs.

![](images/00703.jpeg){.image2}

The above command combines the two options with a single hyphen.

6[.]{.c19}List the volume group information:

![](images/00704.jpeg){.image5}![](images/00704.jpeg){.image5}

The total capacity available in the *vgbook* volume group is 320MB.

7.Display detailed information about the volume group and the physical
volumes it contains:

![](images/00705.jpeg){.image2}

The verbose output includes the physical volume attributes as well.
There are a total of 20 PEs in the volume group (5 in *sdd1* and 15 in
*sde*), and each PE is 16MB in size. The collective size of all the
physical volumes represents the total size of the volume group, which is
20x16 = 320MB.

8[.]{.c19}List the physical volume information:

![](images/00706.jpeg){.image2}

The output shows the physical volumes in *vgbook*, along with their
utilization status.

9[.]{.c19}Display detailed information about the physical volumes:

![](images/00707.jpeg){.image2}

Once a partition or disk is initialized and added to a volume group,
they are treated identically within the volume group. LVM does not
prefer one over the other.

**[Exercise]{#part0026_split_001.html#id_385 .calibre10} 14-2: Create
Logical Volumes**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create two logical volumes, *lvol0* and
*lvbook1*, in the *vgbook* volume group. You will use 120MB for *lvol0*
and 192MB for *lvbook1* from the available pool of space. You will
display the details of the volume group and the logical volumes.

1[.]{.c19}Create a logical volume with the default name *lvol0* using
the *lvcreate* command. Use the -L option to specify the logical volume
size, 120MB. You may use the -v, -vv, or -vvv option with the command
for verbosity.

![](images/00708.jpeg){.image2}

The size for the logical volume may be specified in units such as MBs,
GBs, TBs, or as a count of LEs; however, MB is the default if no unit is
specified (see the previous command). The size of a logical volume is
always in multiples of the PE size. For instance, logical volumes
created in *vgbook* with the PE size set at 16MB can be 16MB, 32MB,
48MB, 64MB, and so on. The output above indicates that the logical
volume is 128MB (16x8), and not 120MB as specified.

2[.]{.c19}Create *lvbook1* of size 192MB (16x12) using the *lvcreate*
command. Use the -l switch to specify the size in logical extents and -n
for the custom name. You may use -v for verbose information.

![](images/00709.jpeg){.image2}

3[.]{.c19}List the logical volume information:

![](images/00710.jpeg){.image2}

Both logical volumes are listed in the output with their attributes and
sizes.

4[.]{.c19}Display detailed information about the volume group including
the logical volumes and the physical volumes:

![](images/00711.jpeg){.image2}

Alternatively, you can run the following to view only the logical volume
details:

![](images/00712.jpeg){.image2}

Review the attributes of the logical volumes as detailed above.

**[Exercise]{#part0026_split_001.html#id_386 .calibre10} 14-3: Extend a
Volume Group and a Logical Volume**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will add another partition *sdd2* of size 158MB to
*vgbook* to increase the pool of allocatable space. You will initialize
the new partition prior to adding it to the volume group. You will
increase the size of *lvbook1* to 336MB. You will display basic
information for the physical volumes, volume group, and logical volume.

1[.]{.c19}Create a partition of size 158MB on *sdd* and set the flag to
"lvm" using the *parted* command. Display the new partition to confirm
the partition number, size, and flag.

![](images/00713.jpeg){.image2}

2[.]{.c19}Initialize *sdd2* using the *pvcreate* command:

![](images/00714.jpeg){.image2}

3[.]{.c19}Extend *vgbook* by adding the new physical volume to it:

![](images/00715.jpeg){.image2}

4[.]{.c19}List the volume group:

![](images/00716.jpeg){.image2}

The output reflects the addition of a third physical volume to *vgbook*.
The total capacity of the volume group has now increased to 464MB with
144MB free.

5[.]{.c19}Extend the size of *lvbook1* to 340MB by adding 144MB using
the *lvextend* command:

![](images/00717.jpeg){.image2}

[**EXAM TIP:**]{.c56} Make sure the expansion of a logical volume does
not affect the file system and the data it contains. More details in
Chapter 15.

6[.]{.c19}Issue *vgdisplay* on *vgbook* with the -v switch for the
updated details:

![](images/00718.jpeg){.image2}

The output will show a lot of information about the volume group and the
logical and physical volumes it contains. It will reflect the updates
made in this exercise. In fact, each time a volume group or a logical
volume is resized, *vgdisplay* will reflect those changes. The above
output will display three physical volumes with the combined allocatable
space grown to 464MB. The number of PEs will have increased to 29, with
all of them allocated to logical volumes and 0 unused. The Logical
Volume sections will display the updated information for the logical
volumes. And at the very bottom, the three physical volumes will show
with their device names, and total and available PEs in each.

7[.]{.c19}View a summary of the physical volumes:

![](images/00719.jpeg){.image2}

8[.]{.c19}View a summary of the logical volumes:

![](images/00720.jpeg){.image2}

This brings the exercise to an end.

**[Exercise]{#part0026_split_001.html#id_387 .calibre10} 14-4: Rename,
Reduce, Extend, and Remove Logical Volumes**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will rename *lvol0* to *lvbook2*. You will
decrease the size of *lvbook2* to 50MB using the *lvreduce* command and
then add 32MB with the *lvresize* command. You will then remove both
logical volumes. You will display the summary for the volume groups,
logical volumes, and physical volumes.

1[.]{.c19}Rename *lvol0* to *lvbook2* using the *lvrename* command and
confirm with *lvs*:

![](images/00721.jpeg){.image2}

2[.]{.c19}Reduce the size of *lvbook2* to 50MB with the *lvreduce*
command. Specify the absolute desired size for the logical volume.
Answer "Do you really want to reduce vgbook/lvbook2?" in the
affirmative.

![](images/00722.jpeg){.image2}

3[.]{.c19}Add 32MB to *lvbook2* with the *lvresize* command:

![](images/00723.jpeg){.image2}

4[.]{.c19}Use the *pvs*, *lvs*, *vgs*, and *vgdisplay* commands to view
the updated allocation.

5[.]{.c19}Remove both *lvbook1* and *lvbook2* logical volumes using the
*lvremove* command. Use the -f option to suppress the "Do you really
want to remove active logical volume" message.

![](images/00724.jpeg){.image2}

![](images/00002.jpeg){.image} Removing a logical volume is a
destructive task. You need to ensure that you perform a backup of any
data in the target logical volume prior to deleting it. You will need to
unmount the file system or disable swap in the logical volume. See
[Chapter 15](#part0027_split_000.html#page_345){.calibre5} on how to
unmount a file system and disable swap.

6[.]{.c19}Execute the *vgdisplay* command and *grep* for "Cur LV" to see
the number of logical volumes currently available in *vgbook*. It should
show 0, as you have removed both logical volumes.

![](images/00725.jpeg){.image2}

This concludes the exercise.

**[Exercise]{#part0026_split_001.html#id_388 .calibre10} 14-5: Reduce
and Remove a Volume Group**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will reduce *vgbook* by removing the *sdd1* and
*sde* physical volumes from it, and then remove the volume group.
Confirm the deletion of the volume group and the logical volumes at the
end.

1[.]{.c19}Remove *sdd1* and *sde* physical volumes from *vgbook* by
issuing the *vgreduce* command:

![](images/00726.jpeg){.image2}

2[.]{.c19}Remove the volume group using the *vgremove* command. This
will also remove the last physical volume, *sdd2*, from it.

![](images/00727.jpeg){.image2}

![](images/00002.jpeg){.image} You can also use the -f option with the
*vgremove* command to force the volume group removal even if it contains
any number of logical and physical volumes in it.

![](images/00002.jpeg){.image} Remember to proceed with caution whenever
you perform reduce and erase operations.

3[.]{.c19}Execute the *vgs* and *lvs* commands for confirmation:

![](images/00728.jpeg){.image2}

This concludes the exercise.

**[Exercise]{#part0026_split_001.html#id_389 .calibre10} 14-6:
Uninitialize Physical Volumes**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will uninitialize all three physical
volumes---*sdd1*, *sdd2*, and *sde*---by deleting the LVM structural
information from them. Use the *pvs* command for confirmation. Remove
the partitions from the *sdd* disk and verify that all disks used in
Exercises 14-1 to 14-5 are now in their original raw state.

1[.]{.c19}Remove the LVM structures from *sdd1*, *sdd2*, and *sde* using
the *pvremove* command:

![](images/00729.jpeg){.image2}

2[.]{.c19}Confirm the removal using the *pvs* command:

![](images/00730.jpeg){.image2}

The partitions and the disk are now back to their raw state and can be
repurposed.

3[.]{.c19}Remove the partitions from *sdd* using the *parted* command:

![](images/00731.jpeg){.image2}

4[.]{.c19}Verify that all disks used in previous exercises have returned
to their original raw state using the *lsblk* command:

![](images/00732.jpeg){.image2}

This brings the exercise to an end.

We will recreate logical volumes in [Chapter
15](#part0027_split_000.html#page_345){.calibre5} and construct file
system and swap structures in them.

**[Stratis]{#part0026_split_001.html#id_390 .calibre10} Volume-Managing
File System**

RHEL 8 introduces a new simplified storage management solution called
*Stratis*. Stratis capitalizes on three existing matured storage
components: the *device mapper* (dm) kernel driver, the LVM solution,
and the XFS file system. It implements the advanced features of these
components to deliver file systems that are encapsulated within logical
volumes. These logical volumes are created and expanded dynamically and
transparently, hiding the underlying complexity. The Stratis solution
delivers file systems that are referred to as *volume-managing file
systems*. Stratis uses the thin provisioning technology as part of the
solution.

![](images/00002.jpeg){.image} File systems are explained in [Chapter
15](#part0027_split_000.html#page_345){.calibre5} "Local File Systems
and Swap".

The central idea surrounding the Stratis solution is a storage *pool*. A
storage pool is created using at least one disk or partition, which is
referred to as a *blockdev*. There can be a combination of the two in a
single pool and more can be added as the space requirement grows. The
total capacity of the pool is the aggregate of the spaces taken from all
the block devices that are part of the pool. Within a pool, *file
systems* can be created, all sharing the entire pool capacity.

![](images/00002.jpeg){.image} LVM logical volumes can also be used as
block devices in a Stratis pool.

[Figure 14-2](#part0026_split_001.html#id_679){.calibre5} provides a
bird's-eye view of the three major objects---pool, blockdev, and file
system---used in Stratis.

::: c49
::: width_
![](images/00733.jpeg){.calibre13}
:::

**Figure 14-2 Stratis Structure**
:::

The diagram in [Figure 14-2](#part0026_split_001.html#id_679){.calibre5}
shows a pool containing multiple disks and partitions. The pool capacity
is the aggregate of all the block storage devices included in the pool,
and this capacity is shared among all the file systems. Each Stratis
file system appears to occupy the entire pool capacity exclusively;
however, they are thinly provisioned. Their actual size grows as the
amount of data stored in them increases. Stratis takes care of the
dynamic expansion of the file systems and the underlying volumes as
needed. As Stratis handles the creation, formatting, and expansion of
the file systems, they must not be manually initialized or reconfigured.

**Stratis Management Operations and Command**

The primary command to manage Stratis is called *stratis*. This command
has a set of subcommands available to perform management operations such
as creating, viewing, renaming, and destroying pools and file systems,
and expanding pools.

[Table 14-2](#part0026_split_001.html#id_742){.calibre5} summarizes the
common Stratis subcommands that are employed to accomplish various
management tasks.

::: c49
  ------------- ------------------------------------------------------------------------------------------------------------------------------
  **Command**   **Description**
  pool          Administers storage pools. Subcommands are available to list, create, rename, expand, and destroy a pool.
  blockdev      Lists block devices
  filesystem    Administers file systems within storage pools. Subcommands are available to list, create, rename, and destroy a file system.
  ------------- ------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0026_split_001.html#id_742} 14-2 Common Stratis
Subcommands**
:::

Stratis runs as a service, so its operational state can be managed with
the *systemctl* command. The *stratis* command interacts with the
Stratis service to manage the pool and file systems dynamically.

For each pool added, Stratis creates a subdirectory under the */stratis*
directory matching the pool name. It then creates a symbolic link for
each file system under that subdirectory to the actual device file
located in the */dev* directory.

Stratis file systems are not fixed-sized; however, pool space can be
reserved to assure availability if multiple file systems share a pool.

**[Exercise]{#part0026_split_001.html#id_391 .calibre10} 14-7: Install
Software and Activate Stratis**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will install the Stratis software packages, start
the Stratis service, and mark it for autostart on subsequent system
reboots.

1[.]{.c19}Install the packages *stratisd* and *stratis-cli*:

![](images/00734.jpeg){.image2}

2[.]{.c19}Start the service and enable it to start automatically on
future system reboots:

![](images/00735.jpeg){.image2}

3[.]{.c19}Check the operational status of the service:

![](images/00736.jpeg){.image2}

The relevant packages for the Stratis storage management solution are
installed, and the Stratis service is started and activated. This
concludes the exercise.

**[Exercise]{#part0026_split_001.html#id_392 .calibre10} 14-8: Create
and Confirm a Pool and File System**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create a Stratis pool and a file system in
it. You will display information about the pool, file system, and device
used.

1[.]{.c19}You allocated 2x1GB disks: *sdg* and *sdh* for Stratis
exercises. Use the *lsblk* command to confirm their availability:

![](images/00737.jpeg){.image2}

The 2x1GB disks are available and unused.

2[.]{.c19}Create a pool called *bookpool* using the *sdg* disk and
verify the creation:

![](images/00738.jpeg){.image2}

The specified pool *bookpool* is created. The second command output
returns the basic information about the pool, including the number of
physical devices used in the pool and their sizes, and the amount of
in-use space in MiBs.

3[.]{.c19}Display the block device that is used to form the pool:

![](images/00739.jpeg){.image2}

The pool *bookpool* has a single disk of size 1GiB and it is currently
in use.

4[.]{.c19}Create a file system called *bookfs* in the *bookpool* and
verify the creation:

![](images/00740.jpeg){.image2}

The pool *bookpool* has a single file system *bookfs* of used size
546MiB. Its device file is **stratis*bookpool/bookfs*. The file system's
UUID is also displayed.

5[.]{.c19}Create a directory called */bookfs1* and mount the new file
system on it:

![](images/00741.jpeg){.image2}

6[.]{.c19}Check the pool usage:

![](images/00742.jpeg){.image2}

Observe how the usage grew from 52MiB to 598MiB. This brings this
exercise to an end.

**[Exercise]{#part0026_split_001.html#id_393 .calibre10} 14-9: Expand
and Rename a Pool and File System**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will expand the Stratis pool *bookpool* (created
in [Exercise 14-8](#part0026_split_001.html#id_392){.calibre5}) using
the *sdh* disk. You will rename the pool and the file system it
contains.

1[.]{.c19}Expand the *bookpool* pool by adding the *sdh* disk to it:

![](images/00743.jpeg){.image2}

2[.]{.c19}Verify the new capacity of the pool:

![](images/00744.jpeg){.image2}

The addition of another 1GB disk brings the total physical size to 2GiB.

3[.]{.c19}Change the name of the pool from *bookpool* to *rhcsapool* and
verify:

![](images/00745.jpeg){.image2}

The new name is depicted in the first column.

4[.]{.c19}Change the name of the file system from *bookfs* to *rhcsafs*,
and verify:

![](images/00746.jpeg){.image2}

The file system name is changed and it's reflected in the output of the
second command. The exercise is completed successfully.

**[Exercise]{#part0026_split_001.html#id_394 .calibre10} 14-10: Destroy
a File System and Pool**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will destroy the Stratis file system and the pool
that was created, expanded, and renamed in Exercises 14-8 and 14-9. You
will verify the deletion with appropriate commands.

1[.]{.c19}The first step in the process is to unmount the file system
*rhcsafs1* from its mount point */bookfs1*:

![](images/00747.jpeg){.image2}

Unmounting a file system ensures that the file system is not busy.

2[.]{.c19}Remove the file system *rhcsafs* from the pool:

![](images/00748.jpeg){.image2}

3[.]{.c19}Remove the pool *rhcsapool* from the system:

![](images/00749.jpeg){.image2}

4[.]{.c19}Confirm the removal of the file system and the pool:

![](images/00750.jpeg){.image2}

The above outputs confirm the removal of the file system and the pool.

5[.]{.c19}Verify that both *sdg* and *sdh* disks used in the previous
Stratis exercises have returned to their original raw state using the
*lsblk* command:

![](images/00751.jpeg){.image2}

This concludes the exercise.

We will recreate Stratis file systems in [Chapter
15](#part0027_split_000.html#page_345){.calibre5} and add them for
persistent mounting.

**[Chapter]{#part0026_split_001.html#id_395 .calibre10} Summary**

This chapter explicated two advanced storage management solutions:
Logical Volume Manager and Stratis. The LVM solution has been around in
RHEL for decades. The Stratis solution, on the other hand, is recently
added.

We discovered how LVM works. We looked at various LVM objects and their
relationship with one another. We explored LVM management commands and
common options available with them. We performed a series of exercises
to demonstrate the creation, expansion, renaming, reduction, and
deletion of physical volumes, storage pools, and logical volumes.

The next advanced storage solution we looked at is called Stratis, which
is a volume-managing file system. Stratis takes advantage of the
underlying LVM functionality and the device mapper kernel driver to
create and expand the XFS file system that it holds. Stratis dynamically
expands the underlying LVM volume without the need for manual
administrative intervention. We executed a couple of step-by-step
exercises to explain the creation, growth, renaming, and removal of
Stratis pools and file systems.

**[Review]{#part0026_split_001.html#id_396 .calibre10} Questions**

1[.]{.c19}The *parted* utility may be used to create LVM logical
volumes. True or False?

2[.]{.c19}What are the two commands that you can use to reduce the
number of logical extents from a logical volume?

3[.]{.c19}Stratis uses LVM as the underlying logical volume management
solution. True or False?

4[.]{.c19}Provide the command to add physical volumes **dev*sdd1* and
**dev*sdc* to *vg20* volume group.

5[.]{.c19}What are the two commands that you can use to add logical
extents to a logical volume?

6[.]{.c19}Provide the command to create a volume group called *vg20* on
**dev*sdd* disk with physical extent size 64MB.

7[.]{.c19}Name the three Stratis objects?

8[.]{.c19}Provide the command to remove *vg20* along with logical and
physical volumes it contains.

9[.]{.c19}What is the default size of a physical extent in LVM?

10[.]{.c23}What is the default name for the first logical volume in a
volume group?

11[.]{.c23}What is one difference between the *pvs* and *pvdisplay*
commands?

12[.]{.c23}When can a disk or partition be referred to as a physical
volume?

13[.]{.c23}Provide the command to remove *webvol* logical volume from
*vg20* volume group.

14[.]{.c23}It is necessary to create file system structures in a logical
volume before it can be used to store files in it. True or False?

15[.]{.c23}What would the command *stratis pool create pool1 *dev*sdg*
do?

16[.]{.c23}Physical and logical extents are typically of the same size.
True or False?

17[.]{.c23}What is the purpose of the *pvremove* command?

18[.]{.c23}What would the command *pvcreate *dev*sdd* do?

19[.]{.c23}A disk or partition can be added to a volume group without
being initialized. True or False?

20[.]{.c23}What is the file system type that is created in a Stratis
file system?

21[.]{.c23}Provide the command to create a logical volume called
*webvol* of size equal to 100 logical extents in *vg20* volume group.

22[.]{.c23}A volume group can be created without any physical volume in
it. True or False?

23[.]{.c23}A single disk can be used by both *parted* and LVM solutions
at the same time. True or False?

24[.]{.c23}Provide the command to add **dev*sdh* to an existing pool
called *pool1*.

25[.]{.c23}Provide the command to erase **dev*sdd1* physical volume from
*vg20* volume group.

26[.]{.c23}A partition can be used as an LVM object. True or False?

27[.]{.c23}Which command would you use to view the details of a volume
group and its objects?

**[Answers]{#part0026_split_001.html#id_397 .calibre10} to Review
Questions**

1[.]{.c19}False.

2[.]{.c19}The *lvreduce* and *lvresize* commands.

3[.]{.c19}True.

*4[.]{.c19}vgextend vg20 *dev*sdd1 *dev*sdc*

5[.]{.c19}The *lvextend* and *lvresize* commands.

*6[.]{.c19}vgcreate -s 64 vg20 *dev*sdd*

7[.]{.c19}The three Stratis objects are pool, block device, and file
system.

*8[.]{.c19}vgremove -f vg20*

9[.]{.c19}The default PE size is 4MB.

10[.]{.c23}*lvol0* is the default name for the first logical volume
created in a volume group.

11[.]{.c23}The *pvs* command lists basic information about physical
volumes whereas the *pvdisplay* command shows the details.

12[.]{.c23}After the *pvcreate* command has been executed on it
successfully.

*13[.]{.c23}lvremove *dev*vg20/webvol*

14[.]{.c23}True.

15[.]{.c23}The command provided will create a Stratis pool called
*pool1* containing **dev*sdg* device.

16[.]{.c23}True.

17[.]{.c23}The *pvremove* command is used to remove LVM information from
a physical volume.

18[.]{.c23}The command provided will prepare the **dev*sdd* disk for use
in a volume group.

19[.]{.c23}False. A disk or partition must be initialized before it can
be added to a volume group.

20[.]{.c23}XFS.

*21[.]{.c23}lvcreate -l 100 -n webvol vg20*

22[.]{.c23}False.

23[.]{.c23}True. A single disk can be shared between *parted*-created
partitions and LVM.

*24[.]{.c23}stratis pool add-data pool1 *dev*sdh*

*25[.]{.c23}vgreduce vg20 *dev*sdd1*

26[.]{.c23}True.

27[.]{.c23}The *vgdisplay* command with the -v option.

**[Do-]{#part0026_split_001.html#id_398 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

Add more storage to *server2* if required.

**[Lab]{#part0026_split_001.html#id_399 .calibre10} 14-1: Create Volume
Group and Logical Volumes**

As *user1* with *sudo* on *server2*, initialize 1x250MB disk for use in
LVM (use *lsblk* to identify available disks). Create volume group
*vg100* with PE size 16MB and add the physical volume. Create two
logical volumes *lvol0* and *swapvol* of sizes 100MB and 120MB. Use the
*vgs*, *pvs*, *lvs*, and *vgdisplay* commands for verification. (Hint:
Logical Volume Manager).

**[Lab]{#part0026_split_001.html#id_400 .calibre10} 14-2: Expand Volume
Group and Logical Volume**

As *user1* with *sudo* on *server2*, create a partition on an available
250MB disk and initialize it for use in LVM (use *lsblk* to identify
available disks). Add the new physical volume to *vg100*. Expand the
*lvol0* logical volume to size 300MB. Use the *vgs*, *pvs*, *lvs*, and
*vgdisplay* commands for verification. (Hint: Logical Volume Manager).

**[Lab]{#part0026_split_001.html#id_401 .calibre10} 14-3: Reduce and
Remove Logical Volumes**

As *user1* with *sudo* on *server2*, reduce the size of *lvol0* logical
volume to 80MB. Then erase both logical volumes *swapvol* and *lvol0*.
Confirm the deletion with *vgs*, *pvs*, *lvs*, and *vgdisplay* commands.
(Hint: Logical Volume Manager).

**[Lab]{#part0026_split_001.html#id_402 .calibre10} 14-4: Remove Volume
Group and Physical Volumes**

As *user1* with *sudo* on *server2*, remove the volume group and
uninitialized the physical volumes. Confirm the deletion with *vgs*,
*pvs*, *lvs*, and *vgdisplay* commands. Use the *lsblk* command and
verify that the disks used for the LVM labs no longer show LVM
information. (Hint: Logical Volume Manager).

**[Lab]{#part0026_split_001.html#id_403 .calibre10} 14-5: Create Stratis
Pool**

As *user1* with *sudo* on *server2*, check to see if Stratis software is
installed and the Stratis service is enabled and started. Identify 2x1GB
disks with the *lsblk* command and ensure they are not in use. Create
pool *strpool* on one of the 1GB disks and verify the pool and the block
device with the *stratis* command. (Hint: Stratis Volume-Managing File
System).

**[Lab]{#part0026_split_001.html#id_404 .calibre10} 14-6: Expand and
Destroy Stratis Pool**

As *user1* with *sudo* on *server2*, use the other 1GB disk and expand
*strpool* using the *stratis* command. Verify the expansion. Finally,
remove the entire pool and confirm the deletion. Use the *lsblk* command
and verify that the disks used for the Stratis labs no longer show
Stratis information. (Hint: Stratis Volume-Managing File System).

[]{#part0027_split_000.html}

## Chapter 15 {#part0027_split_000.html#calibre_pb_0 .calibre11}

**Local File Systems and Swap**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Understand file systems and their
benefits, categories, and types

[![](images/00001.jpeg){.c37}]{.c36}Review file system types: Ext3/Ext4,
XFS, VFAT, and ISO9660

[![](images/00001.jpeg){.c37}]{.c36}Know file system administration
commandset

[![](images/00001.jpeg){.c37}]{.c36}Mount and unmount file systems
manually and persistently

[![](images/00001.jpeg){.c37}]{.c36}Determine and use UUID

[![](images/00001.jpeg){.c37}]{.c36}Apply and use file system label

[![](images/00001.jpeg){.c37}]{.c36}Monitor file system and directory
usage

[![](images/00001.jpeg){.c37}]{.c36}Create and mount different types of
local file systems in partitions

[![](images/00001.jpeg){.c37}]{.c36}Create and mount XFS file system in
VDO volume

[![](images/00001.jpeg){.c37}]{.c36}Create, mount, and resize Ext4 and
XFS file systems in LVM

[![](images/00001.jpeg){.c37}]{.c36}Create, mount, and expand Stratis
file system

[![](images/00001.jpeg){.c37}]{.c36}Understand, create, and activate
swap in partitions and LVM

[RHCSA Objectives:]{.c39}

[31]{#part0027_split_000.html#id_797 .calibre10}[.]{.c29}Configure
systems to mount file systems at boot by Universally Unique ID (UUID) or
label

[32]{#part0027_split_000.html#id_798 .calibre10}[.]{.c29}Add new
partitions and logical volumes, and swap to a system nondestructively
(the first part of this objective is covered in more detail in Chapter
14)

[33]{#part0027_split_000.html#id_799 .calibre10}[.]{.c29}Create, mount,
unmount, and use vfat, ext4, and xfs file systems

35[.]{.c29}Extend existing logical volumes (more details in Chapter 14
also)

::: {#part0027_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0027_split_001.html}

[ F]{.c42}[ile]{.c43} systems are the most common structures created in
partitions and volumes regardless of the underlying storage management
solution employed. They are logical containers employed for file storage
and can be optimized, resized, mounted, and unmounted independently.
They must be connected to the root of the directory hierarchy in order
to be accessed by users and applications. This may be accomplished
automatically at system boot or manually when required. File systems can
be mounted or unmounted using their unique identifiers, labels, or
device files. There is a whole slew of commands available for file
system creation and administration; some of them are file system type
specific while others are general.

The other common structure created in partitions and logical volumes is
the swap space. Swapping provides a mechanism to move out and in pages
of idle data between the physical memory and the swap. Swap areas act as
extensions to the physical memory, and they may be activated or
deactivated independent of swap spaces located in other partitions and
volumes.

This chapter is the last one in the three chapter series (the other two
being [Chapters 13](#part0025_split_000.html#page_301){.calibre5} and
[14](#part0026_split_000.html#page_321){.calibre5}) on storage
management. It elaborates on file systems and swap, and demonstrates
their creation and management in several exercises. It also highlights
the tools to monitor their usage.

**[File]{#part0027_split_001.html#id_405 .calibre10} Systems and File
System Types**

A *file system* is a logical container that stores files and
directories. Each file system is created in a discrete partition, VDO
volume, logical volume, or Stratis pool. A typical production RHEL
system usually has numerous file systems. During OS installation, only
two file systems---*/* and */boot*---are created in the default disk
layout, but you can design a custom disk layout and construct separate
containers to store dissimilar information. Typical additional file
systems created during an installation are */home*, */opt*, */tmp*,
*/usr*, and */var*. The two mandatory file systems---*/* and
*/boot*---are required for installation and booting.

Storing disparate data in distinct file systems versus storing all data
in a single file system offers the following advantages:

[![](images/00562.jpeg){.image2}]{.c1568a}Make any file system
accessible (mount) or inaccessible (unmount) to users independent of
other file systems. This hides or reveals information contained in that
file system.

[![](images/00562.jpeg){.image2}]{.c1568a}Perform file system repair
activities on individual file systems

[![](images/00562.jpeg){.image2}]{.c1568a}Keep dissimilar data in
separate file systems

[![](images/00562.jpeg){.image2}]{.c1568a}Optimize or tune each file
system independently

[![](images/00562.jpeg){.image2}]{.c1568a}Grow or shrink a file system
independent of other file systems

RHEL supports several types of file systems that may be categorized in
three basic groups: *disk-based*, *network-based*, and *memory-based*.
Disk-based file systems are typically created on physical drives using
SATA, USB, Fibre Channel, and other technologies. Network-based file
systems are essentially disk-based file systems shared over the network
for remote access. Memory-based file systems are virtual; they are
created at system startup and destroyed when the system goes down.
Disk-based and network-based file systems store information
persistently, while any data saved in virtual file systems does not
survive across system reboots.

[Table 15-1](#part0027_split_001.html#id_743){.calibre5} lists and
explains various common disk-and network-based file system types
supported in RHEL 8.

::: c49
  ---------------------- -------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **File System Type**   **Category**   **Description**
  Ext3                   Disk           The third generation of the extended file system. It supports metadata journaling for faster recovery, offers superior reliability, allows the creation of up to 32,000 subdirectories, and supports larger file systems and bigger files than its predecessor.
  Ext4                   Disk           The fourth generation of the extended file system developed as the successor to Ext3. It supports all features of Ext3 in addition to a larger file system size, bigger file size, an unlimited number of subdirectories, metadata and quota journaling, and extended user attributes.
  XFS                    Disk           XFS is a highly scalable and high-performing 64-bit file system. It supports metadata journaling for faster crash recovery, and online defragmentation, expansion, quota journaling, and extended user attributes. XFS is the default file system type in RHEL 8.
  VFAT                   Disk           This file system is used for post-Windows 95 file system formats on hard disks, USB drives, and floppy disks.
  ISO9660                Disk           This is used for optical file systems such as CD and DVD.
  NFS                    Network        Network File System. A shared directory or file system for remote access by other Linux systems.
  AutoFS                 Network        Auto File System. An NFS file system set to mount and unmount automatically on remote client systems.
  ---------------------- -------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0027_split_001.html#id_743} 15-1 File System Types**
:::

This chapter covers Ext3, Ext4, XFS, and VFAT file systems at length. It
also touches upon mounting and unmounting ISO9660. For a brief
discussion on memory-based file systems, see [Chapter
02](#part0014_split_000.html#page_35){.calibre5} "Initial Interaction
with the System". NFS and AutoFS are discussed in [Chapter
17](#part0029_split_000.html#page_399){.calibre5} "Network File System".

**[Extended]{#part0027_split_001.html#id_406 .calibre10} File Systems**

*Extended file systems* have been part of RHEL for many years. The first
generation is obsolete and is no longer supported. The second, third,
and fourth generations are currently available and supported. The fourth
generation is the latest in the series and is superior in features and
enhancements to its predecessors.

The structure of an extended file system is built on a partition or
logical volume at the time of file system creation. This structure is
divided into two sets. The first set holds the file system's metadata
and it is very tiny. The second set stores the actual data, and it
occupies almost the entire partition or the logical volume (VDO, LVM,
and Stratis) space.

The metadata includes the *superblock*, which keeps vital file system
structural information, such as the type, size, and status of the file
system, and the number of data blocks it contains. Since the superblock
holds such critical information, it is automatically replicated and
maintained at various known locations throughout the file system. The
superblock at the beginning of the file system is referred to as the
*primary superblock*, and all of its copies as *backup superblocks*. If
the primary superblock is corrupted or lost, it renders the file system
inaccessible. One of the backup superblocks is then used to supplant the
corrupted or lost primary superblock to bring the file system back to
its normal state.

The metadata also contains the *inode table*, which maintains a list of
*index node* (*inode*) numbers. Each file is assigned an inode number at
the time of its creation, and the inode number holds the file's
attributes such as its type, permissions, ownership, owning group, size,
and last access/modification time. The inode also holds and keeps track
of the pointers to the actual data blocks where the file contents are
located.

The Ext3 and Ext4 file systems support a journaling mechanism that
provides them with the ability to recover swiftly after a system crash.
Both Ext3 and Ext4 file systems keep track of recent changes in their
metadata in a *journal* (or log). Each metadata update is written in its
entirety to the journal after completion. The system peruses the journal
of each extended file system following the reboot after a crash to
determine if there are any errors, and it recovers the file system
rapidly using the latest metadata information stored in its journal. The
ext2 file system does not support journaling, but the support for
journaling may be added to it if required.

In contrast to Ext3 that supports file systems up to 16TiB and files up
to 2TiB, Ext4 supports very large file systems up to 1EiB (ExbiByte) and
files up to 16TiB (TebiByte). Additionally, Ext4 uses a series of
contiguous physical blocks on the hard disk called *extents*, resulting
in improved read and write performance with reduced fragmentation. Ext4
supports extended user attributes, acl mount options (to support file
permission allocation to specific users and groups), as well as metadata
and quota journaling.

**[XFS]{#part0027_split_001.html#id_407 .calibre10} File System**

The *X File System* (XFS) is a high-performing 64-bit extent-based
journaling file system type. XFS allows the creation of file systems and
files up to 8EiB (ExbiByte). It does not run file system checks at
system boot; rather, it relies on you to use the *xfs_repair* utility to
manually fix any issues. XFS sets the extended user attributes and acl
mount options by default on new file systems. It enables defragmentation
on mounted and active file systems to keep as much data in contiguous
blocks as possible for faster access. The only major caveat with using
XFS is its inability to shrink.

Like Ext3 and Ext4, XFS also uses journaling for metadata operations,
guaranteeing the consistency of the file system against abnormal or
forced unmounting. The journal information is read and any pending
metadata transactions are replayed when the XFS file system is
remounted.

XFS uses sophisticated techniques in its architecture for speedy
input/output performance. It can be snapshot in a mounted, active state.
The snapshot can then be used for backup or other purposes.

**[VFAT]{#part0027_split_001.html#id_408 .calibre10} File System**

VFAT (*Virtual File Allocation Table*) is an extension to the legacy
*FAT* file system type, also called *FAT16*, that was introduced in
early versions of MS-DOS. The support for FAT16 was later added to
Microsoft Windows, MacOS, and some UNIX versions, enabling them to read
and write files written in that format. FAT16 had limitations; it was
designed to use no more than 8.3 characters in filenames, limiting
filenames to a maximum of eight characters plus three characters as an
extension. Moreover, it only allowed filenames to begin with a letter or
number and to not contain spaces. FAT16 treated lowercase and uppercase
letters alike.

VFAT was introduced with Microsoft Windows 95 and it has since been
available. It supports 255 characters in filenames including spaces and
periods; however, it still does not differentiate between lowercase and
uppercase letters. VFAT support was added to Linux several years ago. A
VFAT file system may be created on hard drives, but it is primarily used
on removable media, such as floppy and USB flash drives, for exchanging
data between Linux and Windows.

**[ISO]{#part0027_split_001.html#id_409 .calibre10}9660 File System**

This file system type conforms to the ISO 9660 standard, hence the name.
It is used for removable optical disc media such as CD/DVD drives for
transporting software and patches, and operating system images in ISO
format between computers. The ISO9660 format originated from the
*High-Sierra File System* (HSFS) format, and it has now been enhanced to
include innovative features.

**[File]{#part0027_split_001.html#id_410 .calibre10} System Management**

Managing file systems involves such operations as creating, mounting,
labeling, viewing, growing, shrinking, unmounting, and removing them.
These management tasks are common to both Extended and XFS types. Most
of these functions are also applicable to VFAT and a few to optical file
systems.

In [Chapters 13](#part0025_split_000.html#page_301){.calibre5} and
[14](#part0026_split_000.html#page_321){.calibre5}, you created several
partitions, VDO volumes, LVM logical volumes, and Stratis volumes.
However, you did not initialize them with a file system type (except for
Stratis), and therefore you could not mount or use them. Later, you
destroyed all the partitions and the volumes that were created. You also
deleted the LVM volume group and the Stratis pool. All the disks were
returned to their unused state after the completion of the exercises.

Here is a listing of the block devices to confirm the current state of
the disks:

![](images/00752.jpeg){.image2}

The output verifies the unused state and availability status for all the
disks---*sdb* through *sdh*. You should be able to reuse them in the
exercises in this chapter.

**[File]{#part0027_split_001.html#id_411 .calibre10} System
Administration Commands**

In order to create and manage file systems, RHEL offers a number of
commands of which some are limited to their operations on the Extended,
XFS, or VFAT file system type, while others are general and applicable
to all file system types. [Table
15-2](#part0027_split_001.html#id_744){.calibre5} describes common file
system administration commands.

::: c49
  ---------------------------------- -------------------------------------------------------------------------------------------------------------------
  **Command**                        **Description**
  **Extended File System**           
  e2label                            Modifies the label of a file system
  mke2fs                             Creates a file system. Can also be invoked as mkfs.ext3, mkfs.ext4, mkfs -t ext3, and mkfs -t ext4.
  resize2fs                          Resizes a file system. This command is automatically invoked when the lvresize command is run with the -r switch.
  tune2fs                            Tunes or displays file system attributes
  **XFS**                            
  mkfs.xfs                           Creates a file system. Can also be invoked as mkfs -t xfs.
  xfs_admin                          Tunes file system attributes
  xfs_growfs                         Extends the size of a file system
  xfs_info                           Exhibits information about a file system
  **VFAT**                           
  mkfs.vfat                          Creates a file system. It is equivalent to using mkfs -t vfat.
  **General File System Commands**   
  blkid                              Displays block device attributes including their UUIDs and labels
  df                                 Reports file system utilization
  du                                 Calculates disk usage of directories and file systems
  lsblk                              Lists block devices and file systems and their attributes including their UUIDs and labels
  mount                              Mounts a file system for user access. Displays currently mounted file systems.
  umount                             Unmounts a file system
  ---------------------------------- -------------------------------------------------------------------------------------------------------------------

**[Table]{#part0027_split_001.html#id_744} 15-2 File System Management
Commands**
:::

Most of these commands are used in this chapter.

**[Mounting]{#part0027_split_001.html#id_412 .calibre10} and Unmounting
File Systems**

In order to enable users to access files and application programs in a
file system, the file system must be connected to the directory
structure at a desired attachment point, which is referred to as the
*mount point*. A mount point in essence is any empty directory that is
created and used for this purpose.

There are many file systems already mounted on your system, such as the
root file system mounted on */* and the boot file system mounted on
*/boot*. Both of them are empty directories and are reserved to connect
the two file systems to the directory hierarchy. You can use the *mount*
command to view information about mounted file systems. The following
shows the XFS file systems only:

![](images/00753.jpeg){.image2}

The "-t xfs" option makes the command to only show the file systems
initialized with the XFS type.

The *mount* command is also used for mounting a file system to a mount
point, and this action is performed with the *root* user privileges. The
command requires the absolute pathnames of the file system block device
and the mount point name. It also accepts the UUID or label of the file
system in lieu of the block device name. Options are available with this
command to mount all or a specific type of file system. The *mount*
command is also used to mount other types of file systems such as those
located in removable media. Upon successful mount, the kernel places an
entry for the file system in the **proc*self/mounts* file.

![](images/00002.jpeg){.image} A mount point should be empty when an
attempt is made to mount a file system on it, otherwise the content of
the mount point will hide. As well, the mount point must not be in use
or the mount attempt will fail.

The *mount* command supports numerous options that may be used as
required to override its default behavior. We can also specify multiple
comma-separated options. [Table
15-3](#part0027_split_001.html#id_745){.calibre5} describes some common
options.

::: c49
  --------------- ------------------------------------------------------------------------------------------------------------------------
  **Option**      **Description**
  acl (noacl)     Enables (disables) the support for ACLs
  auto (noauto)   Mounts (does not mount) the file system when the -a option is specified
  defaults        Mounts a file system with all the default values (async, auto, rw, etc.)
  \_netdev        Used for a file system that requires network connectivity in place before it can be mounted. VDO and NFS are examples.
  remount         Remounts an already mounted file system to enable or disable an option
  ro (rw)         Mounts a file system read-only (read/write)
  --------------- ------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0027_split_001.html#id_745} 15-3 Common mount Command
Options**
:::

The opposite of the *mount* command is *umount*, which is used to detach
a file system from the directory hierarchy and make it inaccessible to
users and applications. This command expects the absolute pathname to
the block device containing the file system or its mount point name in
order to detach it. Options are available with *umount* to unmount all
or a specific type of file system. The kernel removes the corresponding
file system entry from the **proc*self/mounts* file after it has been
successfully disconnected.

**[Determining]{#part0027_split_001.html#id_413 .calibre10} the UUID of
a File System**

Every Extended and XFS file system has a 128-bit (32 hexadecimal
characters) UUID (*Universally Unique IDentifier*) assigned to it at the
time of its creation. In contrast, UUIDs assigned to vfat file systems
are 32-bit (8 hexadecimal characters) in length. Assigning a UUID makes
the file system unique among many other file systems that potentially
exist on the system. The primary benefit of using a UUID is the fact
that it always stays persistent across system reboots. A UUID is used by
default in RHEL 8 in the **etc*fstab* file for any file system that is
created by the system in a standard partition.

![](images/00002.jpeg){.image} RHEL attempts to mount all file systems
listed in the **etc*fstab* file at reboots. Each file system has an
associated device file and UUID, but may or may not have a corresponding
label. The system checks for the presence of each file system's device
file, UUID, or label, and then attempts to mount it.

The */boot* file system, for instance, is located in a partition and the
device file associated with it is on *server2* is **dev*sda1*. You can
use the *xfs_admin* command, the *blkid* command, or the *lsblk* command
as follows to determine its UUID:

![](images/00754.jpeg){.image2}

The UUID reported by the above commands for the */boot* file system is
\"c1ff315e-4320-442c-a3c5-36db403b53f2\". If you *grep* for the string
"boot" on the **etc*fstab* file, you will see that the system uses this
UUID to mount */boot*. A discussion on the **etc*fstab* file is provided
later in this chapter.

![](images/00002.jpeg){.image} For extended file systems, you can use
the *tune2fs* command in addition to the *blkid* and *lsblk* commands to
determine the UUID.

[**EXAM TIP:**]{.c56} Knowing how to find the UUID of a file system
created in a standard partition or with Stratis is important.

A UUID is also assigned to a file system that is created in a VDO or LVM
volume; however, it need not be used in the *fstab* file, as the device
files associated with the logical volumes are always unique and
persistent.

**[Labeling]{#part0027_split_001.html#id_414 .calibre10} a File System**

A unique label may be used instead of a UUID to keep the file system
association with its device file exclusive and persistent across system
reboots. A label is limited to a maximum of 12 characters on the XFS
file system and 16 characters on the Extended file system. By default,
no labels are assigned to a file system at the time of its creation.

The */boot* file system is located in the **dev*sda1* partition and its
type is XFS. You can use the *xfs_admin* command, the *blkid* command,
or the *lsblk* command as follows to determine its label:

![](images/00755.jpeg){.image2}

The output discloses that there is currently no label assigned to the
*/boot* file system.

A label is not needed on a file system if you intend to use its UUID or
if it is created in a VDO or LVM logical volume; however, you can still
apply one using the *xfs_admin* command with the -L option. Labeling an
XFS file system requires that the target file system be unmounted.

The following example demonstrates the steps to unmount */boot*, set the
label "bootfs" on its device file, and remount it:

![](images/00756.jpeg){.image2}

You can confirm the new label by executing **sudo xfs_admin -l
*dev*sda1**, **sudo blkid *dev*sda1**, or **sudo lsblk -f *dev*sda1**.

![](images/00002.jpeg){.image} For extended file systems, you can use
the *e2label* command to apply a label and the *tune2fs*, *blkid*, and
*lsblk* commands to view and verify.

Now you can replace the UUID=\"c1ff315e-4320-442c-a3c5-36db403b53f2\"
for */boot* in the *fstab* file with LABEL=bootfs, and unmount and
remount */boot* as demonstrated above for confirmation.

A label may also be applied to a file system created in a VDO or LVM
volume; however, it is not recommended for use in the *fstab* file, as
the device files for these logical volumes are always unique and remain
persistent across system reboots.

**[Automatically]{#part0027_split_001.html#id_415 .calibre10} Mounting a
File System at Reboots**

File systems defined in the **etc*fstab* file are mounted automatically
at reboots. This file must contain proper and complete information for
each listed file system. An incomplete or inaccurate entry might leave
the system in an undesirable or unbootable state. Another benefit of
adding entries to this file is that you only need to specify one of the
four attributes---block device name, UUID, label, or mount point---of
the file system that you wish to mount manually with the *mount*
command. The *mount* command obtains the rest of the information from
this file. Similarly, you only need to specify one of these attributes
with the *umount* command to detach it from the directory hierarchy.

The default *fstab* file contains entries for file systems that are
created at the time of installation. On *server2*, for instance, this
file currently has the following three entries:

![](images/00757.jpeg){.image2}

[**EXAM TIP:**]{.c56} Any missing or invalid entry in this file may
render the system unbootable. You will have to boot the system in
emergency mode to fix this file. Ensure that you understand each field
in the file for both file system and swap entries.

The format of this file is such that each row is broken out into six
columns to identify the required attributes for each file system to be
successfully mounted. Here is what the columns contain:

**Column 1:** Defines the physical or virtual device path where the file
system is resident, or its associated UUID or label. There can be
entries for network file systems here as well.

**Column 2:** Identifies the mount point for the file system. For swap
partitions, use either "none" or "swap".

**Column 3:** Specifies the type of file system such as Ext3, Ext4, XFS,
VFAT, or ISO9660. For swap, the type "swap" is used. You may use "auto"
instead to leave it up to the *mount* command to determine the type of
the file system.

**Column 4:** Identifies one or more comma-separated options to be used
when mounting the file system. See [Table
15-3](#part0027_split_001.html#id_745){.calibre5} for a description of
some of the options, consult the manual pages of the *mount* command or
the *fstab* file for additional options and details.

**Column 5:** Is used by the *dump* utility to ascertain the file
systems that need to be dumped. A value of 0 (or the absence of this
column) disables this check. This field is applicable only on Extended
file systems; XFS does not use it.

**Column 6:** Expresses the sequence number in which to run the *e2fsck*
(file system check and repair utility for Extended file system types)
utility on the file system at system boot. By default, 0 is used for
memory-based, remote, and removable file systems, 1 for */*, and 2 for
*/boot* and other physical file systems. 0 can also be used for */*,
*/boot*, and other physical file systems you don't want to be checked or
repaired. This field is applicable only on Extended file systems; XFS
does not use it.

![](images/00002.jpeg){.image} A 0 in columns 5 and 6 for XFS, virtual,
remote, and removable file system types has no meaning. You do not need
to add them for these file system types.

This file is edited manually, so care must be observed to circumvent
syntax and typing errors.

**[Monitoring]{#part0027_split_001.html#id_416 .calibre10} File System
Usage**

On a live system, you'll often need to check file system usage to know
if a mounted file system requires an expansion for growth or a clean up
to generate free space. This involves examining the used and available
spaces for a file system. The *df* (*disk free*) command has been used
for this purpose. It reports usage details for mounted file systems. By
default, this command reports the numbers in KBs unless the -m or -h
option is specified to view the sizes in MBs or human-readable format.

![](images/00002.jpeg){.image} This command may not produce correct
information for VDO and Stratis file systems. Use their own tools for
viewing usage.

Let's run this command with the -h option on *server2*:

![](images/00758.jpeg){.image2}

The output shows the file system device file or type in column 1,
followed by the total, used, and available spaces in columns 2, 3, and
4, and then the usage percentage and mount point in columns 5 and 6.

There are a few other useful flags available with the *df* command that
can produce the desired output. These flags include:

-T to add the file system type to the output (example: **df -hT**)

-x to exclude the specified file system type from the output (example:
**df -hx tmpfs**)

-t to limit the output to a specific file system type (example: **df -t
xfs**)

-i to show inode information (example: **df -hi**)

You may use -h with any of these examples to print information in
human-readable format.

**[Calculating]{#part0027_split_001.html#id_417 .calibre10} Disk Usage**

In contrast to the *df* command that returns usage information for an
entire file system, the *du* command reports the amount of space a file
or directory occupies. By default, it shows the output in KBs; however,
you can use the -m or -h option to view the output in MBs or
human-readable format. In addition, you can view a usage summary with
the -s switch and a grand total with -c.

Let's run this command on the **usr*bin* directory to view the usage
summary:

![](images/00759.jpeg){.image2}

To add a "total" row to the output and with numbers displayed in KBs:

![](images/00760.jpeg){.image2}

Try this command with different options on the **usr*sbin/lvm* file and
observe the results.

**Exercise 15-1: Create and Mount Ext4, VFAT, and XFS File Systems in
Partitions**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create 2 x 100MB partitions on the **dev*sdb*
disk, initialize them separately with the Ext4 and VFAT file system
types, define them for persistence using their UUIDs, create mount
points called */ext4fs1* and */vfatfs1*, attach them to the directory
structure, and verify their availability and usage. Moreover, you will
use the disk **dev*sdc* and repeat the above procedure to establish an
XFS file system in it and mount it on */xfsfs1*.

1[.]{.c19}Apply the label "msdos" to the *sdb* disk using the *parted*
command:

![](images/00761.jpeg){.image2}

2[.]{.c19}Create 2 x 100MB primary partitions on *sdb* with the *parted*
command:

![](images/00762.jpeg){.image2}

3[.]{.c19}Initialize the first partition (*sdb1*) with Ext4 file system
type using the *mkfs* command:

![](images/00763.jpeg){.image2}

4[.]{.c19}Initialize the second partition (*sdb2*) with VFAT file system
type using the *mkfs* command:

![](images/00764.jpeg){.image2}

5[.]{.c19}Initialize the whole disk (*sdc*) with the XFS file system
type using the *mkfs.xfs* command. Add the -f flag to force the removal
of any old partitioning or labeling information from the disk.

![](images/00765.jpeg){.image2}

6[.]{.c19}Determine the UUIDs for all three file systems using the
*lsblk* command:

![](images/00766.jpeg){.image2}

7[.]{.c19}Open the **etc*fstab* file, go to the end of the file, and
append entries for the file systems for persistence using their UUIDs:

![](images/00767.jpeg){.image2}

8[.]{.c19}Create mount points */ext4fs1*, */vfatfs1*, and */xfsfs1* for
the three file systems using the *mkdir* command:

![](images/00768.jpeg){.image2}

9[.]{.c19}Mount the new file systems using the *mount* command. This
command will fail if there are any invalid or missing information in the
file.

![](images/00769.jpeg){.image2}

10[.]{.c23}View the mount and availability status as well as the types
of all three file systems using the *df* command:

![](images/00770.jpeg){.image2}

The output verifies the creation and availability status of the three
file systems. They are added to the *fstab* file for persistence. A
system reboot at this point will remount them automatically. These file
systems may now be used to store files.

**[Exercise]{#part0027_split_001.html#id_418 .calibre10} 15-2: Create
and Mount XFS File System in VDO Volume**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create a VDO volume called *vdo1* of logical
size 16GB on the *sdf* disk (the actual size of this disks is 4GB). You
will initialize the volume with the XFS file system type, define it for
persistence using its device files, create a mount point called
*/xfsvdo1*, attach it to the directory structure, and verify its
availability and usage.

Prior to proceeding, ensure that the steps outlined in [Exercise
13-6](#part0025_split_001.html#id_370){.calibre5} for VDO software
installation and service startup have been accomplished.

1[.]{.c19}Create a VDO volume *vdo1* on the *sdf* disk with a logical
size of 16GB and a slab size of 128MB:

![](images/00771.jpeg){.image2}

2[.]{.c19}List the new VDO volume using the *vdo* and *lsblk* commands:

![](images/00772.jpeg){.image2}

The output shows the logical volume size (16GB), type (vdo), and the
actual size (4GB) of the underlying disk.

3[.]{.c19}Initialize the VDO volume with the XFS file system type with
the *mkfs.xfs* command. The VDO volume device file is **dev*mapper/vdo1*
as indicated in the output in step 1. Add the -f flag to force the
removal of any old partitioning or labeling information from the disk.

![](images/00773.jpeg){.image2}

4[.]{.c19}Open the **etc*fstab* file, go to the end of the file, and
append the following entry for the file system for persistence using its
device file:

![](images/00774.jpeg){.image2}

Make sure to include the option **x-systemd.requires=vdo.service** (or
try \_netdev instead) with the file system entry in the *fstab* file or
your system will land into the emergency target on the next reboot. This
option holds the *mount* command from mounting this file system until
the *vdo.service* service has been operational.

5[.]{.c19}Create the mount point */xfsvdo1* using the *mkdir* command:

![](images/00775.jpeg){.image2}

6[.]{.c19}Mount the new file system using the *mount* command. This
command will fail if there are any invalid or missing information in the
file.

![](images/00776.jpeg){.image2}

The *mount* command with the -a flag is a validation test for the
*fstab* file. It should always be executed after updating this file and
before rebooting the server to avoid landing the system in an unbootable
state.

7[.]{.c19}View the mount and availability status as well as the types of
the VDO file system using the *lsblk* and *df* commands:

![](images/00777.jpeg){.image2}

The *lsblk* command output illustrates the VDO volume name (*vdo1*) ,
the disk it is located on (*sdf*), the actual (4GB) and logical (16GB)
sizes, and the mount point (*/xfsvdo1*) where the file system is
connected to the directory structure.

The *df* command shows the logical size of the file system and its usage
status, but it does not reveal the underlying disk information. This
file system is added to the *fstab* file for persistence, meaning a
future system reboot will remount it automatically. This file system may
now be used to store files.

Refer to [Chapter 13](#part0025_split_000.html#page_301){.calibre5}
"Basic Storage Partitioning" for details on VDO.

**[Exercise]{#part0027_split_001.html#id_419 .calibre10} 15-3: Create
and Mount Ext4 and XFS File Systems in LVM Logical Volumes**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create a volume group called *vgfs* comprised
of a 160MB physical volume created in a partition on the **dev*sdd*
disk. The PE size for the volume group should be set at 16MB. You will
create two logical volumes called *ext4vol* and *xfsvol* of sizes 80MB
each and initialize them with the Ext4 and XFS file system types. You
will ensure that both file systems are persistently defined using their
logical volume device filenames. You will create mount points called
*/ext4fs2* and */xfsfs2*, mount the file systems, and verify their
availability and usage.

1[.]{.c19}Create a 150MB partition on the *sdd* disk using the *parted*
command:

![](images/00778.jpeg){.image2}

2[.]{.c19}Initialize the *sdd1* partition for use in LVM using the
*pvcreate* command:

![](images/00779.jpeg){.image2}

3[.]{.c19}Create the volume group *vgfs* with a PE size of 16MB using
the physical volume *sdd1*:

![](images/00780.jpeg){.image2}

The PE size is not easy to alter after a volume group creation, so
ensure it is defined as required at creation.

4[.]{.c19}Create two logical volumes *ext4vol* and *xfsvol* of size 80MB
each in *vgfs* using the *lvcreate* command:

![](images/00781.jpeg){.image2}

5[.]{.c19}Format the *ext4vol* logical volume with the Ext4 file system
type using the *mkfs.ext4* command:

![](images/00782.jpeg){.image2}

You may alternatively use **sudo mkfs -t ext4 *dev*vgfs/ext4vol**.

6[.]{.c19}Format the *xfsvol* logical volume with the XFS file system
type using the *mkfs.xfs* command:

![](images/00783.jpeg){.image2}

You may use **sudo mkfs -t xfs *dev*vgfs/xfsvol** instead.

7[.]{.c19}Open the **etc*fstab* file, go to the end of the file, and
append entries for the file systems for persistence using their device
files:

![](images/00784.jpeg){.image2}

8[.]{.c19}Create mount points */ext4fs2* and */xfsfs2* using the *mkdir*
command:

![](images/00785.jpeg){.image2}

9[.]{.c19}Mount the new file systems using the *mount* command. This
command will fail if there is any invalid or missing information in the
file.

![](images/00786.jpeg){.image2}

Fix any issues in the file if reported.

10[.]{.c23}View the mount and availability status as well as the types
of the new LVM file systems using the *lsblk* and *df* commands:

![](images/00787.jpeg){.image2}

The *lsblk* command output illustrates the LVM logical volumes
(*ext4vol* and *xfsvol*), the disk they are located on (*sdd*), the
sizes (80MB), and the mount points (*/ext4fs2* and */xfsfs2*) where the
file system are connected to the directory structure.

The *df* command shows the size and usage information. Both file systems
are added to the *fstab* file for persistence, meaning future system
reboots will remount them automatically. They may now be used to store
files.

**[Exercise]{#part0027_split_001.html#id_420 .calibre10} 15-4: Resize
Ext4 and XFS File Systems in LVM Logical Volumes**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will grow the size of the *vgfs* volume group that
was created in [Exercise
15-3](#part0027_split_001.html#id_419){.calibre5} by adding the whole
*sde* disk to it. You will extend the *ext4vol* logical volume along
with the file system it contains by 40MB using two separate commands.
You will extend the *xfsvol* logical volume along with the file system
it contains by 40MB using a single command. You will verify the new
extensions.

1[.]{.c19}Initialize the *sde* disk and add it to the *vgfs* volume
group:

![](images/00788.jpeg){.image2}

2[.]{.c19}Confirm the new size of *vgfs* using the *vgs* and *vgdisplay*
commands:

![](images/00789.jpeg){.image2}

There are now two physical volumes in the volume group and the total
size increased to 400MiB.

3[.]{.c19}Grow the logical volume *ext4vol* and the file system it holds
by 40MB using the *lvextend* and *fsadm* command pair. Make sure to use
an uppercase L to specify the size. The default unit is MiB. The plus
sign (+) signifies an addition to the current size.

![](images/00790.jpeg){.image2}

The *resize* subcommand instructs the *fsadm* command to grow the file
system to the full length of the specified logical volume.

4[.]{.c19}Grow the logical volume *xfsvol* and the file system (-r) it
holds by (+) 40MB using the *lvresize* command:

![](images/00791.jpeg){.image2}

5[.]{.c19}Verify the new extensions to both logical volumes using the
*lvs* command. You may also issue the *lvdisplay* or *vgdisplay* command
instead.

![](images/00792.jpeg){.image2}

6[.]{.c19}Check the new sizes and the current mount status for both file
systems using the *df* and *lsblk* commands:

![](images/00793.jpeg){.image2}

The outputs reflect the new sizes (128MB) for both file systems. They
also indicate their mount status.

This concludes the exercise.

**Exercise 15-5: Create, Mount, and Expand XFS File System in Stratis
Volume**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create a Stratis pool called *strpool* and a
file system (*strfs2*) in it (the file system type will be XFS) by
reusing the 1GB *sdg* disk from [Chapter
14](#part0026_split_000.html#page_321){.calibre5}. You will display
information about the pool, file system, and device used. You will
expand the pool to include another 1GB disk *sdh* and confirm.

For further details on Stratis storage management solution, refer to
[Chapter 14](#part0026_split_000.html#page_321){.calibre5} "Advanced
Storage Partitioning". Prior to proceeding, ensure that the steps
outlined in [Exercise 14-7](#part0026_split_001.html#id_391){.calibre5}
for Stratis software installation and service startup have been
accomplished.

1[.]{.c19}Create a pool called *strpool* using the *sdg* disk and verify
the creation:

![](images/00794.jpeg){.image2}

The pool is created with a single disk included, and confirmed.

2[.]{.c19}Show the block device used in the pool:

![](images/00795.jpeg){.image2}

3[.]{.c19}Create a file system called *strfs2* in the pool and verify
the creation:

![](images/00796.jpeg){.image2}

4[.]{.c19}Determine the UUID of the new file system to be added to the
*fstab* file:

![](images/00797.jpeg){.image2}

5[.]{.c19}Open the **etc*fstab* file, go to the end of the file, and
append the following entry for the file system for persistence using the
UUID:

![](images/00798.jpeg){.image2}

Make sure to include the option **x-systemd.requires=stratisd.service**
(or try \_netdev instead) with the file system entry in the *fstab* file
or your system will land into the emergency target on the next reboot.
This option holds the *mount* command from mounting this file system
until the *stratisd.service* service has been started successfully.

6[.]{.c19}Create the mount point */strfs2*:

![](images/00799.jpeg){.image2}

7[.]{.c19}Mount the new file system using the *mount* command as
follows. This command will fail if there are any invalid or missing
information in the file.

![](images/00800.jpeg){.image2}

8[.]{.c19}Check the pool usage:

![](images/00801.jpeg){.image2}

9[.]{.c19}Check the file system usage:

![](images/00802.jpeg){.image2}

10[.]{.c23}Grow the pool by adding the *sdh* disk to it and confirm the
growth:

![](images/00803.jpeg){.image2}

The file system *strfs2* will automatically expand to take advantage of
the additional pool capacity when required.

This concludes the exercise.

**[Swap]{#part0027_split_001.html#id_421 .calibre10} and its
Management**

Physical memory (or main memory) in the system is a finite temporary
storage resource employed for loading kernel and running user programs
and applications. *Swap space* is an independent region on the physical
disk used for holding idle data until it is needed. The system splits
the physical memory into small logical chunks called *pages* and maps
their physical locations to virtual locations on the swap to facilitate
access by system processors. This physical-to-virtual mapping of pages
is stored in a data structure called *page table*, and it is maintained
by the kernel.

When a program or process is spawned, it requires space in the physical
memory to run and be processed. Although many programs can run
concurrently, the physical memory cannot hold all of them at once. The
kernel monitors the memory usage. As long as the free memory remains
below a high threshold, nothing happens. However, when the free memory
falls below that threshold, the system starts moving selected idle pages
of data from physical memory to the swap space in an effort to make room
to accommodate other programs. This piece in the process is referred to
as *page out*. Since the system CPU performs the process execution in a
round-robin fashion, when the system needs this paged-out data for
execution, the CPU looks for that data in the physical memory and a
*page fault* occurs, resulting in moving the pages back to the physical
memory from the swap. This return of data to the physical memory is
referred to as *page in*. The entire process of paging data out and in
is known as *demand paging*.

RHEL systems with less physical memory but high memory requirements can
become over busy with paging out and in. When this happens, they do not
have enough cycles to carry out other useful tasks, resulting in
degraded system performance. The excessive amount of paging that affects
the system performance is called *thrashing*.

When thrashing begins, or when the free physical memory falls below a
low threshold, the system deactivates idle processes and prevents new
processes from being launched. The idle processes are only reactivated
and new processes are only allowed to be started when the system
discovers that the available physical memory has climbed above the
threshold level and thrashing has ceased.

**[Determining]{#part0027_split_001.html#id_422 .calibre10} Current Swap
Usage**

The size of a swap area should not be less than the amount of physical
memory; however, depending on workload requirements, it may be twice the
size or larger. It is also not uncommon to see systems with less swap
than the actual amount of physical memory. This is especially witnessed
on systems with a huge physical memory size.

RHEL offers the *free* command to view memory and swap space
utilization. Use this command to view how much physical memory is
installed (total), used (used), available (free), used by shared library
routines (shared), holding data before it is written to disk (buffers),
and used to store frequently accessed data (cached) on the system. The
-h flag may be specified with the command to list the values in
human-readable format, otherwise -k for KB, -m for MB, -g for GB, and so
on are also supported. Add -t with the command to display a line with
the "total" at the bottom of the output. Here is a sample output from
*server2*:

![](images/00804.jpeg){.image2}

The output indicates that the system has 1.8GiB of total memory of which
895MiB is in use and 447MiB is free. It also shows on the same line the
current memory usages by temporary (tmpfs) file systems (20MiB) and
kernel buffers and page cache (486MiB). It also illustrates an estimate
of free memory available to start new processes (749MiB).

On the subsequent row, it reports the total swap space (1.0GiB)
configured on the system with a look at used (0 Bytes) and free (1.0GiB)
space. The last line prints the combined usage summary of the main
memory and swap.

Try **free -hts 3** and **free -htc 2** to refresh the output every
three seconds (-s) and to display the output twice (-c).

The *free* command reads memory and swap information from the
**proc*meminfo* file to produce the report. The values are shown in KBs
by default, and they are slightly off from what is shown in the above
screenshot with *free*. Here are the relevant fields from this file:

![](images/00805.jpeg){.image2}

This data depicts the system's runtime memory and swap usage, as it is
located in a virtual file.

**[Prioritizing]{#part0027_split_001.html#id_423 .calibre10} Swap
Spaces**

On many production RHEL servers, you may find multiple swap areas
configured and activated to meet the workload demand. The default
behavior of RHEL is to use the first activated swap area and move on to
the next when the first one is exhausted. The system allows us to
prioritize one area over the other by adding the option "pri" to the
swap entries in the *fstab* file. This flag supports a value between -2
and 32767 with -2 being the default. A higher value of "pri" sets a
higher priority for the corresponding swap region. For swap areas with
an identical priority, the system alternates between them.

**[Swap]{#part0027_split_001.html#id_424 .calibre10} Administration
Commands**

In order to create and manage swap spaces on the system, the *mkswap*,
*swapon*, and *swapoff* commands are available. Use *mkswap* to
initialize a partition for use as a swap space. Once the swap area is
ready, you can activate or deactivate it from the command line with the
help of the other two commands, or set it up for automatic activation by
placing an entry in the *fstab* file. The *fstab* file accepts the swap
area's device file, UUID, or label.

**[Exercise]{#part0027_split_001.html#id_425 .calibre10} 15-6: Create
and Activate Swap in Partition and Logical Volume**

This exercise should be done on *server2* as *user1* with *sudo* where
required.

In this exercise, you will create one swap area in a new 40MB partition
called *sdb3* using the *mkswap* command. You will create another swap
area in a 140MB logical volume called *swapvol* in *vgfs*. You will add
their entries to the **etc*fstab* file for persistence. You will use the
UUID and priority 1 for the partition swap and the device file and
priority 2 for the logical volume swap. You will activate them and use
appropriate tools to validate the activation.

[**EXAM TIP:**]{.c56} Use the lsblk command to determine available disk
space.

1[.]{.c19}Use the *parted*'s *print* subcommand on the *sdb* disk and
the *vgs* command on the *vgfs* volume group to determine available
space for a new 40MB partition and a 144MB logical volume:

![](images/00806.jpeg){.image2}

The outputs show 49MB (250MB minus 201MB) free space on the *sdb* disk
and 144MB free space in the volume group.

2[.]{.c19}Create partition called *sdb3* of size 40MB using the *parted*
command:

![](images/00807.jpeg){.image2}

3[.]{.c19}Create logical volume *swapvol* of size 144MB in *vgs* using
the *lvcreate* command:

![](images/00808.jpeg){.image2}

4[.]{.c19}Construct swap structures in *sdb3* and *swapvol* using the
*mkswap* command:

![](images/00809.jpeg){.image2}

5[.]{.c19}Edit the *fstab* file and add entries for both swap areas for
auto-activation on reboots. Obtain the UUID for partition swap with
**lsblk -f *dev*sdb3** and use the device file for logical volume.
Specify their priorities.

![](images/00810.jpeg){.image2}

[**EXAM TIP:**]{.c56} You will not be given any credit for this work if
you forget to add entries to the fstab file.

6[.]{.c19}Determine the current amount of swap space on the system using
the *swapon* command:

![](images/00811.jpeg){.image2}

There is one 1GB swap area on the system configured at the default
priority of -2.

7[.]{.c19}Activate the new swap regions using the *swapon* command:

![](images/00812.jpeg){.image2}

The command would display errors if there were any issues with swap
entries in the *fstab* file.

8[.]{.c19}Confirm the activation using the *swapon* command or by
viewing the **proc*swaps* file:

![](images/00813.jpeg){.image2}

The two new swap regions are activated and listed in the above outputs.
Their sizes and priorities are also visible. The device mapper device
files for the logical volumes and the device file for the partition swap
are also exhibited.

9[.]{.c19}Issue the *free* command to view the reflection of swap
numbers on the Swap and Total lines:

![](images/00814.jpeg){.image2}

The total swap is now 1.2GiB. This concludes the exercise.

**[Chapter]{#part0027_split_001.html#id_426 .calibre10} Summary**

This chapter covered two major storage topics: file systems and swap.
These structures are created in partitions or logical volumes
irrespective of the underlying storage management solution used to build
them.

The chapter began with a detailed look at the concepts, categories,
benefits, and types of file systems. We reviewed file system
administration and monitoring utilities. We discussed the concepts
around mounting and unmounting file systems. We examined the UUID
associated with file systems and applied labels to file systems. We
analyzed the file system table and added entries for auto-activating
file systems at reboots. We explored tools for reporting file system
usage and calculating disk usage. We performed a number of exercises on
file system creation and administration in partitions and VDO, LVM, and
Stratis volumes to reinforce the concepts and theory learned in this and
the last two chapters.

We touched upon the concepts of swapping and paging, and studied how
they work. We performed exercises on creating, activating, viewing,
deactivating, and removing swap spaces, as well as configuring them for
auto-activation at system reboots.

**[Review]{#part0027_split_001.html#id_427 .calibre10} Questions**

1[.]{.c19}Which two file systems are created in a default RHEL 8
installation?

2[.]{.c19}What would the command *lvresize -r -L +30 *dev*vg02/lvol2*
do?

3[.]{.c19}XFS is the default file system type in RHEL 8. True or False?

4[.]{.c19}What type of information does the *blkid* command display?

5[.]{.c19}What would the command *xfs_admin -L bootfs *dev*sda1* do?

6[.]{.c19}The *lsblk* command cannot be used to view file system UUIDs.
True or False?

7[.]{.c19}What is the process of paging out and paging in known as?

8[.]{.c19}What would the command *mkswap *dev*sdc2* do?

9[.]{.c19}What would happen if you mount a file system on a directory
that already contains files in it?

10[.]{.c23}A UUID is always assigned to a file system at its creation
time. True or False?

11[.]{.c23}Arrange the activities to create and activate a swap space
while ensuring persistence: (a) swapon, (b) update fstab, (c) mkswap,
and (d) reboot?

12[.]{.c23}The difference between the primary and backup superblocks is
that the primary superblock includes pointers to the data blocks where
the actual file contents are stored whereas the backup superblocks
don't. True or False?

13[.]{.c23}What would the command *mkfs.ext4 *dev*vgtest/lvoltest* do?

14[.]{.c23}Arrange the tasks in correct sequence: umount file system,
mount file system, create file system, remove file system.

15[.]{.c23}Which of these statements is wrong with respect to file
systems: (a) optimize each file system independently, (b) keep
dissimilar data in separate file systems, (c) grow and shrink a file
system independent of other file systems, and (d) file systems cannot be
expanded independent of other file systems.

16[.]{.c23}Which command can be used to create a label for an XFS file
system?

17[.]{.c23}What would the *mount* command do with the -a switch?

18[.]{.c23}What would the command *df -t xfs* do?

19[.]{.c23}What is the difference between the *mkfs.ext4* and *mke2fs*
commands?

20[.]{.c23}Which command can be used to determine the total and used
physical memory and swap in the system?

21[.]{.c23}Which virtual file contains information about the current
swap?

22[.]{.c23}The **etc*fstab* file can be used to activate swap spaces
automatically at system reboots. True or False?

23[.]{.c23}What is the default file system type used for optical media?

24[.]{.c23}The *xfs_repair* command must be run on a mounted file
system. True or False?

25[.]{.c23}Provide two commands that can be used to activate and
deactivate swap spaces manually.

26[.]{.c23}Provide the *fstab* file entry for an Ext4 file system
located in device **dev*mapper/vg20-lv1* and mounted with default
options on the */ora1* directory.

27[.]{.c23}What is the name of the virtual file that holds currently
mounted file system information?

28[.]{.c23}Which option must be specified with persistent VDO file
system mounting to avoid landing the system in an unbootable state?

29[.]{.c23}Both Ext3 and Ext4 file system types support journaling. True
or False?

30[.]{.c23}Name three commands that can be employed to view the UUID of
an XFS file system?

**[Answers]{#part0027_split_001.html#id_428 .calibre10} to Review
Questions**

1[.]{.c19}*/* and */boot*.

2[.]{.c19}The command provided will expand the logical volume *lvol2* in
volume group *vg02* along with the file system it contains by 30MB.

3[.]{.c19}True.

4[.]{.c19}The *blkid* command displays attributes for block devices.

5[.]{.c19}The command provided will apply the specified label to the XFS
file system in **dev*sda1*.

6[.]{.c19}False.

7[.]{.c19}The process of paging out and in is known as demand paging.

8[.]{.c19}The command provided will create swap structures in the
**dev*vdc2* partition.

9[.]{.c19}The files in the directory will hide.

10[.]{.c23}True.

11[.]{.c23}c/a/b or c/b/a.

12[.]{.c23}False.

13[.]{.c23}The command provided will format **dev*vgtest/lvoltest*
logical volume with Ext4 file system type.

14[.]{.c23}Create, mount, unmount, and remove.

15[.]{.c23}d is incorrect.

16[.]{.c23}The *xfs_admin* command can be used to create a label for an
XFS file system.

17[.]{.c23}The command provided will mount all file systems listed in
the **etc*fstab* file but are not currently mounted.

18[.]{.c23}The command provided will display all mounted file systems of
type XFS.

19[.]{.c23}No difference.

20[.]{.c23}The *free* command.

21[.]{.c23}The **proc*swaps* file contains information about the current
swap.

22[.]{.c23}True.

23[.]{.c23}The default file system type for optical devices is ISO9660.

24[.]{.c23}False.

25[.]{.c23}The *swapon* and *swapoff* commands.

26[.]{.c23}*dev*mapper/vg20-lv1 /ora1 swap defaults 0 0

27[.]{.c23}The *mounts* file under **proc*self* directory.

28[.]{.c23}The \_netdev option.

29[.]{.c23}True.

30[.]{.c23}You can use the *xfs_admin*, *lsblk*, and *blkid* commands to
view the UUID of an XFS file system.

**[Do-]{#part0027_split_001.html#id_429 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

To use the same storage that was used in the labs for [Chapters
13](#part0025_split_000.html#page_301){.calibre5} and
[14](#part0026_split_000.html#page_321){.calibre5}, destroy all traces
of the partitions, volume groups, and pools that were created before
proceeding. Alternatively, you can add more storage to *server2* for the
following labs.

**[Lab]{#part0027_split_001.html#id_430 .calibre10} 15-1: Create VFAT,
Ext4, and XFS File Systems in Partitions & Mount Persistently**

As *user1* with *sudo* on *server2*, create three 70MB primary
partitions on one of the available 250MB disks (*lsblk*) by invoking the
*parted* utility directly at the command prompt. Apply label "msdos" if
the disk is new. Initialize partition 1 with VFAT, partition 2 with
Ext4, and partition 3 with XFS file system types. Create mount points
*/vfatfs5*, */ext4fs5*, and */xfsfs5*, and mount all three manually.
Determine the UUIDs for the three file systems, and add them to the
*fstab* file. Unmount all three file systems manually, and execute
**mount -a** to mount them all. Run **df -h** for verification. (Hint:
File System Management).

**Lab 15-2: Create XFS File System in VDO Volume and Mount
Persistently**

As *user1* with *sudo* on *server2*, ensure that VDO software is
installed and the VDO service is enabled and started. Create a volume
*vdo5* with a logical size 16GB on an available 4GB disk (*lsblk*) using
the *vdo* command. Select an appropriate slab size for the volume.
Verify the volume creation with the *vdo*, *lsblk*, and *vdostats*
commands. Initialize the volume with XFS file system type. Create mount
point */vdofs5*, and mount it manually. Add the file system information
to the *fstab* file, and use "\_netdev" as a mount option. Unmount the
file system manually, and execute **mount -a** to mount it back. Run
**df -h** to confirm. (Hint: File System Management).

**[Lab]{#part0027_split_001.html#id_431 .calibre10} 15-3: Create Ext4
and XFS File Systems in LVM Volumes and Mount Persistently**

As *user1* with *sudo* on *server2*, initialize an available 250MB disk
for use in LVM (*lsblk*). Create volume group *vg200* with PE size 8MB
and add the physical volume. Create two logical volumes *lv200* and
*lv300* of sizes 120MB and 100MB. Use the *vgs*, *pvs*, *lvs*, and
*vgdisplay* commands for verification. Initialize the volumes with Ext4
and XFS file system types. Create mount points */lvmfs5* and */lvmfs6*,
and mount them manually. Add the file system information to the *fstab*
file using their device files. Unmount the file systems manually, and
execute **mount -a** to mount them back. Run **df -h** to confirm.
(Hint: File System Management).

**[Lab]{#part0027_split_001.html#id_432 .calibre10} 15-4: Extend Ext4
and XFS File Systems in LVM Volumes**

As *user1* with *sudo* on *server2*, initialize an available 250MB disk
for use in LVM (*lsblk*). Add the new physical volume to volume group
*vg200*. Expand logical volumes *lv200* and *lv300* along with the
underlying file systems to 200MB and 250MB. Use the *vgs*, *pvs*, *lvs*,
*vgdisplay*, and *df* commands for verification. (Hint: File System
Management).

**[Lab]{#part0027_split_001.html#id_433 .calibre10} 15-5: Create XFS
File System in Stratis Volume and Mount Persistently**

As *user1* with *sudo* on *server2*, confirm Stratis software is
installed and the Stratis service is enabled and started. Create pool
*strpool5* on an available 1GB disk and confirm. Create file system
*strfs5* in the pool and verify. Create mount point */strfs5*. Determine
the UUID of the Stratis file system and add it to the *fstab* file for
persistence. Use the x-systemd.requires=stratisd.service as a mount
option. Reboot the system and confirm mounting with **df -h**. (Hint:
File System Management).

**[Lab]{#part0027_split_001.html#id_434 .calibre10} 15-6: Create Swap in
Partition and LVM Volume and Activate Persistently**

As *user1* with *sudo* on *server2*, create two 100MB partitions on an
available 250MB disk (*lsblk*) by invoking the *parted* utility directly
at the command prompt. Apply label "msdos" if the disk is new.
Initialize one of the partitions with swap structures. Apply label
*swappart* to the swap partition, and add it to the *fstab* file.
Execute **swapon -a** to activate it. Run **swapon -s** to confirm
activation.

Initialize the other partition for use in LVM. Expand volume group
*vg200* (Lab 15-4) by adding this physical volume. Create logical volume
*swapvol* of size 180MB. Use the *vgs*, *pvs*, *lvs*, and *vgdisplay*
commands for verification. Initialize the logical volume for swap. Add
an entry to the *fstab* file for the new swap area using its device
file. Execute **swapon -a** to activate it. Run **swapon -s** to confirm
activation. (Hint: Swap and its Management).

[]{#part0028_split_000.html}

## Chapter 16 {#part0028_split_000.html#calibre_pb_0 .calibre11}

**Networking, Network Devices, and Network Connections**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Overview of basic networking
concepts: hostname, IPv4, network classes, subnetting, subnet mask,
CIDR, protocol, TCP/UDP, well-known ports, ICMP, Ethernet address, IPv6,
IPv4/IPv6 differences, consistent device naming, *etc.*

[![](images/00001.jpeg){.c37}]{.c36}Change hostname of the system

[![](images/00001.jpeg){.c37}]{.c36}Understand the concepts of network
device and connection

[![](images/00001.jpeg){.c37}]{.c36}Anatomy of a network connection
profile

[![](images/00001.jpeg){.c37}]{.c36}Know network device and connection
management tools and techniques

[![](images/00001.jpeg){.c37}]{.c36}Configure network connections by
hand and using commands

[![](images/00001.jpeg){.c37}]{.c36}Describe the hosts table

[![](images/00001.jpeg){.c37}]{.c36}Test network connectivity using
hostname and IP address

[RHCSA Objectives:]{.c39}

[47]{#part0028_split_000.html#id_813 .calibre10}. Configure IPv4 and
IPv6 addresses

::: {#part0028_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0028_split_001.html}

[ A]{.c42}computer network is formed when two or more physical or
virtual computers are connected together for sharing resources and data.
The computers may be linked via wired or wireless means, and a device
such as a switch is used to interconnect several computers to allow them
to communicate with one another on the network. There are numerous
concepts and terms that need to be grasped in order to work effectively
and efficiently with network device and network connection configuration
and troubleshooting, and several other network services. This chapter
provides a wealth of that information.

For a system to be able to talk to other systems, one of its network
devices must have a connection profile attached containing a unique IP
address, hostname, and other essential network parameters. The network
assignments may be configured statically or obtained automatically from
a DHCP server. Few files are involved in the configuration, which may be
modified by hand or using commands. Testing follows the configuration to
confirm the system's ability to communicate.

**[Networking]{#part0028_split_001.html#id_435 .calibre10}
Fundamentals**

The primary purpose of computer networks is to allow users to share data
and resources. A simple network is formed when two computers are
interconnected. Using a networking device such as a *switch*, this
network can be expanded to include additional computers, as well as
printers, scanners, storage, and other devices (collectively referred to
as *nodes* or *entities*). A computer on the network can be configured
to act as a file server, storage server, or as a gateway to the Internet
for the rest of the networked computers. Nodes may be interconnected
using wired or wireless means. A corporate network may have thousands of
nodes linked via a variety of data transmission media. The Internet is
the largest network of networks with millions of nodes interconnected.

There are many elementary concepts and terms that you need to grasp
before being able to configure network interfaces, connection profiles,
and client/server setups that are elaborated in this and other chapters.
As well, there are many configuration files and commands related to
various network services that you need to understand thoroughly in order
to manage a RHEL-based environment effectively. Some of the concepts,
terms, configuration files, and commands are explained in this chapter.

**[Hostname]{#part0028_split_001.html#id_436 .calibre10}**

A *hostname* is a unique alphanumeric label (the hyphen (-), underscore
(\_), and period (.) characters are also allowed) that is assigned to a
node to identify it on the network. It can consist of up to 253
characters. It is normally allotted based on the purpose and principal
use of the system. In RHEL, the hostname is stored in the
**etc*hostname* file.

The hostname can be viewed with several different commands, such as
*hostname*, *hostnamectl*, *uname*, and *nmcli*, as well as by
displaying the content of the **etc*hostname* file. Let's run these
commands on *server1*:

![](images/00815.jpeg){.image2}

All the above commands displayed the same output.

**[Exercise]{#part0028_split_001.html#id_437 .calibre10} 16-1: Change
System Hostname**

This exercise should be done on both *server1* and *server2* as *user1*
with *sudo* where required.

In this exercise, you will change the hostnames of both lab servers
persistently. You will rename
*[server1.example.com](http://server1.example.com){.calibre5}* to
*[server10.example.com](http://server10.example.com){.calibre5}* by
editing a file and restarting the corresponding service daemon. You will
rename *[server2.example.com](http://server2.example.com){.calibre5}* to
*[server20.example.com](http://server20.example.com){.calibre5}* using a
command. You will validate the change on both systems.

**On *server1*:**

1[.]{.c19}Open the **etc*hostname* file in a text editor and change the
current entry to the following:

![](images/00816.jpeg){.image2}

2[.]{.c19}Execute the *systemctl* command to restart the
*systemd-hostnamed* service daemon and verify the new hostname with the
*hostname* command:

![](images/00817.jpeg){.image2}

3[.]{.c19}To view the reflection of the new hostname in the command
prompt, log off and log back in as *user1*. The new prompt will look
like:

![](images/00818.jpeg){.image2}

**On *server2*:**

1[.]{.c19}Execute the *hostnamectl* command to change the hostname to
*[server20.example.com](http://server20.example.com:){.calibre5}*[:](http://server20.example.com:){.calibre5}

![](images/00819.jpeg){.image2}

2[.]{.c19}To view the reflection of the new hostname in the command
prompt, log off and log back in as *user1*. You can also use the
*hostname* command to view the new name.

![](images/00820.jpeg){.image2}

You can also change the system hostname using the *nmcli* command. For
instance, you could have used **nmcli general hostname
[server20.example.com](http://server20.example.com){.calibre5}** to
rename *[server2.example.com](http://server2.example.com){.calibre5}*.
The *nmcli* command is explained in detail later in this chapter.

[**EXAM TIP:**]{.c56} You need to know only one of the available methods
to change the hostname of the system.

Going forward, you will be using the new hostnames *server10* and
*server20*.

**[IPv]{#part0028_split_001.html#id_438 .calibre10}4 Address**

IPv4 stands for *Internet Protocol version 4* and represents a unique
32-bit software address that every single entity on the network must
have in order to communicate with other entities. It was the first
version of IP that was released for public use. IPv4 addresses are also
referred to as *dotted-quad* addresses, and they can be assigned on a
temporary or permanent basis. Temporary addresses are referred to as
*dynamic* addresses and are typically leased from a DHCP server for a
specific period of time. Permanent addresses, on the other hand, are
called *static* addresses and they are manually set.

You can use the *ip* command with the addr argument to view the current
IP assignments on the system. Let's run this command on *server10* and
see what it returns:

![](images/00821.jpeg){.image2}

The output indicates one configured network connection (number 2 above)
called *enp0s3* with IPv4 address 192.168.0.110 assigned to it. The
other connection (number 1 above), represented as *lo*, is a special
purpose software device reserved for use on every Linux system. Its IPv4
address is always 127.0.0.1, and it is referred to as the system's
*loopback* (or *localhost*) address. Network programs and applications
that communicate with the local system employ this hostname.

**Network Classes**

An IPv4 address is comprised of four period-separated octets (4 x 8 = 32
bit address) that are divided into a *network* portion (or network
ID/bits) comprising of the *Most Significant Bits* (MSBs) and a *node*
portion (or node/host ID/bits) containing the *Least Significant Bits*
(LSBs). The network portion identifies the correct destination network,
and the node portion represents the correct destination node on that
network. Public network addresses are classified into three categories:
class A, class B, and class C. Private network addresses are classified
into two categories: class D and class E. Class D addresses are
multicast and they are employed in special use cases only. Class E
addresses are experimental and are reserved for future use.

**Class A**

Class A addresses are used for large networks with up to 16 million
nodes. This class uses the first octet as the network portion and the
rest of the octets as the node portion. The total number of usable
network and node addresses can be up to 126 and 16,777,214,
respectively. The network address range for class A networks is between
0 and 127. See an example below of a random class A IP address, which
also shows two reserved addresses:

  ------------------- ----------------------
  10.121.51.209       (class A IP address)
  10.121.51.**0**     (network address)
  10.121.51.**255**   (broadcast address)
  ------------------- ----------------------

The 0 and 255 (highlighted) are network and broadcast addresses, and
they are always reserved.

**Class B**

Class B addresses are used for mid-sized networks with up to 65 thousand
nodes. This class employs the first two octets as the network portion
and the other two as the node portion. The total number of usable
network and node addresses can be up to 16,384 and 65,534, respectively.
The network address range for class B networks is between 128 and 191.
See an example below of a random class B IP address, which also shows
two reserved addresses:

  ---------------------------- ----------------------
  161**.**121**.**51**.**209   (class B IP address)
  161.121.51.**0**             (network address)
  161.121.51.**255**           (broadcast address)
  ---------------------------- ----------------------

The 0 and 255 (highlighted) are network and broadcast addresses, and
they are always reserved.

**Class C**

Class C addresses are employed for small networks with up to 254 nodes.
This class uses the first three octets as the network portion and the
last octet as the node portion. The total number of usable network and
node addresses can be up to 2,097,152 and 254, respectively. The network
address range for class C networks is between 192 and 223. See an
example below of an arbitrary class C IP address, which also shows two
reserved addresses:

  ---------------------------- ----------------------
  215**.**121**.**51**.**209   (class C IP address)
  215.121.51.**0**             (network address)
  215.121.51.**255**           (broadcast address)
  ---------------------------- ----------------------

The 0 and 255 (highlighted) are network and broadcast addresses, and
they are always reserved.

**Class D**

Class D addresses range from 224 to 239.

**Class E**

Class E addresses range from 240 to 255.

**[Subnetting]{#part0028_split_001.html#id_439 .calibre10}**

*Subnetting* is a technique by which a large network address space is
divided into several smaller and more manageable logical subnetworks,
referred to as *subnets*. Subnetting results in reduced network traffic,
improved network performance, and de-centralized and easier
administration, among other benefits. Subnetting does not touch the
network bits; it uses the node bits only.

The following should be kept in mind when dealing with subnetting:

[![](images/00562.jpeg){.image2}]{.c1568a}Subnetting does not increase
the number of IP addresses in a network. In fact, it reduces the number
of usable addresses.

[![](images/00562.jpeg){.image2}]{.c1568a}All nodes in a given subnet
have the same subnet mask.

[![](images/00562.jpeg){.image2}]{.c1568a}Each subnet acts as an
isolated network and requires a router to talk to other subnets.

[![](images/00562.jpeg){.image2}]{.c1568a}The first and the last IP
address in a subnet are reserved. The first address points to the subnet
itself, and the last address is the broadcast address.

**[Subnet]{#part0028_split_001.html#id_440 .calibre10} Mask**

A *subnet mask* or *netmask* is the network portion plus the subnet
bits. It segregates the network bits from the node bits. It is used by
routers to pinpoint the start and end of the network/subnet portion and
the start and end of the node portion for a given IP address.

The subnet mask, like an IP address, can be represented in either
decimal or binary notation. The 1s in the subnet mask isolate the subnet
bits from the node bits that contain 0s. The default subnet masks for
class A, B, and C networks are 255.0.0.0, 255.255.0.0, and
255.255.255.0, respectively.

To determine the subnet address for an arbitrary IP address, such as
192.168.12.72 with netmask 255.255.255.224, write the IP address in
binary format. Then write the subnet mask in binary format with all
network and subnet bits set to 1 and all node bits set to 0. Then
perform a logical AND operation. For each matching 1 you get a 1,
otherwise you get a 0. The following highlights the ANDed bits:

  --------------------------------------------------------------------- --------------------------------------------
  **11**000000.**1**0**1**0**1**000.0000**11**00.0**1**001000           (IP address 192.168.12.72)
  **11**111111**.1**1**1**1**1**111**.**1111**11**11**.**1**1**100000   (subnet mask 255.255.255.224)
  ==============================                                        
  **11**000000.**1**0**1**0**1**000.0000**11**00.0**1**000000           (subnet IP 192.168.12.64 in binary format)
  192 **.** 168 **.** 12 **.** 64                                       (subnet IP in decimal format)
  --------------------------------------------------------------------- --------------------------------------------

This calculation enables you to ascertain the subnet address from a
given IP and subnet mask.

**[Classless]{#part0028_split_001.html#id_441 .calibre10} Inter-Domain
Routing (CIDR) Notation**

*Classless Inter-Domain Routing* (CIDR) is a technique designed to
control the quick depletion of IPv4 addresses and the rapid surge in the
number of routing tables required to route IPv4 traffic on the network
and the Internet. This technique was introduced as a substitute for the
*classful* scheme, which was not scalable and had other limitations.
Using CIDR, IPv4 addresses can be allocated in custom blocks suitable
for networks of all sizes. This technique has resulted in smaller and
less cluttered routing tables. CIDR was originally designed to address
IPv4 needs; however, it has been extended to support IPv6 as well.

An IPv4 address written in CIDR notation has a leading forward slash (/)
character followed by the number of routing bits. A sample class C IP
address of 192.168.0.20 with the default class C subnet mask of
255.255.255.0 will be written as 192.168.0.20/24. This notation presents
a compact method of denoting an IP address along with its subnet mask.

**[Protocol]{#part0028_split_001.html#id_442 .calibre10}**

A *protocol* is a set of rules governing the exchange of data between
two network entities. These rules include how data is formatted, coded,
and controlled. The rules also provide error handling, speed matching,
and data packet sequencing. In other words, a protocol is a common
language that all nodes on the network speak and understand. Protocols
are defined in the **etc*protocols* file. An excerpt from this file is
provided below:

![](images/00822.jpeg){.image2}

Column 1 in the output lists the name of a protocol, followed by the
associated port number, alias, and a short description in columns 2, 3,
and 4.

Some common protocols are TCP, UDP, IP, and ICMP.

**[TCP]{#part0028_split_001.html#id_443 .calibre10} and UDP Protocols**

TCP (*Transmission Control Protocol*) and UDP (*User Datagram Protocol*)
protocols are responsible for transporting data packets between network
entities. TCP is reliable, connection-oriented, and point-to-point. It
inspects for errors and sequencing upon a packet's arrival on the
destination node, and returns an acknowledgement to the source node,
establishing a point-to-point connection with the peer TCP layer on the
source node. If the packet is received with an error or if it is lost in
transit, the destination node requests a resend of the packet. This
ensures guaranteed data delivery and makes TCP reliable. Due to its
reliability and connection-oriented nature, TCP is widely implemented in
network applications.

UDP, in contrast, is unreliable, connectionless, and multi-point. If a
packet is lost or contains errors upon arrival at the destination, the
source node is unaware of it. The destination node does not send an
acknowledgment back to the source node. A common use of this protocol is
in broadcast-only applications where reliability is not sought.

**Well-Known Ports**

Both TCP and UDP use ports for data transmission between a client and
its server program. Ports are either well-known or private. A well-known
port is reserved for an application's exclusive use, and it is
standardized across all network operating systems. Well-known ports are
defined in the **etc*services* file, an excerpt of which is presented
below:

![](images/00823.jpeg){.image2}

Column 1 lists the official name of a network service, followed by the
port number and transport layer protocol the service uses, optional
aliases, and comments in successive columns.

Some common services and the ports they listen on are FTP (*File
Transfer Protocol*) 21, SSH (*Secure Shell*) 22, SMTP (*Simple Mail
Transfer Protocol*) 25, DNS (*Domain Name System*) 53, HTTP (*HyperText
Transfer Protocol*) 80, NTP (*Network Time Protocol*) 123, secure HTTP
(*HyperText Transfer Protocol Secure*) 443, and rsyslog 514.

A private port, on the other hand, is an arbitrary number generated when
a client application attempts to establish a communication session with
its server process. This port number no longer exists after the session
has ended.

**[ICMP]{#part0028_split_001.html#id_444 .calibre10} Protocol**

The *Internet Control Message Protocol* (ICMP) is a key protocol. It is
primarily used for testing and diagnosing network connections. Commands
such as *ping* uses this protocol to send a stream of messages to remote
network devices to examine their health and report statistical and
diagnostic information. The report includes the number of packets
transmitted, received, and lost; a round-trip time for individual
packets with an overall average; a percentage of packets lost during the
communication; and so on. See a sample below that shows two packets
(-c2) sent from *server10* to the IP address of *server20*:

![](images/00824.jpeg){.image2}

Other commands, such as *traceroute*, also employ this protocol for
route determination and debugging between network entities. The IPv6
version of ICMP is referred to as *ICMPv6* and it is used by tools such
as *ping6* and *tracepath6*.

**[Ethernet]{#part0028_split_001.html#id_445 .calibre10} Address**

An *Ethernet* address represents an exclusive 48-bit address that is
used to identify the correct destination node for data packets
transmitted from the source node. The data packets include hardware
addresses for the source and the destination node. The Ethernet address
is also referred to as the *hardware*, *physical*, *link layer*, or
*MAC* address.

You can use the *ip* command to list all network interfaces available on
the system along with their Ethernet addresses:

![](images/00825.jpeg){.image2}

IP and hardware addresses work hand in hand, and a combination of both
is critical to identifying the correct destination node on the network.
A network protocol called *Address Resolution Protocol* (ARP) is used to
enable IP and hardware addresses to work in tandem. ARP determines the
hardware address of the destination node when its IP address is known.

**[IPv]{#part0028_split_001.html#id_446 .calibre10}6 Address**

With the explosive growth of the Internet, the presence of an extremely
large number of network nodes requiring an IP, and an ever-increasing
demand for additional addresses---the conventional IPv4 address space,
which provides approximately 4.3 billion addresses---has been exhausted.
To meet the future demand, a new version of IP is now available and its
use is on the rise. This new version is referred to as IPv6 (*IP version
6*) or IPng (*IP next generation*). By default, IPv6 is enabled in RHEL
8 on all configured network connections.

IPv6 is a 128-bit software address, providing access to approximately
340 undecillion (340 followed by 36 zeros) addresses. This is an
extremely large space, and it is expected to fulfill the IP requirements
for several decades to come.

IPv6 uses a messaging protocol called *Neighbor Discovery Protocol*
(NDP) to probe the network to discover neighboring IPv6 devices,
determine their reachability, and map their associations. This protocol
also includes enhanced functionalities (provided by ICMP and ARP on IPv4
networks) for troubleshooting issues pertaining to connectivity, address
duplication, and routing.

[Unlike]{#part0028_split_001.html#id_835} IPv4 addresses, which are
represented as four dot-separated octets, IPv6 addresses contain eight
colon-separated groups of four hexadecimal numbers. A sample v6 IP would
be 1204:bab1:21d1:bb43:23a1:9bde:87df:bac9. It looks a bit daunting at
first sight, but there are methods that will simplify their
representation.

The *ip addr* command also shows IPv6 addresses for the interfaces:

![](images/00826.jpeg){.image2}

It returns two IPv6 addresses. The first one belongs to the loopback
interface, and the second one is assigned to the *enp0s3* connection.

**Major Differences between IPv4 and IPv6**

There are a number of differences between IPv4 and IPv6 protocols. Some
of the major ones are highlighted in [Table
16-1](#part0028_split_001.html#id_746){.calibre5}.

::: c49
  --------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------
  **IPv4**                                                                                                  **IPv6**
  Uses 4x8-bit, period-separated decimal number format for address representation. Example: 192.168.0.100   Uses 8x16-bit, colon-separated hexadecimal number format for address representation. Example: fe80::a00:27ff:feae:f35b
  Number of address bits: 32                                                                                Number of address bits: 128
  Maximum number of addresses: \~4.3 billion.                                                               Maximum number of addresses: virtually unlimited
  Common testing and troubleshooting tools: ping, traceroute, tracepath, *etc.*                             Common testing and troubleshooting tools: ping6, traceroute6, tracepath6, *etc.*
  Support for IP autoconfiguration: no                                                                      Support for IP autoconfiguration: yes
  Packet size: 576 bytes                                                                                    Packet size: 1280 bytes
  --------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0028_split_001.html#id_746} 16-1 IPv4 vs IPv6**
:::

These and other differences not listed here are due to enhancements and
new features added to IPv6.

**[Network]{#part0028_split_001.html#id_447 .calibre10} Devices and
Connections**

*Network Interface Cards* (NICs) are hardware adapters that provide one
or more Ethernet ports for network connectivity. NICs may also be
referred to as *network adapters* and individual ports as *network
interfaces* or *network devices*. NICs may be built-in to the system
board or are add-on adapters. They are available in one, two, and four
port designs on a single adapter.

Individual interfaces (devices) can have one or more connection profiles
attached to them with different configuration settings. Each connection
profile has a unique name and includes settings such as the device name,
hardware address, activating the connection on reboot, and so on. A
connection profile can be configured by editing files or using commands.
A device can have multiple connection profiles attached, but only one of
them can be active at a time.

**[Consistent]{#part0028_split_001.html#id_448 .calibre10} Network
Device Naming**

In RHEL versions earlier than 7, network interfaces were named *eth*
(Ethernet), *em* (embedded), and *wlan* (wireless lan), and were
numbered 0 and onwards as the interfaces were discovered during a system
boot. This was the default scheme that had been in place for network
device naming for years. Given a large number of interfaces located
onboard and on add-on NICs, the number assignments could possibly change
on the next boot due to failures or errors in their detection, which
will result in connectivity and operational issues.

As of RHEL 7, the default naming scheme has been augmented to base on
several rules governed by the *udevd* service. The default ruleset is to
assign names using the device's location and topology, and the setting
in firmware. The underlying virtualization layer (VMware, VirtualBox,
KVM) also plays a role in the naming. Some sample device names are
*enp0s3*, *ens160*, *etc.*

This advanced ruleset has resulted in consistent and predictable naming,
eliminating the odds of reenumeration during a hardware rescan.
Moreover, the designated names are not affected by the addition or
removal of interface cards. This naming scheme helps in identifying,
configuring, troubleshooting, and replacing the right adapter without
hassle.

**[Understanding]{#part0028_split_001.html#id_449 .calibre10} Interface
Connection Profile**

Each network connection has a configuration file that defines IP
assignments and other relevant parameters for it. The networking
subsystem reads this file and applies the settings at the time the
connection is activated. Connection configuration files (or *connection
profiles*) are stored in a central location under the
**etc*sysconfig/network-scripts* directory. The filenames begin with
*ifcfg-*and are followed by the name of the connection. Some instances
of connection filenames are *ifcfg-enp0s3*, *ifcfg-ens160*, and
*ifcfg-em1*.

On *server10* and *server20*, the device name for the first interface is
*enp0s3* with connection name *enp0s3* and relevant connection
information stored in the *ifcfg-enp0s3* file. This connection was
established at the time of installation. The current content of the
*ifcfg-enp0s3* file from *server10* are presented below with IPv6
directives excluded:

![](images/00827.jpeg){.image2}

These directives and a few others that can be defined in this file are
listed in alphabetical order in [Table
16-2](#part0028_split_001.html#id_747){.calibre5}.

::: c49
  -------------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Directive**        **Description**
  BOOTPROTO            Defines the boot protocol to be used. Common values include dhcp to obtain IP assignments from a DHCP server and none or static to use a static IP as set with the IPADDR directive.
  BROWSER_ONLY         Works if PROXY_METHOD is set to auto. Default is no.
  DEFROUTE             Whether to use this connection as the default route
  DEVICE               Specifies the device name for the network interface
  DNS1                 Defines the IP address or the hostname of the first DNS server. This address/hostname is placed in the *etc*resolv.conf file if the PEERDNS directive is set to no in this file.
  GATEWAY              Specifies the gateway address for the connection if the BOOTPROTO directive is set to none or static
  HWADDR               Describes the hardware address for the device
  IPADDR               Specifies the static IP for the connection if the BOOTPROTO directive is set to none or static
  IPV4_FAILURE_FATAL   Whether to disable the device if IPv4 configuration fails. Default is no.
  IPV6INIT             Whether to enable IPv6 support for this connection
  NAME                 Any description given to this connection. The default matches the device name.
  NETMASK              Sets the netmask address for the connection if the BOOTPROTO directive is set to none or static
  NM_CONTROLLED        Whether the NetworkManager service is to be allowed to modify the configuration for this connection. It should be turned off on computers that use static IP addresses. Default is yes.
  ONBOOT               Whether to autoactivate this connection at system boot
  PEERDNS              Whether to modify the DNS client resolver file *etc*resolv.conf. Default is yes if BOOTPROTO is set to dhcp.
  PREFIX               Defines the number of subnet bits. This directive may be used in lieu of NETMASK.
  PROXY_METHOD         Method to be used for proxy setting. Default is no.
  UUID                 The UUID associated with this connection
  TYPE                 Specifies the type of this connection
  -------------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0028_split_001.html#id_747} 16-2 Network Connection
Configuration Directives**
:::

There are numerous other directives, including those for IPv6, that may
be defined in connection profiles. Run **man nm-settings** for a
description of additional directives.

**[Exercise]{#part0028_split_001.html#id_450 .calibre10} 16-2: Add
Network Devices to server10 and server20**

This exercise will add one network interface to *server10* and one to
*server20* using VirtualBox.

In this exercise, you will shut down *server10* and *server20*. You will
launch VirtualBox and add one network interface to *server10* and one to
*server20* in preparation for Exercises 17-3 and 17-4.

1[.]{.c19}Execute **sudo shutdown now** on *server10*.

2[.]{.c19}Start VirtualBox on your Windows/Mac computer and highlight
the *RHEL8-VM1* virtual machine that you created in [Exercise
1-2](#part0013_split_001.html#id_11){.calibre5}. Make sure it is powered
off.

3[.]{.c19}Click Settings at the top and then Network on the window that
pops up. Click on "Adapter 2" and check the "Enable Network Adapter"
box. Select "Internal Network" from the drop down list next to "Attached
to". See [Figure 16-1](#part0028_split_001.html#page_387){.calibre5}.

::: c49
::: width_
![](images/00828.jpeg){.calibre13}
:::

**Figure 16-1 VirtualBox -- Add Network Interface**
:::

4[.]{.c19}Click OK to return to the main VirtualBox interface.

5[.]{.c19}Power on *RHEL8-VM1* to boot RHEL 8 in it.

6[.]{.c19}When the server is up, log on as *user1* and run the *ip*
command as follows to verify the new interface:

![](images/00829.jpeg){.image2}

The output reveals a new network device called *enp0s8*. This is the new
interface you just added to the VM.

7[.]{.c19}Repeat steps 1 through 6 to add a network interface to
*server20* and verify.

This completes the addition of new network interfaces to *server10* and
*server20*. You are now ready to use them in the upcoming exercises.

**[Network]{#part0028_split_001.html#id_451 .calibre10} Device and
Connection Administration Tools**

There are a few tools and methods available for configuring and
administering network interfaces, connections, and connection profiles.
The *NetworkManager* service includes a toolset for this purpose as
well. Let's take a quick look at the basic management tools in [Table
16-3](#part0028_split_001.html#id_748){.calibre5}.

::: c49
  ------------- --------------------------------------------------------------------------------------------------------------------------
  **Command**   **Description**
  ip            A powerful and versatile tool for displaying, monitoring, and managing interfaces, connections, routing, traffic, *etc.*
  ifup          Activates a connection
  ifdown        Deactivates a connection
  ------------- --------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0028_split_001.html#id_748} 16-3 Basic Network Management
Tools**
:::

You can manually create a connection profile and attach it to a network
device. Many Linux administrators prefer this approach. RHEL also offers
an alternative method for this purpose, which is discussed later in this
chapter.

In [Exercise 16-3](#part0028_split_001.html#id_452){.calibre5}, you'll
use the manual method to configure a connection profile for a new
network device that was added in [Exercise
16-2](#part0028_split_001.html#id_450){.calibre5} and employ the tools
listed in [Table 16-3](#part0028_split_001.html#id_748){.calibre5}.

**[Exercise]{#part0028_split_001.html#id_452 .calibre10} 16-3: Configure
New Network Connection Manually**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will create a connection profile for the new
network interface *enp0s8* using a text editing tool. You will assign
the IP 172.10.10.110/24 with gateway 172.10.10.1 and set it to
autoactivate at system reboots. You will deactivate and reactivate this
interface at the command prompt.

1[.]{.c19}Create a file called *ifcfg-enp0s8* in the
**etc*sysconfig/network-scripts* directory and enter the following
information to establish a connection profile:

![](images/00830.jpeg){.image2}

2[.]{.c19}Deactivate and reactivate this interface using the *ifdown*
and *ifup* commands:

![](images/00831.jpeg){.image2}

3[.]{.c19}Verify the activation of *enp0s8* connection:

![](images/00832.jpeg){.image2}

The new connection profile called *enp0s8* has been applied to the new
network device called *enp0s8*. The connection is now ready for use. The
connectivity to *server10* over this new connection is tested later in
this chapter.

**[The]{#part0028_split_001.html#id_453 .calibre10} NetworkManager
Service**

*NetworkManager* is the default interface and connection configuration,
administration, and monitoring service used in RHEL 8. This service has
a daemon program called *NetworkManager*, which is responsible for
keeping available interfaces and connections up and active. It offers a
powerful command line tool called *nmcli* to manage interfaces and
connections, and to control the service. This utility offers many
options for their effective management. The *NetworkManager* service
also furnishes a text-based interface called *nmtui* and a graphical
equivalent called *nm-connection-editor* that you may use in lieu of
*nmcli*.

**[The]{#part0028_split_001.html#id_454 .calibre10} nmcli Command**

*nmcli* is a NetworkManager command line tool that is employed to
create, view, modify, remove, activate, and deactivate network
connections, and to control and report network device status. It
operates on seven different object categories, with each category
supporting several options to form a complete command. The seven
categories are general, networking, connection, device, radio, monitor,
and agent. This discussion only focuses on the connection and device
object categories. They are described in [Table
16-4](#part0028_split_001.html#id_749){.calibre5} along with management
operations that they can perform.

::: c49
  ----------------------------------------------------------------------------- --------------------------------------------------------------------
  **Object**                                                                    **Description**
  **Connection: activates, deactivates, and administers network connections**   
  show                                                                          Lists connections
  up / down                                                                     Activates/deactivates a connection
  add                                                                           Adds a connection
  edit                                                                          Edits an existing connection or adds a new one
  modify                                                                        Modifies one or more properties of a connection
  delete                                                                        Deletes a connection
  reload                                                                        Instructs NetworkManager to re-read all connection profiles
  load                                                                          Instructs NetworkManager to re-read a connection profile
  **Device: displays and administers network interfaces**                       
  status                                                                        Exhibits device status
  show                                                                          Displays detailed information about all or the specified interface
  ----------------------------------------------------------------------------- --------------------------------------------------------------------

**[Table]{#part0028_split_001.html#id_749} 16-4 Network Connection and
Device Administration Tools**
:::

Object categories and the objects within them may be written in an
abridged form to save typing. For instance, the connection category may
be abbreviated as a "c" or "con" and the device category as a "d" or
"dev". The same rule applies to object names as well. For instance, add
may be specified as an "a", delete as a "d", and so on. Check the manual
pages for nmcli-examples.

The *nmcli* command supports tab completion to make its use easier.
Let's run a few examples on *server10* to understand the command's
usage.

To show (s) all available connections (c) including both active and
inactive:

![](images/00833.jpeg){.image2}

The output lists two connection profiles (NAME) and the devices (DEVICE)
they are attached to. It also shows their UUID and type.

To deactivate (down) the connection (c) *enp0s8*:

![](images/00834.jpeg){.image2}

The connection profile is detached from the device, disabling the
connection. You can check with **nmcli c s**.

To activate (up) the connection (c) enp0s8:

![](images/00835.jpeg){.image2}

The connection profile is reattached to the device, enabling the
connection. You can check with **nmcli c s**.

To display the status (s) of all available network devices (d):

![](images/00836.jpeg){.image2}

The output shows three devices and their types, states, and the
connection profiles attached to them. The loopback interface is not
managed by the NetworkManager service.

**[Exercise]{#part0028_split_001.html#id_455 .calibre10} 16-4: Configure
New Network Connection Using nmcli**

This exercise should be done on *server20* as *user1* with *sudo* where
required.

In this exercise, you will create a connection profile using the *nmcli*
command for the new network interface *enp0s8* that was added to
*server20* in [Exercise
16-2](#part0028_split_001.html#id_450){.calibre5}. You will assign the
IP 172.10.10.120/24 with gateway 172.10.10.1, and set it to autoactivate
at system reboots. You will deactivate and reactivate this interface at
the command prompt.

1[.]{.c19}Check the running status of the *NetworkManager* service:

![](images/00837.jpeg){.image2}

The service is up and active on the server.

2[.]{.c19}Check the presence of the new interface:

![](images/00838.jpeg){.image2}

The output signifies the presence of a new network device called
*enp0s8*. It does not have a connection profile attached to it.

3[.]{.c19}Add (add) a connection profile (con) and attach it to the new
interface. Use the type Ethernet, device name (ifname) *enp0s8* with a
matching connection name (con-name), CIDR (ip4) 172.10.10.120/24, and
gateway (gw4) 172.10.10.1:

![](images/00839.jpeg){.image2}

A new connection has been added, attached to the new interface, and
activated. In addition, the command has saved the connection information
in a new file called *ifcfg-enp0s8* and stored it in the
**etc*sysconfig/network-scripts* directory.

4[.]{.c19}Confirm the new connection status:

![](images/00840.jpeg){.image2}

The output indicates the association of the new connection with the
network device.

5[.]{.c19}Check the content of the *ifcfg-enp0s8* connection profile:

![](images/00841.jpeg){.image2}

There are a number of default directives added to the connection profile
in addition to the configuration items you entered with the *nmcli*
command above. The ONBOOT directive is also set to yes automatically.
This setting is an indicative of the fact that the connection will be
auto-enabled at system reboots.

6[.]{.c19}Check the IP assignments for the new connection:

![](images/00842.jpeg){.image2}

The IP is assigned to the interface. The connection is tested later in
this chapter.

7[.]{.c19}Deactivate this connection to detach it from the interface:

![](images/00843.jpeg){.image2}

The connection profile is now detached from the interface, deactivating
the connection. You can check with **nmcli c s**.

8[.]{.c19}Reactivate the connection to attach it to the interface:

![](images/00844.jpeg){.image2}

The connection profile is now reattached to the interface, activating
the connection. You can check with **nmcli c s**.

This brings the exercise to a conclusion.

[**EXAM TIP:**]{.c56} You need to know only one of the available methods
to set IP assignments on the system.

**[Understanding]{#part0028_split_001.html#id_456 .calibre10} Hosts
Table**

Each IP address used on the system should have a hostname assigned to
it. In an environment with multiple systems on the network, it is
prudent to have some sort of a hostname to IP address resolution method
in place to avoid typing the destination system IP repeatedly to access
it. DNS is one such method. It is designed for large networks such as
corporate networks and the Internet. For small, internal networks, the
use of a local hosts table (the **etc*hosts* file) is also common. This
table is used to maintain hostname to IP mapping for systems on the
local network, allowing us to access a system by simply employing its
hostname. In this book, there are two systems in place:
*[server10.example.com](http://server10.example.com){.calibre5}* with IP
192.168.0.110 and alias *server10*, and
*[server20.example.com](http://server20.example.com){.calibre5}* with IP
192.168.0.120 and alias *server20*. You can append this information to
the **etc*hosts* file on both *server10* and *server20* as shown below:

  --------------- ---------------------------------------------------------------- ----------
  192.168.0.110   [server10.example.com](http://server10.example.com){.calibre5}   server10
  192.168.0.120   [server20.example.com](http://server20.example.com){.calibre5}   server20
  --------------- ---------------------------------------------------------------- ----------

Each row in the file contains an IP address in column 1 followed by the
*official* (or *canonical*) hostname in column 2, and one or more
optional aliases thereafter. The official hostname and one or more
aliases give users the flexibility of accessing the system using any of
these names.

[**EXAM TIP:**]{.c56} In the presence of an active DNS with all
hostnames resolvable, there is no need to worry about updating the hosts
file.

As expressed above, the use of the *hosts* file is common on small
networks and it should be updated on each individual system to reflect
any changes for best inter-system connectivity experience.

**[Testing]{#part0028_split_001.html#id_457 .calibre10} Network
Connectivity**

RHEL includes the *ping* command to examine network connectivity between
two systems. It uses the IP address of the destination system to send a
series of 64-byte *Internet Control Message Protocol* (ICMP) test
packets to it. A response from the remote system validates connectivity
and health. With the -c option, you can specify the number of packets
that you want transmitted.

The following sends two packets from *server10* to 192.168.0.120 (IP for
*server20*):

![](images/00845.jpeg){.image2}

Under "192.168.0.120 ping statistics," the output depicts the number of
packets transmitted, received, and lost. The packet loss should be 0%,
and the round trip time should not be too high for a healthy connection.
In general, you can use this command to test connectivity with the
system's own IP, the loopback IP (127.0.0.1), a static route, the
default gateway, and any other address on the local or remote network.

If a *ping* response fails, you need to check if the NIC is seated
properly, its driver is installed, network cable is secured
appropriately, IP and netmask values are set correctly, and the default
or static route is accurate.

**[Exercise]{#part0028_split_001.html#id_458 .calibre10} 16-5: Update
Hosts Table and Test Connectivity**

This exercise should be done on *server10* and *server20* as *user1*
with *sudo* where required.

In this exercise, you will update the **etc*hosts* file on both
*server10* and *server20*. You will add the IP addresses assigned to
both connections and map them to hostnames *server10*, *server10s8*,
*server20*, and *server20s8* appropriately. You will test connectivity
from *server10* to *server20* and from *server10s8* to *server20s8*
using their IP addresses and then their hostnames.

**On *server20*:**

1[.]{.c19}Open the **etc*hosts* file and add the following entries:

![](images/00846.jpeg){.image2}

The IP addresses for both connections are added for both servers.

**On *server10*:**

2[.]{.c19}Open the **etc*hosts* file and add the following entries:

![](images/00847.jpeg){.image2}

The IP addresses for both connections are added for both servers.

3[.]{.c19}Send two packets from *server10* to the IP address of
*server20*:

![](images/00848.jpeg){.image2}

4[.]{.c19}Issue two ping packets on *server10* to the hostname of
*server20*:

![](images/00849.jpeg){.image2}

5[.]{.c19}Send one packet from *server10* to the IP address of
*server20s8*:

![](images/00850.jpeg){.image2}

6[.]{.c19}Issue one ping packet on *server10* to the hostname of
*server20s8*:

![](images/00851.jpeg){.image2}

Steps 3 through 6 verified the connectivity to the remote server over
both connections. Each server has two IP addresses, and each IP address
has a unique hostname assigned to it.

This concludes the exercise.

**[Chapter]{#part0028_split_001.html#id_459 .calibre10} Summary**

This chapter discussed the rudiments of networking. It began by
providing an understanding of various essential networking terms and
concepts to build the foundation for networking topics going forward.
Topics such as hostname, IPv4, IPv6, network classes, subnetting, subnet
mask, CIDR notation, protocol, port, Ethernet address, and consistent
device naming were covered in sufficient detail.

We modified hostnames on both lab servers by modifying a configuration
file and restarting the hostname service on one server and using a
single command on the other. We employed two different methods to
demonstrate multiple ways of doing the same thing. A third method was
also mentioned to rename the hostname.

Next, we described the terms network devices and network connections,
and realized the difference between the two. We examined a connection
profile and looked at a number of directives that may be defined for a
network connection. We added a new network device to each lab server and
configured them by employing two different methods. We activated the new
connections and performed a ping test for functional validation. Lastly,
we populated the hosts table with the IP and hostname mapping on both
lab servers.

**[Review]{#part0028_split_001.html#id_460 .calibre10} Questions**

1[.]{.c19}What is the use of *ifup* and *ifdown* commands?

2[.]{.c19}Which service is responsible for maintaining consistent device
naming?

3[.]{.c19}List three key differences between TCP and UDP protocols.

4[.]{.c19}What is the significance of the NAME and DEVICE directives in
a connection profile?

5[.]{.c19}Which class of IP addresses has the least number of node
addresses?

6[.]{.c19}Which command can you use to display the hardware address of a
network device?

7[.]{.c19}Define protocol.

8[.]{.c19}Which directory stores the network connection profiles?

9[.]{.c19}True or False. A network device is a physical or virtual
network port and a network connection is a configuration file attached
to it.

10[.]{.c23}IPv4 is a 32-bit software address. How many bits does an IPv6
address have?

11[.]{.c23}Which file defines the port and protocol mapping?

12[.]{.c23}What would the command *hostnamectl set-hostname host20* do?

13[.]{.c23}Name the file that stores the hostname of the system.

14[.]{.c23}What would the command *nmcli cs* do?

15[.]{.c23}What is the purpose of the ONBOOT directive in the network
connection profile?

16[.]{.c23}The **etc*hosts* file maintains hostname to hardware address
mappings. True or False?

17[.]{.c23}Which file contains service, port, and protocol mappings?

18[.]{.c23}What would the *ip addr* command produce?

19[.]{.c23}Which file would you consult to identify the port number and
protocol associated with a network service?

20[.]{.c23}Adding a connection profile with the *nmcli* command creates
a connection profile in the **etc*sysconfig/network-scripts* directory.
True or False?

21[.]{.c23}Name four commands that can be used to display the system
hostname?

22[.]{.c23}List any two benefits of subnetting.

**[Answers]{#part0028_split_001.html#id_461 .calibre10} to Review
Questions**

1[.]{.c19}The *ifup* and *ifdown* commands are used to enable and
disable a network connection, respectively.

2[.]{.c19}The *udevd* service handles consistent naming of network
devices.

3[.]{.c19}TCP is connection-oriented, reliable, and point-to-point; UDP
is connectionless, unreliable, and multi-point.

4[.]{.c19}The NAME directive sets the name for the network connection
and the DEVICE directive defines the network device the connection is
associated with.

5[.]{.c19}The C class supports the least number of node addresses.

6[.]{.c19}The *ip* command.

7[.]{.c19}A set of rules that govern the exchange of information between
two network entities.

8[.]{.c19}The **etc*sysconfig/network-scripts* directory.

9[.]{.c19}True.

10[.]{.c23}128.

11[.]{.c23}The **etc*protocols* file.

12[.]{.c23}The command provided will update the **etc*hostname* file
with the specified hostname and restart the *systemd-hostnamed* daemon
for the change to take effect.

13[.]{.c23}The **etc*hostname* file.

14[.]{.c23}The command provided will display the status information for
all network connections.

15[.]{.c23}The purpose of the ONBOOT directive is to direct the boot
scripts whether to activate this connection.

16[.]{.c23}False. This file maintains hostname to IP address mapping.

17[.]{.c23}The **etc*services* file.

18[.]{.c23}This command provided will display information about network
connections including IP assignments and hardware address.

19[.]{.c23}The **etc*services* file.

20[.]{.c23}True.

21[.]{.c23}The *hostname*, *uname*, *hostnamectl*, and *nmcli* commands
can be used to view the system hostname.

22[.]{.c23}Better manageability and less traffic.

**[Do-]{#part0028_split_001.html#id_462 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

**[Lab]{#part0028_split_001.html#id_463 .calibre10} 16-1: Add New
Interface and Configure Connection Profile with nmcli**

Add a new network interface to *RHEL8-VM1* in VirtualBox.

As *user1* with *sudo* on *server10*, run **ip a** and verify the
presence of the new interface (*enp0s8*). Use the *nmcli* command and
assign IP 192.168.0.210/24 and gateway 192.168.0.1. Set the network
connection to autoactivate on system reboots. Deactivate and reactivate
this connection manually. (Hint: Network Devices and Connections).

**[Lab]{#part0028_split_001.html#id_464 .calibre10} 16-2: Add New
Interface and Configure Connection Profile Manually**

Add a new network interface to *RHEL8-VM2* in VirtualBox.

As *user1* with *sudo* on *server20*, run **ip a** and verify the
presence of the new interface (*enp0s8*). Make a copy of the
*ifcfg-enp0s3* file as *ifcfg-enp0s8* under the
**etc*sysconfig/network-scripts* directory. Remove the HWADDR and UUID
directives, and set the values for IPADDR, NETMASK, and GATEWAY
directives appropriately. Set the network connection to autoactivate on
system reboots. Deactivate and reactivate this connection manually.
(Hint: Network Devices and Connections).

[]{#part0029_split_000.html}

## Chapter 17 {#part0029_split_000.html#calibre_pb_0 .calibre11}

**Network File System**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Overview of Network File System
service and key components

[![](images/00001.jpeg){.c37}]{.c36}Network File System benefits and
versions

[![](images/00001.jpeg){.c37}]{.c36}Export a share on NFS server

[![](images/00001.jpeg){.c37}]{.c36}Mount the share on NFS client using
standard mount method

[![](images/00001.jpeg){.c37}]{.c36}Understand the AutoFS service, and
its benefits and functioning

[![](images/00001.jpeg){.c37}]{.c36}Analyze AutoFS configuration maps

[![](images/00001.jpeg){.c37}]{.c36}Mount the exported share on NFS
client using AutoFS

[![](images/00001.jpeg){.c37}]{.c36}Configure NFS and AutoFS to share
and mount user home directories

[RHCSA Objectives:]{.c39}

[34]{#part0029_split_000.html#id_800 .calibre10}. Mount and unmount
network file systems using NFS

::: {#part0029_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0029_split_001.html}

[ N]{.c42}[etwork]{.c43} shares may be mounted on RHEL and accessed the
same way as local file systems. This can be done manually using the same
tools that are employed for mounting and unmounting local file systems.
An alternative solution is to implement the AutoFS service to
automatically mount and unmount them without the need to execute any
commands explicitly. AutoFS monitors activities in mount points based on
which it triggers a mount or unmount action.

RHEL exports shares using the Network File System service for remote
mounting on clients. The combination of NFS and AutoFS/standard mount is
prevalent in real world scenarios. This chapter elaborates on the
benefits of the file sharing solution and expounds upon AutoFS maps. It
demonstrates a series of exercises to detail the configuration of the
NFS server, NFS client, and AutoFS service. It also explores the
automatic mounting of user home directories on clients.

**[Network]{#part0029_split_001.html#id_465 .calibre10} File System**

*Network File System* (NFS) is a networking protocol that allows file
sharing over the network. The Network File System service is based upon
the client/server architecture whereby users on one system access files,
directories, and file systems (collectively called "shares") that reside
on a remote system as if they are mounted locally on their system. The
remote system that makes its shares available for network access is
referred to as an NFS server, and the process of making the shares
accessible is referred to as *exporting*. The shares the NFS server
exports may be accessed by one or more systems. These systems are called
NFS clients, and the process of making the shares accessible on clients
is referred to as *mounting*. See [Figure
17-1](#part0029_split_001.html#id_680){.calibre5} for a simple NFS
client/server arrangement that shows two shares---*/export1* and
*/export2*---exported on the network to a remote system, which has them
mounted there.

::: c49
::: width_
![](images/00852.jpeg){.calibre13}
:::

**Figure 17-1 NFS Server/Client**
:::

A system can provide both server and client functionality concurrently.
When a directory or file system share is exported, the entire directory
structure beneath it becomes available for mounting on the client. A
subdirectory or the parent directory of a share cannot be re-exported if
it exists in the same file system. Similarly, a mounted share cannot be
exported further. A single exported file share is mounted on a directory
mount point.

**[Benefits]{#part0029_split_001.html#id_466 .calibre10} of Using NFS**

The use of NFS provides several benefits, some of which are highlighted
below:

[![](images/00562.jpeg){.image2}]{.c1568a}Supports a variety of
operating system platforms including Linux, UNIX, and Microsoft Windows.

[![](images/00562.jpeg){.image2}]{.c1568a}Multiple NFS clients can
access a single share simultaneously.

[![](images/00562.jpeg){.image2}]{.c1568a}Enables the sharing of common
application binaries and other read-only information, resulting in
reduced administration overhead and storage cost.

[![](images/00562.jpeg){.image2}]{.c1568a}Gives users access to uniform
data.

[![](images/00562.jpeg){.image2}]{.c1568a}Allows the consolidation of
scattered user home directories on the NFS server and then exporting
them to the clients. This way users will have only one home directory to
maintain.

**[NFS]{#part0029_split_001.html#id_467 .calibre10} Versions**

RHEL 8 provides the support for NFS versions 3, 4.0, 4.1, and 4.2, with
version 4.2 being the default. NFSv3 supports both TCP and UDP transport
protocols, asynchronous writes, and 64-bit file sizes that gives clients
the ability to access files of sizes larger than 2GB.

NFSv4.x are *Internet Engineering Task Force* (IETF) series of protocols
that provide all the features of NFSv3, plus the ability to transit
firewalls and work on the Internet. They provide enhanced security and
support for encrypted transfers and ACLs, as well as greater
scalability, better cross-platform interoperability, and better system
crash handling. They use usernames and group names rather than UIDs and
GIDs for files located on network shares. NFSv4.0 and NFSv4.1 use the
TCP protocol by default, but can work with UDP for backward
compatibility. In contrast, NFSv4.2 only supports TCP.

**[NFS]{#part0029_split_001.html#id_468 .calibre10} Server and Client
Configuration**

This section presents two exercises, one demonstrating how to export a
share on a server (NFS server) and the other outlines the steps on
mounting and accessing a share on a remote system (NFS client). The
basic setup of the NFS service is straightforward. It requires adding an
entry of the share to a file called **etc*exports* and using a command
called *exportfs* to make it available on the network. It also requires
the addition of a firewall rule to allow access to the share by NFS
clients.

The *mount* command employed on an NFS client is the same command that
was used in [Chapter 15](#part0027_split_000.html#page_345){.calibre5}
to mount local file systems. Moreover, the *fstab* file requires an
entry for the NFS share on the client for persistent mounting.

The exercises in this section illustrate the usage of both commands and
the syntax of both files.

**[Exercise]{#part0029_split_001.html#id_469 .calibre10} 17-1: Export
Share on NFS Server**

This exercise should be done on *server20* as *user1* with *sudo* where
required.

In this exercise, you will create a directory called */common* and
export it to *server10* in read/write mode. You will ensure that NFS
traffic is allowed through the firewall. You will confirm the export.

1[.]{.c19}Install the NFS software called *nfs-utils*:

![](images/00853.jpeg){.image2}

2[.]{.c19}Create */common* directory to be exported as a share:

![](images/00854.jpeg){.image2}

3[.]{.c19}Add full permissions to */common*:

![](images/00855.jpeg){.image2}

4[.]{.c19}Add the NFS service persistently to the firewalld
configuration to allow the NFS traffic to pass through, and load the new
rule:

![](images/00856.jpeg){.image2}

![](images/00002.jpeg){.image} Refer to [Chapter
20](#part0032_split_000.html#page_449){.calibre5} for details around the
firewall service.

5[.]{.c19}Start the NFS service and enable it to autostart at system
reboots:

![](images/00857.jpeg){.image2}

6[.]{.c19}Verify the operational status of the NFS services:

![](images/00858.jpeg){.image2}

7[.]{.c19}Open the **etc*exports* file in a text editor and add an entry
for */common* to export it to *server10* with read/write option:

![](images/00859.jpeg){.image2}

8[.]{.c19}Export the entry defined in the **etc*exports* file. The -a
option exports all the entries listed in the file and -v displays
verbose output.

![](images/00860.jpeg){.image2}

The NFS service is now set up on *server20* with the */common* share
available for mounting on *server10* (NFS client in this case).

For practice, you can unexport the share by issuing the *exportfs*
command with the -u flag as follows:

![](images/00861.jpeg){.image2}

Before proceeding, re-export the share using **sudo exportfs -av**.

**[Exercise]{#part0029_split_001.html#id_470 .calibre10} 17-2: Mount
Share on NFS Client**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will mount the */common* share exported in
[Exercise 17-1](#part0029_split_001.html#id_469){.calibre5}. You will
create a mount point called */local*, mount the remote share manually,
and confirm the mount. You will add the remote share to the file system
table for persistence. You will remount the share and confirm the mount.
You will create a test file in the mount point and confirm the file
creation on the NFS server (*server20*).

1[.]{.c19}Install the NFS software called *nfs-utils*:

![](images/00862.jpeg){.image2}

2[.]{.c19}Create */local* mount point:

![](images/00863.jpeg){.image2}

3[.]{.c19}Mount the share manually using the *mount* command:

![](images/00864.jpeg){.image2}

The remote share is successfully mounted on *server10*, and it can be
accessed as any other local file system.

4[.]{.c19}Confirm the mount using either the *mount* or the *df*
command:

![](images/00865.jpeg){.image2}

The *mount* command output returns the NFS protocol version (NFS4.2) and
all the default options used in mounting the share.

5[.]{.c19}Open the **etc*fstab* file and append the following entry for
persistence:

![](images/00866.jpeg){.image2}

![](images/00002.jpeg){.image} The \_netdev option will make the system
wait for networking to establish before attempting to mount this share.

![](images/00002.jpeg){.image} A mount point should be empty and must
not be in use when an attempt is made to mount a share on it.

6[.]{.c19}Unmount the share manually using the *umount* command and
remount it via the *fstab* file to validate the accuracy of the entry
placed in the file:

![](images/00867.jpeg){.image2}

7[.]{.c19}Run **mount** and **df -h** again to verify the remounting.

8[.]{.c19}Create a file called *nfsfile* under */local* and verify:

![](images/00868.jpeg){.image2}

9[.]{.c19}Confirm the file creation on the NFS server (*server2*):

![](images/00869.jpeg){.image2}

[**EXAM TIP:**]{.c56} Do not forget to update the *etc*fstab file on the
client.

This completes the setup and testing of mounting, unmounting, and
remounting of an NFS share on the client.

**[Auto]{#part0029_split_001.html#id_471 .calibre10} File System
(AutoFS)**

In the previous section, you learned how to attach (mount) an NFS share
to the Linux directory tree manually for access by users and
applications on an NFS client. Once attached, the share was treated just
like any other local file system. You also learned how to detach
(unmount) an NFS share manually from the directory tree to make it
inaccessible to users and applications. You placed an entry for the
share in the *fstab* file to guarantee a remount during system reboots.

RHEL offers an alternative way of mounting and unmounting a share on the
clients during runtime as well as system reboots. This feature is
delivered by a service called the *Auto File System* (AutoFS). AutoFS is
a client-side service, which is employed to mount an NFS share
on-demand. With a proper entry placed in AutoFS configuration files, the
AutoFS service automatically mounts a share upon detecting an activity
in its mount point with a command such as *ls* or *cd*. In the same
manner, AutoFS unmounts the share automatically if it has not been
accessed for a predefined period of time.

![](images/00002.jpeg){.image} To avoid inconsistencies, mounts managed
with AutoFS should not be mounted or unmounted manually or via the
*etc*fstab file.

The use of AutoFS saves the kernel from dedicating system resources to
maintain unused NFS shares, resulting in slight improvement in system
performance.

**[Benefits]{#part0029_split_001.html#id_472 .calibre10} of Using
AutoFS**

There are several benefits associated with using the AutoFS service over
placing entries in the **etc*fstab* file. Some of the key benefits are
described below:

[![](images/00562.jpeg){.image2}]{.c1568a}AutoFS requires that NFS
shares be defined in text configuration files called *maps*, which are
located in the */etc* or **etc*auto.master.d* directory. AutoFS does not
make use of the **etc*fstab* file.

[![](images/00562.jpeg){.image2}]{.c1568a}AutoFS does not require *root*
privileges to mount an NFS share; manual mounting and mounting via
*fstab* do require that privilege.

[![](images/00562.jpeg){.image2}]{.c1568a}AutoFS prevents an NFS client
from hanging if an NFS server is down or inaccessible. With the other
method, the unavailability of the NFS server may cause the NFS client to
hang.

[![](images/00562.jpeg){.image2}]{.c1568a}With AutoFS, a share is
unmounted automatically if it is not accessed for five minutes by
default. With the *fstab* method, the share stays mounted until it is
either manually unmounted or the client shuts down.

[![](images/00562.jpeg){.image2}]{.c1568a}AutoFS supports wildcard
characters and environment variables, which the other method does not
support.

**[How]{#part0029_split_001.html#id_473 .calibre10} AutoFS Works**

The AutoFS service consists of a daemon called *automount* in the
userland that mounts configured shares automatically upon access. This
daemon is invoked at system boot. It reads the AutoFS master map and
creates initial mount point entries, but it does not mount any shares
yet. When the service detects a user activity under a mount point during
runtime, it mounts the requested file system at that time. If a share
remains idle for a certain time period, *automount* unmounts it by
itself.

**[AutoFS]{#part0029_split_001.html#id_474 .calibre10} Configuration
File**

The configuration file for the AutoFS service is **etc*autofs.conf*,
which AutoFS consults at service startup. Some key directives from this
file are shown below along with preset values:

master_map_name = auto.master

timeout = 300

negative_timeout = 60

mount_nfs_default_protocol = 4

logging = none

There are additional directives available in this file and more can be
added to modify the default behavior of the AutoFS service. [Table
17-1](#part0029_split_001.html#id_750){.calibre5} describes the above
directives.

::: c49
  ---------------------------- ---------------------------------------------------------------------------------------------------------------------------
  **Directive**                **Description**
  master_map_name              Defines the name of the master map. The default is auto.master located in the /etc directory.
  timeout                      Specifies, in seconds, the maximum idle time after which a share is automatically unmounted. The default is five minutes.
  negative_timeout             Expresses, in seconds, a timeout value for failed mount attempts. The default is one minute.
  mount_nfs_default_protocol   Sets the default NFS version to be used to mount shares.
  logging                      Configures a logging level. Options are none, verbose, and debug. The default is none (disabled).
  ---------------------------- ---------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0029_split_001.html#id_750} 17-1 AutoFS Directives**
:::

The directives in the *autofs.conf* file are normally left to their
default values, but you can alter them if required.

**[AutoFS]{#part0029_split_001.html#id_475 .calibre10} Maps**

The AutoFS service needs to know the NFS shares to be mounted and their
locations. It also needs to know any specific options to use with
mounting them. This information is defined in AutoFS files called
*maps*. There are three common AutoFS map types: *master*, *direct*, and
*indirect*.

**The Master Map**

The *auto.master* file located in the */etc* directory is the default
master map, as defined in the **etc*autofs.conf* configuration file with
the master_map_name directive. This map may be used to define entries
for indirect and direct maps. However, it is recommended to store
user-defined map files in the **etc*auto.master.d* directory, which the
AutoFS service automatically parses at startup. The following presents
two samples to explain the format of the map entries:

  ------- --------------------------------
  /-      *etc*auto.master.d/auto.direct
  /misc   *etc*auto.misc
  ------- --------------------------------

Line 1 defines a direct map and points to the
**etc*auto.master.d/auto.direct* file for mount details.

The second one is for an indirect map, notifying AutoFS to refer to the
**etc*auto.misc* file for mount details. The umbrella mount point
*/misc* will precede all mount point entries listed in the
**etc*auto.misc* file. This indirect map entry is normally used to
automount removable file systems, such as CD, DVD, external USB disks,
and so on. Any custom indirect map file should be located in the
**etc*auto.master.d* directory.

You may append an option to either entry in the *auto.master* file;
however, that option will apply globally to all subentries in the
specified map file.

**The Direct Map**

The direct map is used to mount shares automatically on any number of
unrelated mount points. Some key points to note when working with direct
maps are:

[![](images/00562.jpeg){.image2}]{.c1568a}Direct mounted shares are
always visible to users.

[![](images/00562.jpeg){.image2}]{.c1568a}Local and direct mounted
shares can coexist under one parent directory.

[![](images/00562.jpeg){.image2}]{.c1568a}Accessing a directory
containing many direct mount points mounts all shares.

Each direct map entry places a separate share entry to the **etc*mtab*
file, which maintains a list of all mounted file systems whether they
are local or remote. This file is updated whenever a local file system,
removable file system, or a network share is mounted or unmounted.

**[Exercise]{#part0029_split_001.html#id_476 .calibre10} 17-3: Access
NFS Share Using Direct Map**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will configure a direct map to automount the NFS
share */common* that is available from *server20*. You will install the
relevant software, create a local mount point */autodir*, and set up
AutoFS maps to support the automatic mounting. Note that */common* is
already mounted on the */local* mount point on *server10* (NFS client)
via the *fstab* file. There should'nt be any conflict in configuration
or functionality between the two.

1[.]{.c19}Install the AutoFS software package called *autofs*:

![](images/00870.jpeg){.image2}

2[.]{.c19}Create a mount point */autodir* using the *mkdir* command:

![](images/00871.jpeg){.image2}

3[.]{.c19}Edit the **etc*auto.master* file and add the following entry
at the beginning of the file. This entry will point the AutoFS service
to the *auto.dir* file for additional information.

![](images/00872.jpeg){.image2}

4[.]{.c19}Create **etc*auto.master.d/auto.dir* file and add the mount
point, NFS server, and share information to it:

![](images/00873.jpeg){.image2}

5[.]{.c19}Start the AutoFS service now and set it to autostart at system
reboots:

![](images/00874.jpeg){.image2}

6[.]{.c19}Verify the operational status of the AutoFS service. Use the
-l and \--no-pager options to show full details without piping the
output to a pager program (the *pg* command in this case).

![](images/00875.jpeg){.image2}

7[.]{.c19}Run the *ls* command on the mount point */autodir* and then
issue the *mount* command to verify that the share is automounted and
accessible:

![](images/00876.jpeg){.image2}

Observe the above outcomes. The *mount* command output depicts the path
of the AutoFS map (**etc*auto.master.d/auto.dir*), the file system type
(autofs), and the options used during the mount process. An activity in
the mount point (the *ls* command) caused AutoFS to mount the share
*/common* on */autodir*. Wait five minutes and run the *mount* command
again. You'll see that the auto file system has disappeared. A *cd*,
*ls*, or some other activity in the mount point will bring it back.

This completes the AutoFS setup for an NFS share on the client using a
direct map.

**The Indirect Map**

The indirect map is preferred over the direct map if you want to mount
all of the shares under one common parent directory. Some key points to
note when working with indirect maps are:

[![](images/00562.jpeg){.image2}]{.c1568a}Indirect mounted shares become
visible only after they have been accessed.

[![](images/00562.jpeg){.image2}]{.c1568a}Local and indirect mounted
shares cannot coexist under the same parent directory.

[![](images/00562.jpeg){.image2}]{.c1568a}Each indirect map puts only
one entry in the **etc*mtab* mount table.

[![](images/00562.jpeg){.image2}]{.c1568a}Accessing a directory
containing many indirect mount points shows only the shares that are
already mounted.

Both direct and indirect maps have their own merits and demerits. By
comparing their features, it seems more prudent to use the indirect map
for automounting NFS shares. However, this statement may not be true for
every environment, as there could be specifics that would dictate which
option to go with.

**[Exercise]{#part0029_split_001.html#id_477 .calibre10} 17-4: Access
NFS Share Using Indirect Map**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will configure an indirect map to automount the
NFS share */common* that is available from *server20*. You will install
the relevant software and set up AutoFS maps to support the automatic
mounting. You will observe that the specified mount point "autoindir" is
created automatically under */misc*.

Note that */common* is already mounted on the */local* mount point via
the *fstab* file and it is also configured via a direct map for
automounting on */autodir*. There should occur no conflict in
configuration or functionality among the three.

1[.]{.c19}Install the AutoFS software package called *autofs*:

![](images/00877.jpeg){.image2}

2[.]{.c19}Confirm the entry for the indirect map */misc* in the
**etc*auto.master* file exists:

![](images/00878.jpeg){.image2}

3[.]{.c19}Edit the **etc*auto.misc* file and add the mount point, NFS
server, and share information to it:

![](images/00879.jpeg){.image2}

4[.]{.c19}Start the AutoFS service now and set it to autostart at system
reboots:

![](images/00880.jpeg){.image2}

5[.]{.c19}Verify the operational status of the AutoFS service. Use the
-l and \--no-pager options to show full details without piping the
output to a pager program (the *pg* command in this case):

![](images/00881.jpeg){.image2}

6[.]{.c19}Run the *ls* command on the mount point **misc*autoindir* and
then *grep* for both *auto.misc* and *autoindir* on the *mount* command
output to verify that the share is automounted and accessible:

![](images/00882.jpeg){.image2}

Observe the above outcomes. The *mount* command output illustrates the
path of the AutoFS map (**etc*auto.misc*), the auto-generated mount
point (**misc*autoindir*), file system type (autofs), and the options
used during the mount process. An activity in the mount point (*ls*
command in this case) caused AutoFS to mount the share */common* on
**misc*autoindir*. You can use the same umbrella mount point */misc* to
mount additional auto-generated mount points.

This mount point will automatically disappear after five minutes of
idling. You can verify that by issuing the *mount* command again. A
*cd*, *ls*, or some other activity in the mount point will bring it
back.

This completes the AutoFS setup for an NFS share on the client using an
indirect map.

**[Automounting]{#part0029_split_001.html#id_478 .calibre10} User Home
Directories**

AutoFS allows us to automount user home directories by exploiting two
special characters in indirect maps. The asterisk (\*) replaces the
references to specific mount points and the ampersand (&) substitutes
the references to NFS servers and shared subdirectories. With user home
directories located under */home*, on one or more NFS servers, the
AutoFS service will connect with all of them simultaneously when a user
attempts to log on to a client. The service will mount only that
specific user's home directory rather than the entire */home*. The
indirect map entry for this type of substitution is defined in an
indirect map, such as **etc*auto.master.d/auto.home*, and will look
like:

![](images/00883.jpeg){.image2}

With this entry in place, there is no need to update any AutoFS
configuration files if NFS servers with */home* shared are added or
removed. Similarly, if user home directories are added or deleted, there
will be no impact on the functionality of AutoFS. If there is only one
NFS server sharing the home directories, you can simply specify its name
in lieu of the first & symbol in the above entry.

**[Exercise]{#part0029_split_001.html#id_479 .calibre10} 17-5: Automount
User Home Directories Using Indirect Map**

There are two portions for this exercise. The first portion should be
done on *server20* (NFS server) and the second portion on *server10*
(NFS client) as *user1* with *sudo* where required.

In the first portion, you will create a user account called *user30*
with UID 3000. You will add the */home* directory to the list of NFS
shares so that it becomes available for remote mount.

In the second portion, you will create a user account called *user30*
with UID 3000, base directory */nfshome*, and no user home directory.
You will create an umbrella mount point called */nfshome* for mounting
the user home directory from the NFS server. You will install the
relevant software and establish an indirect map to automount the remote
home directory of *user30* under */nfshome*. You will observe that the
home directory of *user30* is automounted under */nfshome* when you sign
in as *user30*.

**On NFS server *server20*:**

1[.]{.c19}Create a user account called *user30* with UID 3000 (-u) and
assign password "password1":

![](images/00884.jpeg){.image2}

2[.]{.c19}Edit the **etc*exports* file and add an entry for */home* (do
not remove the previous entry):

![](images/00885.jpeg){.image2}

3[.]{.c19}Export all the shares listed in the **etc*exports* file:

![](images/00886.jpeg){.image2}

**On NFS client *server10*:**

1[.]{.c19}Install the AutoFS software package called *autofs*:

![](images/00887.jpeg){.image2}

![](images/00888.jpeg){.image2}

2[.]{.c19}Create a user account called *user30* with UID 3000 (-u), base
home directory location */nfshome* (-b), no home directory (-M), and
password "password1":

![](images/00889.jpeg){.image2}

This is to ensure that the UID for the user is consistent on the server
and the client to avoid access issues.

3[.]{.c19}Create the umbrella mount point */nfshome* to automount the
user's home directory:

![](images/00890.jpeg){.image2}

4[.]{.c19}Edit the **etc*auto.master* file and add the mount point and
indirect map location to it:

![](images/00891.jpeg){.image2}

5[.]{.c19}Create the **etc*auto.master.d/auto.home* file and add the
following information to it:

![](images/00892.jpeg){.image2}

For multiple user setup, you can replace "user30" with the & character,
but ensure that those users exist on both the server and the client with
consistent UIDs.

6[.]{.c19}Start the AutoFS service now and set it to autostart at system
reboots. This step is not required if AutoFS is already running and
enabled.

![](images/00893.jpeg){.image2}

7[.]{.c19}Verify the operational status of the AutoFS service. Use the
-l and \--no-pager options to show full details without piping the
output to a pager program (the *pg* command):

![](images/00894.jpeg){.image2}

8[.]{.c19}Log in as *user30* and run the *pwd*, *ls*, and *df* commands
for verification:

![](images/00895.jpeg){.image2}

The user is successfully logged in with their home directory automounted
from the NFS server. The *pwd* command confirms the path. The *df*
command verifies the NFS server name and the source home directory path
for *user30*, as well as the mount location. You can also use the
*mount* command and pipe the output to *grep* for *user30* to view mount
details (**mount \| grep user30**).

[**EXAM TIP:**]{.c56}You may need to configure AutoFS for mounting a
remote user home directory.

This completes the setup for an automounted home directory share for a
user.

**[Chapter]{#part0029_split_001.html#id_480 .calibre10} Summary**

This chapter discussed the sharing and mounting of remote file systems
using the Network File System protocol. It elucidated the concepts,
benefits, and versions of the NFS service, and described the commands
and configuration files involved in NFS management on the server and the
client.

Next, we performed an exercise to demonstrate the configuration and
sharing of a directory on one of the lab servers (NFS server) and
another exercise on the second lab system (NFS client) to mount that
share manually and persistently using the standard NFS mount method.

We explored the client-side service called AutoFS for automounting NFS
shares. We discussed the concepts, benefits, and components associated
with AutoFS, and analyzed its maps. We performed exercises to mount,
confirm, and unmount the remote NFS share using both direct and indirect
methods.

Finally, we described the AutoFS setting to automount user home
directories from the NFS server.

**[Review]{#part0029_split_001.html#id_481 .calibre10} Questions**

1[.]{.c19}What would the entry *\* server10:/home/&* in an AutoFS
indirect map imply?

2[.]{.c19}Which command is used to export a share?

3[.]{.c19}An NFS server exports a share and an NFS server mounts it.
True or False?

4[.]{.c19}Which command would you use to unexport a share?

5[.]{.c19}What is the name of the NFS server configuration file?

6[.]{.c19}What would the line entry */dir1 \*(rw)* in the **etc*exports*
file mean?

7[.]{.c19}What type of AutoFS map would have the */- *etc*auto.media*
entry in the *auto.master* file?

8[.]{.c19}AutoFS requires *root* privileges to automatically mount a
network file system. True or False?

9[.]{.c19}What is the default timeout value for a file system before
AutoFS unmounts it automatically?

10[.]{.c23}Name the three common types of maps that AutoFS support.

11[.]{.c23}Arrange the tasks in three different correct sequences to
export a share using NFS: (a) update **etc*exports*, (b) add service to
firewalld, (c) run *exportfs*, (d) install *nfs-utils*, and (e) start
nfs service.

12[.]{.c23}What is the name of the AutoFS configuration file and where
is it located?

13[.]{.c23}The name of the AutoFS service daemon is *autofs*. True or
False?

**[Answers]{#part0029_split_001.html#id_482 .calibre10} to Review
Questions**

1[.]{.c19}This indirect map entry would mount individual user home
directories from *server10*.

2[.]{.c19}The *exportfs* command.

3[.]{.c19}True.

4[.]{.c19}The *exportfs* command with the -u switch.

5[.]{.c19}The **etc*nfs.conf* file.

6[.]{.c19}The line entry would export */dir1* in read/write mode to all
systems.

7[.]{.c19}A direct map.

8[.]{.c19}False.

9[.]{.c19}Five minutes.

10[.]{.c23}The three common AutoFS maps are master, direct, and
indirect.

11[.]{.c23}d/e/b/a/c, d/e/a/c/b, or d/e/a/b/c.

12[.]{.c23}The name of the AutoFS configuration file is *autofs.conf*
and it is located in the */etc* directory.

13[.]{.c23}False. The name of the AutoFS service daemon is *automount*.

**[Do-]{#part0029_split_001.html#id_483 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

**Lab 17-1: Configure NFS Share and Automount with Direct Map**

As *user1* with *sudo* on *server10*, share directory */sharenfs*
(create it) in read/write mode using NFS. On *server20* as *user1* with
*sudo*, install the AutoFS software and start the service. Configure the
master and a direct map to automount the share on */mntauto* (create
it). Run *ls* on */mntauto* to trigger the mount. Use **df -h** to
confirm. (Hint: NFS Server and Client Configuration, and Auto File
System).

**[Lab]{#part0029_split_001.html#id_484 .calibre10} 17-2: Automount NFS
Share with Indirect Map**

As *user1* with *sudo* on *server20*, configure the master and an
indirect map to automount the share under */autoindir* (create it). Run
*ls* on */autoindir/sharenfs* to trigger the mount. Use **df -h** to
confirm. (Hint: Auto File System).

[]{#part0030_split_000.html}

## Chapter 18 {#part0030_split_000.html#calibre_pb_0 .calibre11}

**Time Synchronization and Hostname Resolution**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Describe time synchronization and
the role of Network Time Protocol

[![](images/00001.jpeg){.c37}]{.c36}Comprehend the terms: time source,
NTP roles, and stratum levels

[![](images/00001.jpeg){.c37}]{.c36}Anatomy of the Chrony service
configuration file

[![](images/00001.jpeg){.c37}]{.c36}Configure and verify NTP/Chrony
client service

[![](images/00001.jpeg){.c37}]{.c36}View and set system date and time

[![](images/00001.jpeg){.c37}]{.c36}Overview of Domain Name System and
hostname resolution

[![](images/00001.jpeg){.c37}]{.c36}Understand various DNS roles

[![](images/00001.jpeg){.c37}]{.c36}Analyze entries in resolver
configuration files

[![](images/00001.jpeg){.c37}]{.c36}Perform name resolution using a
variety of lookup tools

[RHCSA Objectives:]{.c39}

43\. Configure time service clients

[48]{#part0030_split_000.html#id_809 .calibre10}. Configure hostname
resolution

::: {#part0030_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0030_split_001.html}

[ T]{.c42}[he]{.c43} Chrony service, a new implementation of the Network
Time Protocol, maintains the clock on the system and keeps it
synchronized with a more accurate and reliable source of time. Providing
accurate and uniform time for systems on the network allows
time-sensitive applications such as monitoring software, backup tools,
scheduling utilities, billing systems, file sharing protocols, and
authentication programs to perform correctly and precisely. It also aids
logging and auditing services to capture and record messages and alerts
in log files with accurate timestamps.

Domain Name System is an OS-and hardware-independent network service
employed for determining the IP address of a system when its hostname is
known, and vice versa. This mechanism is implemented to map
human-friendly hostnames to their assigned numeric IP addresses by
consulting one or more servers offering the hostname resolution service.
This service has been used on the Internet and corporate networks as the
de facto standard for this purpose. DNS clients use this service to
communicate with remote systems. There are several lookup programs that
use DNS to obtain information.

**[Time]{#part0030_split_001.html#id_485 .calibre10} Synchronization**

*Network Time Protocol* (NTP) is a networking protocol for synchronizing
the system clock with remote time servers for accuracy and reliability.
This protocol has been in use with tens of millions of computing devices
employing it to synchronize their clocks with tens of thousands of time
servers deployed across the globe. Having steady and exact time on
networked systems allows time-sensitive applications, such as
authentication and email applications, backup and scheduling tools,
financial and billing systems, logging and monitoring software, and file
and storage sharing protocols, to function with precision.

NTP sends a stream of messages to configured time servers and binds
itself to the one with least amount of delay in its responses, the most
accurate, and may or may not be the closest distance-wise. The client
system maintains a drift in time in a file and references this file for
gradual drop in inaccuracy.

RHEL version 8 introduces a new implementation of NTP called *Chrony*.
Chrony uses the UDP protocol over the well-known port 123. If enabled,
it starts at system boot and continuously operates to keep the system
clock in sync with a more accurate source of time.

Chrony performs well on computers that are occasionally connected to the
network, attached to busy networks, do not run all the time, or have
variations in temperature.

**[Time]{#part0030_split_001.html#id_486 .calibre10} Sources**

A *time source* is any reference device that acts as a provider of time
to other devices. The most precise sources of time are the atomic
clocks. They use *Universal Time Coordinated* (UTC) for time accuracy.
They produce radio signals that radio clocks use for time propagation to
computer servers and other devices that require correctness in time.
When choosing a time source for a network, preference should be given to
the one that takes the least amount of time to respond. This server may
or may not be closest physically.

The common sources of time employed on computer networks are the local
system clock, an Internet-based public time server, and a radio clock.

The local system clock can be used as a provider of time. This requires
the maintenance of correct time on the server either manually or
automatically via *cron*. Keep in mind that this server has no way of
synchronizing itself with a more reliable and precise external time
source. Therefore, using the local system clock as a time server is the
least recommended option.

Several public time servers are available over the Internet for general
use (visit *[www.ntp.org](http://www.ntp.org){.calibre5}* for a list).
These servers are typically operated by government agencies, research
and scientific organizations, large software vendors, and universities
around the world. One of the systems on the local network is identified
and configured to receive time from one or more public time servers.
This option is preferred over the use of the local system clock.

The official *[ntp.org](http://ntp.org){.calibre5}* site also provides a
common pool called *[pool.ntp.org](http://pool.ntp.org){.calibre5}* for
vendors and organizations to register their own NTP servers voluntarily
for public use. Examples include
*[rhel.pool.ntp.org](http://rhel.pool.ntp.org){.calibre5}* and
*[ubuntu.pool.ntp.org](http://ubuntu.pool.ntp.org){.calibre5}* for
distribution-specific pools, and
*[ca.pool.ntp.org](http://ca.pool.ntp.org){.calibre5}* and
*[oceania.pool.ntp.org](http://oceania.pool.ntp.org){.calibre5}* for
country and continent/region-specific pools. Under these sub-pools, the
owners maintain multiple time servers with enumerated hostnames such as
*[0.rhel.pool.ntp.org](http://0.rhel.pool.ntp.org){.calibre5}*,*[1.rhel.pool.ntp.org](http://1.rhel.pool.ntp.org){.calibre5}*,
*[2.rhel.pool.ntp.org](http://2.rhel.pool.ntp.org){.calibre5}*, and so
on.

A radio clock is regarded as the perfect provider of time, as it
receives time updates straight from an atomic clock. *Global Positioning
System* (GPS), WWVB, and DCF77 are some popular radio clock methods. A
direct use of signals from these sources requires connectivity of some
hardware to the computer identified to act as an organizational or
site-wide time server.

**[NTP]{#part0030_split_001.html#id_487 .calibre10} Roles**

From an NTP standpoint, a system can be configured to operate as a
primary server, secondary server, peer, or client.

A *primary* server gets time from a time source and provides time to
secondary servers or directly to clients.

A *secondary* server receives time from a primary server and can be
configured to furnish time to a set of clients to offload the primary or
for redundancy. The presence of a secondary server on the network is
optional but highly recommended.

A *peer* reciprocates time with an NTP server. All peers work at the
same stratum level, and all of them are considered equally reliable.
Both primary and secondary servers can be peers of each other.

A *client* receives time from a primary or a secondary server and
adjusts its clock accordingly.

**[Stratum]{#part0030_split_001.html#id_488 .calibre10} Levels**

As mentioned, there are different types of time sources available so you
can synchronize the system clock. These time sources are categorized
hierarchically into several levels that are referred to as *stratum
levels* based on their distance from the reference clocks (atomic,
radio, and GPS). The reference clocks operate at stratum level 0 and are
the most accurate provider of time with little to no delay. Besides
stratum 0, there are fifteen additional levels that range from 1 to 15.
Of these, servers operating at stratum 1 are considered perfect, as they
get time updates directly from a stratum 0 device. See [Figure
18-1](#part0030_split_001.html#page_420){.calibre5} for a sample
hierarchy.

::: c49
::: width_
![](images/00896.jpeg){.calibre13}
:::

**Figure 18-1 NTP Stratum Levels**
:::

A stratum 0 device cannot be used on the network directly. It is
attached to a computer, which is then configured to operate at
stratum 1. Servers functioning at stratum 1 are called *time servers*
and they can be set up to deliver time to stratum 2 servers. Similarly,
a stratum 3 server can be configured to synchronize its time with a
stratum 2 server and deliver time to the next lower level servers, and
so on. Servers sharing the same stratum can be configured as peers to
exchange time updates with one another.

![](images/00002.jpeg){.image} If a secondary server also gets time
updates from a stratum 1 server, it will act as a peer to the primary
server.

There are a number of public NTP servers available for free that
synchronize time. They normally operate at higher stratum levels such as
2 and 3.

**[Chrony]{#part0030_split_001.html#id_489 .calibre10} Configuration
File**

The key configuration file for the Chrony service is *chrony.conf*
located in the */etc* directory. This file is referenced by the Chrony
daemon at startup to determine the sources to synchronize the clock, the
log file location, and other details. This file can be modified by hand
to set or alter directives as required. Some common directives used in
this file along with real or mock values are presented below with an
explanation in [Table 18-1](#part0030_split_001.html#id_751){.calibre5}:

  ----------- ---------------------------------------------------------------------------
  driftfile   *var*lib/chrony/drift
  logdir      *var*log/chrony
  pool        [0.rhel.pool.ntp.org](http://0.rhel.pool.ntp.org){.calibre5} iburst
  server      [server20s8.example.com](http://server20s8.example.com){.calibre5} iburst
  server      127.127.1.0
  peer        [prodntp1.abc.net](http://prodntp1.abc.net){.calibre5}
  ----------- ---------------------------------------------------------------------------

[Table 18-1](#part0030_split_001.html#id_751){.calibre5} describes these
directives.

::: c49
  --------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Directive**   **Description**
  driftfile       Indicates the location and name of the drift file to be used to record the rate at which the system clock gains or losses time. This data is used by Chrony to maintain local system clock accuracy.
  logdir          Sets the directory location to store the log files in
  pool            Defines the hostname that represents a pool of time servers. Chrony binds itself with one of the servers to get updates. In case of a failure of that server, it automatically switches the binding to another server within the pool.
                  The iburst option dictates the Chrony service to send the first four update requests to the time server every 2 seconds. This allows the daemon to quickly bring the local clock closer to the time server at startup.
  server          Defines the hostname or IP address of a single time server. The IP 127.127.1.0 is a special address that epitomizes the local system clock.
  peer            Identifies the hostname or IP address of a time server running at the same stratum level. A peer provides time to a server as well as receives time from the same server.
  --------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0030_split_001.html#id_751} 18-1 Chrony Directives**
:::

There are plenty of other directives and options available with Chrony
that may be defined in this file. Use **man chrony.conf** for details.

**[Chrony]{#part0030_split_001.html#id_490 .calibre10} Daemon and
Command**

The Chrony service runs as a daemon program called *chronyd* that
handles time synchronization in the background. It uses the
configuration defined in the **etc*chrony.conf* file at startup and sets
its behavior accordingly. If the local clock requires a time adjustment,
Chrony takes multiple small steps toward minimizing the gap rather than
doing it abruptly in a single step. There are a number of additional
options available that may be passed to the service daemon if required.

The Chrony service has a command line program called *chronyc* available
that can be employed to monitor the performance of the service and
control its runtime behavior. There are a few subcommands available with
*chronyc*; the *sources* and *tracking* subcommands list current sources
of time and view performance statistics, respectively.

**[Exercise]{#part0030_split_001.html#id_491 .calibre10} 18-1: Configure
NTP Client**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will install the Chrony software package and
activate the service without making any changes to the default
configuration. You will validate the binding and operation.

1[.]{.c19}Install the Chrony package using the *dnf* command:

![](images/00897.jpeg){.image2}

The software is already installed on the system.

2[.]{.c19}Ensure that preconfigured public time server entries are
present in the **etc*chrony.conf* file:

![](images/00898.jpeg){.image2}

There is a single pool entry set in the file by default. This pool name
is backed by multiple NTP servers behind the scene.

3[.]{.c19}Start the Chrony service and set it to autostart at reboots:

![](images/00899.jpeg){.image2}

4[.]{.c19}Examine the operational status of Chrony:

![](images/00900.jpeg){.image2}

The service has started successfully and it is set for autostart.

5[.]{.c19}Inspect the binding status using the *sources* subcommand with
*chronyc*:

![](images/00901.jpeg){.image2}

The output shows the number of available time sources in row 1 and rest
of the information in eight columns. Columns 1 to 4---M, S, Name/IP, and
Stratum---illustrate the mode, state, name/IP, and stratum level of the
source. The \^ means server and the \* implies current association.

Columns 4 to 8---Poll, Reach, LastRx, and Last Sample---display the
polling rate (6 means 64 seconds), reachability register (377 indicates
a valid response was received), how long ago the last sample was
received, and the offset between the local clock and the source at the
last measurement. Check out the manual pages of the *chronyc* command
and search for the section 'sources' for additional details.

The last line in the output depicts the *server10* binding with time
server *[gpg.n1zyy.com](http://gpg.n1zyy.com){.calibre5}*. This
association is identified with the asterisk character (\*) beside the
time server.

6[.]{.c19}Display the clock performance using the *tracking* subcommand
with *chronyc*:

![](images/00902.jpeg){.image2}

Lines 1 and 2 in the above output identify the current source of time
(Reference ID) and the stratum level it is configured at (Stratum). Line
3 shows the reference time at which the last measurement from the time
source was processed (Ref time). Line 4 displays the local time offset
from NTP time (System time). Line 5 depicts the last reported offset
from the NTP server (Last offset). Line 6 identifies the frequency at
which time adjustments are occurring (Frequency). The rest of the lines
in the output show additional information. Check out the manual pages of
the *chronyc* command and search for the section "tracking" for
additional details.

[**EXAM TIP:**]{.c56} You will not have access to the outside network
during the exam, so you will need to point your system to an NTP server
available on the exam network. Simply comment the default server/pool
directive(s) and add a single directive "server \<hostname\>" to the
file. Replace \<hostname\> with the NTP server name or its IP address as
provided.

The concludes the exercise.

**[Displaying]{#part0030_split_001.html#id_492 .calibre10} and Setting
System Date and Time**

System date and time can be viewed and manually adjusted with native
Linux tools such as the *timedatectl* command. This command can modify
the date, time, and time zone. When executed without any option, as
shown below, it outputs the local time, Universal time, RTC time
(*real-time clock*, a battery-backed hardware clock located on the
system board), time zone, and the status of NTP:

![](images/00903.jpeg){.image2}

This command requires that the NTP/Chrony service is deactivated in
order to make time adjustments. Run the *timedatectl* command as follows
to turn off NTP and verify:

![](images/00904.jpeg){.image2}

To modify the current date to January 1, 2020, and confirm:

![](images/00905.jpeg){.image2}

To change the time to 11:20 p.m. and date to November 18, 2019:

![](images/00906.jpeg){.image2}

To reactivate NTP:

![](images/00907.jpeg){.image2}

Check out the manual pages of the *timedatectl* command for more
subcommands and usage examples.

Alternatively, you can use the *date* command to view or modify the
system date and time.

To view current date and time:

![](images/00908.jpeg){.image2}

To change the date and time to November 22, 2019 1:00 p.m.:

![](images/00909.jpeg){.image2}

There are many options available with the *date* command. Consult its
manual pages for details.

**[DNS]{#part0030_split_001.html#id_493 .calibre10} and Name
Resolution**

*Domain Name System* (DNS) is an inverted tree-like structure employed
on the Internet and private networks (including home and corporate
networks) as the de facto standard for resolving hostnames to their
numeric IP addresses. DNS is platform-independent with support
integrated in every operating system. DNS is also referred to as BIND,
*Berkeley Internet Name Domain*, which is an implementation of DNS, and
it has been the most popular DNS application in use. *Name resolution*
is the technique that uses DNS/BIND for hostname lookups.

In order to understand DNS, a brief discussion of its components and
roles is imperative. The following subsections provide a look at the
client-side configuration files and commands, along with examples on how
to use the tools for resolving hostnames.

**[DNS]{#part0030_split_001.html#id_494 .calibre10} Name Space and
Domains**

The DNS *name space* is a hierarchical organization of all the domains
on the Internet. The root of the name space is represented by a period
(.). The hierarchy below the root (.) denotes the *top-level domains*
(TLDs) with names such as .com, .net, .edu, .org, .gov, .ca, and .de. A
DNS *domain* is a collection of one or more systems. Subdomains fall
under their parent domains and are separated by a period (.). For
example, *[redhat.com](http://redhat.com){.calibre5}* is a second-level
subdomain that falls under .com, and
*[bugzilla.redhat.com](http://bugzilla.redhat.com){.calibre5}* is a
third-level subdomain that falls under
*[redhat.com](http://redhat.com){.calibre5}*.

[Figure 18-2](#part0030_split_000.html#page_426){.calibre5} exhibits a
sample hierarchy of the name space, showing the top three domain levels.

::: c49
![](images/00910.jpeg){.image2}

**[Figure]{#part0030_split_001.html#id_752} 18-2 Sample DNS Hierarchy**
:::

At the deepest level of the hierarchy are the *leaves* (systems, nodes,
or any device with an IP address) of the name space. For example, a
network switch *net01* in *.travel.gc.ca* subdomain will be known as
*net01.travel.gc.ca*. If a period (.) is added to the end of this name
to look like *net01.travel.gc.ca.*, it will be referred to as the *Fully
Qualified Domain Name* (FQDN) for *net01*.

**[DNS]{#part0030_split_001.html#id_495 .calibre10} Roles**

From a DNS perspective, a system can be configured to operate as a
primary server, secondary server, or client. A DNS server is also
referred to as a *nameserver*.

A *primary* (a.k.a. *master*) *server* is responsible for its domain (or
subdomain). It maintains a master database of all the hostnames and
their associated IP addresses that are included in that domain. Any
changes in the database is done on this server. Each domain must have
one primary server with one or more optional *secondary* (a.k.a.
*slave*) servers for load balancing and redundancy. A secondary server
also stores an updated copy of the master database and it continues to
provide name resolution service in the event the primary server becomes
unavailable or inaccessible.

A *DNS client* queries nameservers for name lookups. Every system with
access to the Internet or other external networks will have the DNS
client functionality configured and operational. Setting up DNS client
on Linux involves two text files that are discussed in the next two
subsections.

**[Understanding]{#part0030_split_001.html#id_496 .calibre10} Resolver
Configuration File**

The *resolv.conf* file under */etc* is the DNS resolver configuration
file where information to support hostname lookups is defined. This file
may be edited manually with a text editor. It is referenced by resolver
utilities to construct and transmit queries. There are three key
directives set in this file--- domain, nameserver, and search---and they
are described in [Table
18-2](#part0030_split_001.html#id_766){.calibre5}.

::: c49
  --------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Directive**   **Description**
  domain          Identifies the default domain name to be searched for queries
  nameserver      Declares up to three DNS server IP addresses to be queried one at a time in the order in which they are listed. Nameserver entries may be defined as separate line items with the directive or on a single line.
  search          Specifies up to six domain names, of which the first must be the local domain. No need to define the domain directive if the search directive is used.
  --------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0030_split_001.html#id_766} 18-2 The Resolver
Configuration File**
:::

A sample entry showing the syntax is provided below for reference:

  ------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  domain       [example.com](http://example.com){.calibre5}
  search       [example.net](http://example.net){.calibre5} [example.org](http://example.org){.calibre5} [example.edu](http://example.edu){.calibre5} [example.gov](http://example.gov){.calibre5}
  nameserver   192.168.0.1 8.8.8.8 8.8.4.4
  ------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

A variation of the above would be:

  ------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  domain       [example.com](http://example.com){.calibre5}
  search       [example.net](http://example.net){.calibre5} [example.org](http://example.org){.calibre5} [example.edu](http://example.edu){.calibre5} [example.gov](http://example.gov){.calibre5}
  nameserver   192.168.0.1
  nameserver   8.8.8.8
  nameserver   8.8.4.4
  ------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Currently, there are two entries "search
[example.com](http://example.com){.calibre5}" and "nameserver
192.168.0.1" defined in the *resolv.conf* file on *server10* and
*server20*.

![](images/00911.jpeg){.image2}

On a system with this file absent, the resolver utilities only query the
nameserver configured on the localhost, determine the domain name from
the hostname of the system, and construct the search path based on the
domain name.

**Viewing and Adjusting Name Resolution Sources and Order** The
*nsswitch.conf* file under */etc* directs the lookup utilities to the
correct source to get hostname information. In the presence of multiple
sources, this file also identifies the order in which to consult them
and an action to be taken next. There are four keywords---success,
notfound, unavail, and tryagain---that oversee this behavior, and are
described along with default actions in [Table
18-3](#part0030_split_001.html#id_753){.calibre5}.

::: c49
  ------------- ------------------------------------------------------------------- -------------------------------------
  **Keyword**   **Meaning**                                                         **Default Action**
  success       Information found in source and provided to the requester           return (do not try the next source)
  notfound      Information not found in source                                     continue (try the next source)
  unavail       Source down or not responding; service disabled or not configured   continue (try the next source)
  tryagain      Source busy, retry later                                            continue (try the next source)
  ------------- ------------------------------------------------------------------- -------------------------------------

**[Table]{#part0030_split_001.html#id_753} 18-3 Name Service Source and
Order Determination**
:::

The following example entry shows the syntax of a relevant entry from
the *nsswitch.conf* file. It shows two sources for name resolution:
files (**etc*hosts*) and DNS (**etc*resolv.conf*).

  -------- ------- -----
  hosts:   files   dns
  -------- ------- -----

Based on the default behavior, the search will terminate if the
requested information is found in the *hosts* table. However, you can
alter this behavior and instruct the lookup programs to return if the
requested information is not found there. The modified entry will look
like:

  -------- --------------------------- -----
  hosts:   files \[notfound=return\]   dns
  -------- --------------------------- -----

This altered entry will ignore the DNS.

![](images/00002.jpeg){.image} See [Chapter
16](#part0028_split_000.html#page_375){.calibre5} for details on the
*etc*hosts file.

Once the *resolv.conf* and *nsswitch.conf* files are configured
appropriately, you can use any of the native client resolver tools for
lookups. Common query tools available in RHEL 8 include *dig*, *host*,
*nslookup*, and *getent*.

**[Performing]{#part0030_split_001.html#id_497 .calibre10} Name
Resolution with dig**

*dig* (*domain information groper*) is a DNS lookup utility. It queries
the nameserver specified at the command line or consults the
*resolv.conf* file to determine the nameservers to be queried. This tool
may be used to troubleshoot DNS issues due to its flexibility and
verbosity. The following shows a few usage examples.

To get the IP for *redhat.com* using the nameserver listed in the
*resolv.conf* file:

![](images/00912.jpeg){.image2}

The output shows the total time (13 milliseconds) it took to get the
result, the IP address (209.132.183.105) of
[redhat.com](http://redhat.com){.calibre5}, the nameserver IP
(192.168.0.1) used for the query, the DNS port number (53), query
timestamp, the size of the received message, and other information.

To perform a reverse lookup on the
[redhat.com](http://redhat.com){.calibre5} IP (209.132.183.105), use the
-x option with the command:

![](images/00913.jpeg){.image2}

![](images/00914.jpeg){.image2}

Reference the command's manual pages for details and options.

**[Performing]{#part0030_split_001.html#id_498 .calibre10} Name
Resolution with host**

*host* is an elementary DNS lookup utility that works on the same
principles as the *dig* command in terms of nameserver determination.
This tool produces lesser data in the output by default; however, you
can add the -v option for verbosity.

To perform a lookup on
*[redhat.com](http://redhat.com:){.calibre5}*[:](http://redhat.com:){.calibre5}

![](images/00915.jpeg){.image2}

Rerun the above with -v added. The output will be similar to that of the
*dig* command.

To perform a reverse lookup on the IP of
*[redhat.com](http://redhat.com){.calibre5}* using the -v flag to add
details:

![](images/00916.jpeg){.image2}

Refer to the command's manual pages for options and more information.

**[Performing]{#part0030_split_001.html#id_499 .calibre10} Name
Resolution with nslookup**

*nslookup* queries the nameservers listed in the *resolv.conf* file or
specified at the command line. The following shows a few usage examples.

To get the IP for *[redhat.com](http://redhat.com){.calibre5}* using
nameserver 8.8.8.8 instead of the nameserver defined in *resolv.conf*:

![](images/00917.jpeg){.image2}

To perform a reverse lookup on the IP of
*[redhat.com](http://redhat.com){.calibre5}* using the nameserver from
the resolver configuration file:

![](images/00918.jpeg){.image2}

Consult the command's manual pages on how to use it in interactive mode.

**[Performing]{#part0030_split_001.html#id_500 .calibre10} Name
Resolution with getent**

The *getent* (*get entries*) command is a rudimentary tool that can
fetch matching entries from the databases defined in the *nsswitch.conf*
file. This command reads the corresponding database and displays the
information if found. For name resolution, use the *hosts* database and
*getent* will attempt to resolve the specified hostname or IP address.
For instance, run the following for forward and reverse lookups:

![](images/00919.jpeg){.image2}

Check the command's manual pages for available flags and additional
information.

**[Chapter]{#part0030_split_001.html#id_501 .calibre10} Summary**

The focus of this chapter was on two topics: network time
synchronization and hostname resolution.

The chapter began with a discussion of Network Time Protocol, what role
it plays in keeping the clocks synchronized, and what is its
relationship with the Chrony service. We explored various sources for
obtaining time, different roles that systems could play, and the strata
paradigm. We analyzed the configuration file to understand some key
directives and their possible settings. We performed an exercise to
configure the service, display clock association, and analyze the
results. We also employed a couple of other RHEL tools to display the
system time and set it instantly.

We concluded the chapter with a deliberation of DNS and name resolution.
We discussed the concepts and roles, analyzed the resolver configuration
file, and examined the source/order determination file. We added
required entries to the resolver configuration file and tested the
functionality by employing client tools for hostname lookup.

**[Review]{#part0030_split_001.html#id_502 .calibre10} Questions**

1[.]{.c19}Chrony is an implementation of the Network Time Protocol. True
or False?

2[.]{.c19}What is the name and location of the DNS resolver file?

3[.]{.c19}What stratum level do two peer systems operate on a network?

4[.]{.c19}Provide the maximum number of nameservers that can be defined
in the resolver configuration file.

5[.]{.c19}What is the purpose of a drift file in Chrony/NTP?

6[.]{.c19}What is a relative distinguished name?

7[.]{.c19}What would you add to the Chrony configuration file if you
want to use the local system clock as the provider of time?

8[.]{.c19}BIND is an implementation of Domain Name System. True or
False?

9[.]{.c19}Define time source.

10[.]{.c23}Name the three DNS roles that a RHEL system can play.

11[.]{.c23}What would you run to check the NTP bind status with time
servers?

12[.]{.c23}Define DNS name space.

13[.]{.c23}Name the four Chrony/NTP roles that a RHEL system can play.

14[.]{.c23}Which file defines the name resolution sources and controls
the order in which they are consulted?

15[.]{.c23}What is the filename and directory location for the Chrony
configuration file?

16[.]{.c23}The Chrony client is preconfigured when the *chrony* software
package is installed. You just need to start the service to synchronize
the clock. True or False?

17[.]{.c23}List any three DNS lookup tools.

18[.]{.c23}List two utilities that you can use to change system time.

**[Answers]{#part0030_split_001.html#id_503 .calibre10} to Review
Questions**

1[.]{.c19}True.

2[.]{.c19}The DNS resolver file is called *resolv.conf* and it is
located in the */etc* directory.

3[.]{.c19}Two peers on a network operate at the same stratum level.

4[.]{.c19}Three.

5[.]{.c19}The purpose of a drift file is to keep track of the rate at
which the system clock gains or losses time.

6[.]{.c19}A relative distinguished name represents individual components
of a distinguished name.

7[.]{.c19}You will add "server 127.127.1.0" to the Chrony configuration
file and restart the service.

8[.]{.c19}True.

9[.]{.c19}A time source is a reference device that provides time to
other devices.

10[.]{.c23}From a DNS perspective, a RHEL machine can be a primary
server, a secondary server, or a client.

11[.]{.c23}You will run *chronyc sources* to check the binding status.

12[.]{.c23}DNS name space is a hierarchical organization of all the
domains on the Internet.

13[.]{.c23}From a Chrony/NTP standpoint, a RHEL machine can be a primary
server, a secondary server, a peer, or a client.

14[.]{.c23}The **etc*nsswitch.conf* file.

15[.]{.c23}The name of the Chrony configuration file is *chrony.conf*
and it is located in the */etc* directory.

16[.]{.c23}True.

17[.]{.c23}Name resolution tools are *dig*, *host*, *getent*, and
*nslookup*.

18[.]{.c23}The *timedatectl* and *date* commands can be used to modify
the system time.

**Do-It-Yourself Challenge Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

**[Lab]{#part0030_split_001.html#id_504 .calibre10} 18-1: Modify System
Date and Time**

As *user1* with *sudo* on *server10*, execute the *date* and
*timedatectl* commands to check the current system date and time.
Identify the distinctions between the two outputs. Use *timedatectl* and
change the system date to a future date. Issue the *date* command and
change the system time to one hour ahead of the current time. Observe
the new date and time with both commands. Reset the date and time to the
current actual time using either the *date* or the *timedatectl*
command. (Hint: Time Synchronization).

**[Lab]{#part0030_split_001.html#id_505 .calibre10} 18-2: Configure
Chrony**

As *user1* with *sudo* on *server10*, install Chrony and mark the
service for autostart on reboots. Edit the Chrony configuration file and
comment all line entries that begin with "pool" or "server". Go to the
end of the file, and add a new line "server 127.127.1.0". Start the
Chrony service and run **chronyc sources** to confirm the binding.
(Hint: Time Synchronization).

[]{#part0031_split_000.html}

## Chapter 19 {#part0031_split_000.html#calibre_pb_0 .calibre11}

**The Secure Shell Service**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Understand the OpenSSH service,
versions, and algorithms

[![](images/00001.jpeg){.c37}]{.c36}Overview of encryption techniques
and authentication methods

[![](images/00001.jpeg){.c37}]{.c36}Describe OpenSSH administration
commands and configuration files

[![](images/00001.jpeg){.c37}]{.c36}Configure private/public key-based
authentication

[![](images/00001.jpeg){.c37}]{.c36}Access OpenSSH server from other
Linux systems

[![](images/00001.jpeg){.c37}]{.c36}Use OpenSSH client tools to transfer
files

[![](images/00001.jpeg){.c37}]{.c36}Synchronize files remotely over
OpenSSH

[RHCSA Objectives:]{.c39}

[04]{#part0031_split_000.html#id_770 .calibre10}. Access remote systems
using ssh

[26]{#part0031_split_000.html#id_792 .calibre10}. Securely transfer
files between systems

[57]{#part0031_split_000.html#id_821 .calibre10}. Configure key-based
authentication for SSH

::: {#part0031_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0031_split_001.html}

[ S]{.c42}[ecure]{.c43} Shell is a network service that delivers a
secure mechanism for data transmission between source and destination
systems over insecure network paths. It provides a set of utilities that
allows users to generate key pairs and use them to set up trusted logins
between systems for themselves. Additional utilities in the set gives
remote users the ability to log in, execute commands, and transfer files
securely over encrypted network channels. These tools have predominantly
supplanted their insecure counterparts in the corporate world.

**[The]{#part0031_split_001.html#id_506 .calibre10} OpenSSH Service**

*Secure Shell* (SSH) delivers a secure mechanism for data transmission
between source and destination systems over IP networks. It was designed
to replace the old remote login programs that transmitted user passwords
in clear text and data unencrypted. SSH employs digital signatures for
user authentication with encryption to secure a communication channel.
As a result, this makes it extremely hard for unauthorized people to
gain access to passwords or the data in transit. It also monitors the
data being transferred throughout a session to ensure integrity. SSH
includes a set of utilities for remote users to log in, transfer files,
and execute commands securely. Due to strong security features, SSH
utilities have supplanted their conventional, unsecure login and file
transfer counterpart programs.

*OpenSSH* is a free, open source implementation of the proprietary SSH.
Once applied successfully on the system, the unsecure
services---*telnet*, *rlogin*, *rcp*, and *ftp*---can be disabled after
a careful examination to eliminate potential impact. The secure command
that has substituted *telnet* and *rlogin* remote login services is
called *ssh*, and those for *rcp* and *ftp* are called *scp* and *sftp*,
respectively.

**[Common]{#part0031_split_001.html#id_507 .calibre10} Encryption
Techniques**

*Encryption* is a way of scrambling information with the intent to
conceal the real information from unauthorized access. OpenSSH can
utilize various encryption techniques during an end-to-end communication
session between two entities (client and server). The two common
techniques are *symmetric* and *asymmetric*. They are also referred to
as *secret key encryption* and *public key encryption* techniques.

**Symmetric Technique**

This technique uses a single key called a *secret* key that is generated
as a result of a negotiation process between two entities at the time of
their initial contact. Both sides use the same secret key during
subsequent communication for data encryption and decryption.

**Asymmetric Technique**

This technique uses a combination of *private* and *public* keys, which
are randomly generated and mathematically related strings of
alphanumeric characters attached to messages being exchanged. The client
transmutes the information with a *public* key and the server decrypts
it with the paired *private* key. The private key must be kept secure
since it is private to a single sender; the public key is disseminated
to clients. This technique is used for channel encryption as well as
user authentication.

**Authentication Methods**

Once an encrypted channel is established between the client and server,
additional negotiations take place between the two to authenticate the
user trying to access the server. OpenSSH offers several methods for
this purpose; they are listed below in the order in which they are
attempted during the authentication process:

[]{.c85}GSSAPI-based (*Generic Security Service Application Program
Interface*) authentication

[]{.c85}Host-based authentication

[]{.c85}Public key-based authentication

[]{.c85}Challenge-response authentication

[]{.c85}Password-based authentication

Let's review each one in detail.

**GSSAPI-Based Authentication**

GSSAPI provides a standard interface that allows security mechanisms,
such as Kerberos, to be plugged in. OpenSSH uses this interface and the
underlying Kerberos for authentication. With this method, an exchange of
tokens takes place between the client and server to validate user
identity.

**Host-Based Authentication**

This type of authentication allows a single user, a group of users, or
all users on the client to be authenticated on the server. A user may be
configured to log in with a matching username on the server or as a
different user that already exists there. For each user that requires an
automatic entry on the server, a \~/*.shosts* file is set up containing
the client name or IP address, and, optionally, a different username.

The same rule applies to a group of users or all users on the client
that require access to the server. In that case, the setup is done in
the **etc*ssh/shosts.equiv* file on the server.

**Private/Public Key-Based Authentication**

This method uses a private/public key combination for user
authentication. The user on the client has a public key and the server
stores the corresponding private key. At the login attempt, the server
prompts the user to enter the passphrase associated with the key and
logs the user in if the passphrase and key are validated.

**ChallengeResponse Authentication**

This method is based on the response(s) to one or more arbitrary
challenge questions that the user has to answer correctly in order to be
allowed to log in to the server.

**Password-Based Authentication**

This is the last fall back option. The server prompts the user to enter
their password. It checks the password against the stored entry in the
*shadow* file and allows the user in if the password is confirmed.

Of the five authentication methods, the password-based method is common
and requires no further explanation. The GSSAPI-based, host-based, and
challenge-response methods are beyond the scope of this book. The
public/private authentication and encryption methods will be the focus
in the remainder of this chapter.

**OpenSSH Protocol Version and Algorithms**

OpenSSH has evolved over the years. Its latest and the default version
in RHEL 8, version 2, has numerous enhancements, improvements, and
sophisticated configuration options. It supports various algorithms for
data encryption and user authentication (digital signatures) such as
RSA, DSA, and ECDSA. RSA is more common and it is widely employed partly
because it supports both encryption and authentication. In contrast, DSA
and ECDSA are restricted to authentication only. These algorithms are
used to generate public and private key pairs for the asymmetric
technique.

RSA stands for *Rivest-Shamir-Adleman*, who first published this
algorithm, DSA for *Digital Signature Algorithm*, and ECDSA is an
acronym for *Elliptic Curve Digital Signature Algorithm*.

**[OpenSSH]{#part0031_split_001.html#id_508 .calibre10} Packages**

OpenSSH has three software packages that are of interest. These are
*openssh*, *openssh-clients*, and *openssh-server*. The *openssh*
package provides the *ssh-keygen* command and some library routines; the
*openssh-clients* package includes commands, such as *scp*, *sftp*,
*ssh*, and *ssh-copy-id*, and a client configuration file
**etc*ssh/ssh_config*; and the *openssh-server* package contains the
*sshd* service daemon, server configuration file **etc*ssh/sshd_config*,
and library routines. By default, all three packages are installed
during OS installation.

**[OpenSSH]{#part0031_split_001.html#id_509 .calibre10} Server Daemon
and Client Commands**

The OpenSSH server program *sshd* is preconfigured and operational on
new RHEL installations, allowing remote users to log in to the system
using an ssh client program such as PuTTY or the *ssh* command. This
daemon listens on well-known TCP port 22 as documented in the
**etc*ssh/sshd_config* file with the Port directive.

The client software includes plenty of utilities such as those listed
and described in [Table
19-1](#part0031_split_001.html#id_754){.calibre5}.

::: c49
  ------------- -------------------------------------------------------------------------------------
  **Command**   **Description**
  scp           The secure remote copy command that replaced the non-secure rcp command
  sftp          The secure remote copy command that replaced the non-secure ftp command
  ssh           The secure remote login command that replaced non-secure telnet and rlogin commands
  ssh-copy-id   Copies public key to remote systems
  ssh-keygen    Generates and manages private and public key pairs
  ------------- -------------------------------------------------------------------------------------

**[Table]{#part0031_split_001.html#id_754} 19-1 OpenSSH Client Tools**
:::

The use of these commands is demonstrated in the following subsections.

**[Server]{#part0031_split_001.html#id_510 .calibre10} Configuration
File**

The OpenSSH server daemon *sshd* has a configuration file that defines
default global settings on how it should operate. This file is located
in the **etc*ssh* directory and called *sshd_config*. There are a number
of directives preset in this file that affect all inbound ssh
communication and are tuned to work as-is for most use cases. In
addition, the **var*log/secure* log file is used to capture
authentication messages.

A few directives with their default values from the *sshd_config* file
are displayed below:

![](images/00920.jpeg){.image2}

  --------------------------------- ----------------------
  #Port                             22
  #Protocol                         2
  ListenAddress                     0.0.0.0
  SyslogFacility                    AUTHPRIV
  #LogLevel                         INFO
  PermitRootLogin                   yes
  #PubkeyAuthentication             yes
  AuthorizedKeysFile                .ssh/authorized_keys
  PasswordAuthentication            yes
  #PermitEmptyPasswords             no
  ChallengeResponseAuthentication   no
  UsePAM                            yes
  X11Forwarding                     yes
  --------------------------------- ----------------------

The above directives are elaborated in [Table
19-2](#part0031_split_001.html#id_755){.calibre5}.

::: c49
  --------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Directive**                     **Description**
  Port                              Specifies the port number to listen on. Default is 22.
  Protocol                          Specifies the default protocol version to use.
  ListenAddress                     Sets the local addresses the sshd service should listen on. Default is to listen on all local addresses.
  SyslogFacility                    Defines the facility code to be used when logging messages to the *var*log/secure file. This is based on the configuration in the *etc*rsyslog.conf file. Default is AUTHPRIV.
  LogLevel                          Identifies the level of criticality for the messages to be logged. Default is INFO.
  PermitRootLogin                   Allows or disallows the root user to log in directly to the system. Default is yes.
  PubKeyAuthentication              Enables or disables public key-based authentication. Default is yes.
  AuthorizedKeysFile                Sets the name and location of the file containing a user's authorized keys. Default is \~/.ssh/authorized_keys.
  PasswordAuthentication            Enables or disables local password authentication. Default is yes.
  PermitEmptyPasswords              Allows or disallows the use of null passwords. Default is no.
  ChallengeResponseAuthentication   Enables or disables challenge-response authentication mechanism. Default is yes.
  UsePAM                            Enables or disables user authentication via PAM. If enabled, only root will be able to run the sshd daemon. Default is yes.
  X11Forwarding                     Allows or disallows remote access to graphical applications. Default is yes.
  --------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0031_split_001.html#id_755} 19-2 OpenSSH Server
Configuration File**
:::

There are many more settings available that may be added to the file for
additional control. Check out the manual pages of the *sshd_config* file
(**man 5 sshd_config**) for details.

**Client Configuration File**

Each RHEL client machine that uses ssh to access a remote OpenSSH server
has a local configuration file that directs how the client should
behave. This file, *ssh_config*, is located in the **etc*ssh* directory.
There are a number of directives preset in this file that affect all
outbound ssh communication and are tuned to work as-is for most use
cases.

A few directives with their default values from the *ssh_config* file
are displayed below:

![](images/00921.jpeg){.image2}

  --------------------------- ----------------
  \# Host \*                  
  \# ForwardX11               no
  \# PasswordAuthentication   yes
  \# StrictHostKeyChecking    ask
  \# IdentityFile             \~/.ssh/id_rsa
  \# IdentityFile             \~/.ssh/id_dsa
  \# Port                     22
  \# Protocol                 2
  --------------------------- ----------------

The above directives are described in [Table
19-3](#part0031_split_001.html#id_756){.calibre5}.

::: c49
  ------------------------ --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Directive**            **Description**
  Host                     Container that declares directives applicable to one host, a group of hosts, or all hosts. It ends when another occurrence of Host or Match is encountered. Default is \*, which sets global defaults for all hosts.
  ForwardX11               Enables or disables automatic redirection of X11 traffic over SSH connections. Default is no.
  PasswordAuthentication   Allows or disallows password authentication. Default is yes.
  StrictHostKeyChecking    Controls (1) whether to add host keys (host fingerprints) to */.ssh/known_hosts when accessing a host for the first time, and (2) what to do when the keys of a previously-accessed host mismatch with what is stored in* /.ssh/known_hosts. Options are:
                           **no:** adds new host keys and ignores changes to existing keys. **yes:** adds new host keys and disallows connections to hosts with non-matching keys. **accept-new:** adds new host keys and disallows connections to hosts with non-matching keys. **ask (default):** prompts whether to add new host keys and disallows connections to hosts with non-matching keys.
  IdentityFile             Defines the name and location of a file that stores a user's private key for their identity validation. Defaults are id_rsa, id_dsa, and id_ecdsa based on the type of algorithm used. Their corresponding public key files with .pub extension are also stored at the same directory location.
  Port                     Sets the port number to listen on. Default is 22.
  Protocol                 Specifies the default protocol version to use
  ------------------------ --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0031_split_001.html#id_756} 19-3 OpenSSH Client
Configuration File**
:::

The *\~/.ssh* directory does not exist by default; it is created when a
user executes the *ssh-keygen* command for the first time to generate a
key pair or connects to a remote ssh server and accepts its host key for
the first time. In the latter case, the client stores the server's host
key locally in a file called *known_hosts* along with its hostname or IP
address. On subsequent access attempts, the client will use this
information to verify the server's authenticity.

There are a lot more settings available that may be added to the file
for additional control. Check out the manual pages of the *ssh_config*
file (**man 5 ssh_config**) for details.

**[System]{#part0031_split_001.html#id_511 .calibre10} Access and File
Transfer**

A user must log in to the Linux system in order to use it or transfer
files. The login process identifies the user to the system. For
accessing a RHEL system remotely, use the *ssh* command, and the *scp*
or the *sftp* command for copying files. These commands require either a
resolvable hostname of the target system or its IP address in order to
try to establish a connection. All these commands are secure and may be
used over secure and unsecure network channels to exchange data.

The following subsections and exercises describe multiple access
scenarios including accessing a RHEL system (*server20*) from another
RHEL system (*server10*) and a Windows computer, accessing a RHEL system
(*server20*) using ssh keys, and transferring files using *scp* and
*sftp*.

**[Exercise]{#part0031_split_001.html#id_512 .calibre10} 19-1: Access
RHEL System from Another RHEL System**

This exercise should be done on *server10* and *server20* as *user1*.

This exercise works under two assumptions: (1) *user1* exists on both
*server10* and *server20*, and (2) hostname and IP mapping is in place
in the **etc*hosts* file ([Chapter
16](#part0028_split_000.html#page_375){.calibre5}). Use the IP address
in lieu of the hostname if the mapping is unavailable for *server20*.

In this exercise, you will issue the *ssh* command as *user1* on
*server10* to log in to *server20*. You will run appropriate commands on
*server20* for validation. You will log off and return to the
originating system.

1[.]{.c19}Issue the *ssh* command as *user1* on *server10*:

![](images/00922.jpeg){.image2}

Answer 'yes' to the question presented and press Enter to continue. This
step adds the hostname of *server20* to a file called *known_hosts*
under **home*user1/.ssh* directory on the originating computer
(*server10*). This message will not reappear on subsequent login
attempts to *server20* for this user. Enter the correct password for
*user1* to be allowed in. You will be placed in the home directory of
*user1* on *server20*. The command prompt will reflect that information.

2[.]{.c19}Issue the basic Linux commands *whoami*, *hostname*, and *pwd*
to confirm that you are logged in as *user1* on *server20* and placed in
the correct home directory:

![](images/00923.jpeg){.image2}

3[.]{.c19}Run the *logout* or the *exit* command or simply press the key
combination Ctrl+d to log off *server20* and return to *server10*:

![](images/00924.jpeg){.image2}

This concludes the exercise.

If you wish to log on as a different user such as *user2* (assuming
*user2* exists on the target server *server20*), you may run the *ssh*
command in either of the following ways:

![](images/00925.jpeg){.image2}

The above will allow you to log in if the password entered for *user2*
is valid.

**[Exercise]{#part0031_split_001.html#id_513 .calibre10} 19-2: Generate,
Distribute, and Use SSH Keys**

This exercise should be done on *server10* and *server20* as *user1* and
*sudo* where required.

In this exercise, you will generate a password-less ssh key pair using
RSA algorithm for *user1* on *server10*. You will display the private
and public file contents. You will distribute the public key to
*server20* and attempt to log on to *server20* from *server10*. You will
show the log file message for the login attempt.

1[.]{.c19}Log on to *server10* as *user1*.

2[.]{.c19}Generate RSA keys without a password (-N) and without detailed
output (-q). Press Enter when prompted to provide the filename to store
the private key.

![](images/00926.jpeg){.image2}

The content of the *id_rsa* (private key) file is shown below:

![](images/00927.jpeg){.image2}

The content of the *id_rsa.pub* (public key) file is displayed below:

![](images/00928.jpeg){.image2}

3[.]{.c19}Copy the public key file to *server20* under
**home*user1/.ssh* directory. Accept the fingerprints for *server20*
when prompted (only presented on the first login attempt). Enter the
password for *user1* set on *server20* to continue with the file copy.
The public key will be copied as *authorized_keys*.

![](images/00929.jpeg){.image2}

At the same time, this command also creates or updates the *known_hosts*
file on *server10* and stores the fingerprints for *server20* in it.
Here is what is currently stored in it:

![](images/00930.jpeg){.image2}

4[.]{.c19}On *server10*, run the *ssh* command as *user1* to connect to
*server20*. You will not be prompted for a password because there was
none assigned to the ssh keys.

![](images/00931.jpeg){.image2}

You can view this login attempt in the **var*log/secure* file on
*server20*:

![](images/00932.jpeg){.image2}

The log entry shows the timestamp, hostname, process name and PID,
username and source IP, and other relevant information. This file will
log all future login attempts for this user.

**[Executing]{#part0031_split_001.html#id_514 .calibre10} Commands
Remotely Using ssh**

The *ssh* command is a secure replacement for the legacy unsecure tools
*telnet*, *rlogin*, and *rsh*. It allows you to securely sign in to a
remote system or execute a command without actually logging on to it.
[Exercise 19-2](#part0031_split_001.html#id_513){.calibre5} demonstrated
how a user can log in using this command. The following shows a few
basic examples on how to use *ssh* to execute a command on a remote
system.

Invoke the *ssh* command on *server10* to execute the *hostname* command
on *server20*:

![](images/00933.jpeg){.image2}

Run the *nmcli* command on *server20* to show (s) active network
connections (c):

![](images/00934.jpeg){.image2}

You can run any command on *server20* this way without having to log in
to it.

**[Copying]{#part0031_split_001.html#id_515 .calibre10} Files Remotely
Using scp**

Similar to *ssh*, a user can execute the *scp* command to transfer files
from *server10* to *server20*, and vice versa. This program can be run
by a normal user as long as the user has the required read and write
permissions on the source and destination, or by the *root* user. Here
are a few examples to understand the program's syntax and usage.

To transfer the **etc*chrony.conf* file from *server20* to */tmp* on
*server10* and confirm:

![](images/00935.jpeg){.image2}

The program took not even a second to transfer the file. The file size
is 1103 bytes. The *ls* command confirms the pull.

Now let's transfer the entire **etc*sysconfig* directory (-r) from
*server10* into */tmp* on *server20* and confirm. Ignore any permission
errors reported in the output.

![](images/00936.jpeg){.image2}

Run the *ls* command on *server20* for verification:

![](images/00937.jpeg){.image2}

The output verifies the directory copy.

In the above examples, the user account that was used on the source and
target servers is the same user, *user1*. To transfer a file or
directory using a different user account on the target server, you need
to include that user's name with the command. You must know the password
for the user on the target server. Here is the syntax:

![](images/00938.jpeg){.image2}

Check the manual pages of the *scp* command for more details and usage
examples.

**[Transferring]{#part0031_split_001.html#id_516 .calibre10} Files
Remotely Using sftp**

The *sftp* command is an interactive file transfer tool that can be used
instead of *scp*. This tool can be launched as follows on *server10* to
connect to *server20*:

![](images/00939.jpeg){.image2}

Type ? at the prompt to list available commands along with a short
description:

![](images/00940.jpeg){.image2}

As shown in the above screenshot, there are many common commands
available at the sftp\> prompt. These include *cd* to change directory,
*get*/*put* to download/upload a file, *ls* to list files, *pwd* to
print working directory, *mkdir* to create a directory, *rename* to
rename a file, *rm* to remove a file, and *bye*/*quit/exit* to exit the
program and return to the command prompt. These commands will run on the
remote server (*server20*). The following screenshot shows how these
commands are used:

![](images/00941.jpeg){.image2}

Furthermore, there are four commands beginning with an 'l'---*lcd*,
*lls*, *lpwd*, and *lmkdir*---at the sftp\> prompt. These commands are
intended to be run on the source server (*server10*). Other Linux
commands are also available at the sftp\> prompt that you may use for
basic file management operations on the remote server.

Type *quit* at the sftp\> prompt to exit the program when you're done.

You may use either *sftp* or *scp* for transferring files depending on
your comfort level. Consult the manual pages of the commands for options
and additional details.

**[Synchronizing]{#part0031_split_001.html#id_517 .calibre10} Files
Remotely Using rsync**

The *rsync* (*remote synchronization*) program works in a manner similar
to the *cp*, *rcp*, and *scp* commands to copy files between the source
and destination. With *rsync*, the source and destination could be on
the same system or different systems. The first initiation of *rsync*
copies all files from the source to the destination with subsequent
executions copy only the updated files. The *rsync* command uses the ssh
protocol by default.

The following examples explain the usage of the program and introduce
some common flags.

To copy a single file such as *grub.conf* to */tmp* on the same system:

![](images/00942.jpeg){.image2}

The -a option in the above example instructs the command to perform an
archive operation and preserve all file attributes such as permissions,
ownership, symlinks, and timestamps. The -v switch is used for
verbosity.

The actual size of the *grub.cfg* file is 5,032 bytes. The additional
bytes sent (5,126 minus 5,032 = 94 bytes) contain the metadata and other
overhead, and the received bytes signify the metadata received. The
output displays the files being copied and the file transfer rate as
well.

Subsequent invocations of the above would produce an output similar to
the following if the file has not been modified:

![](images/00943.jpeg){.image2}

It shows no filenames under the file list, as there was no transfer
occurred.

To copy **etc*rsyslog.conf* to */tmp* to *server20* using in-transit
compression (-z) and displaying the transfer progress (-P):

![](images/00944.jpeg){.image2}

To copy the entire **home*user1* directory recursively (-r) from
*server20* to **tmp*trans* directory on *server10* (create **tmp*trans*
before running the *rsync* command):

![](images/00945.jpeg){.image2}

The *rsync* command is fast and versatile, and has numerous other
options available. Refer to the command's manual pages for a description
of options and usage examples.

**[Chapter]{#part0031_split_001.html#id_518 .calibre10} Summary**

This chapter discussed the open source version of the secure shell
service. It started with an overview of the service, and described what
it is, how it works, available versions, and algorithms employed. We
skimmed through various encryption techniques and authentication
methods. We touched upon the service daemon, client and server
configuration files, and commands. We demonstrated accessing a lab
server from another lab server. We generated and distributed
password-less private/public key pair and employed ssh utilities to
remote execute commands and transfer files.

Lastly, we examined a program that may be put into action to keep files
synchronized between two systems over an ssh channel.

**[Review]{#part0031_split_001.html#id_519 .calibre10} Questions**

1[.]{.c19}What is the secure equivalent for the *rcp* command?

2[.]{.c19}What would the command *ssh-keygen -N ""* do?

3[.]{.c19}The primary secure shell server configuration file is
*ssh_config*. True or False?

4[.]{.c19}Which three common algorithms are used with SSH version 2 for
encryption and/or authentication?

5[.]{.c19}What is the secure shell equivalent for the *telnet* command?

6[.]{.c19}True or False? By default, the *root* user can directly log on
to a RHEL system.

7[.]{.c19}What is the default location to store user SSH keys?

8[.]{.c19}What would the command *ssh-copy-id* do?

9[.]{.c19}What is the *rsync* command used for?

10[.]{.c23}Which two of the five authentication methods mentioned in
this chapter are more prevalent?

11[.]{.c23}What is the use of the *ssh-keygen* command?

12[.]{.c23}Name the default algorithm used with SSH.

13[.]{.c23}What kind of information does the *\~/.ssh/known_hosts* file
store?

14[.]{.c23}List the two encryption techniques described in this chapter.

15[.]{.c23}What is the default port used by the secure shell service?

16[.]{.c23}Which log file stores authentication messages?

17[.]{.c23}The *ssh* tool provides a non-secure tunnel over a network
for accessing a RHEL system. True or False?

18[.]{.c23}Name the SSH client-side configuration file.

19[.]{.c23}What would the command *ssh server10 ls* do?

**[Answers]{#part0031_split_001.html#id_520 .calibre10} to Review
Questions**

1[.]{.c19}The *scp* command is the secure equivalent for the *rcp*
command.

2[.]{.c19}The command provided will generate a password-less ssh key
pair using the default RSA algorithm.

3[.]{.c19}False. The primary secure shell configuration file is
*sshd_config*.

4[.]{.c19}The SSH version 2 uses RSA, DSA, and ECDSA algorithms.

5[.]{.c19}The secure equivalent for *telnet* is the *ssh* command.

6[.]{.c19}True.

7[.]{.c19}Under the *\~/.ssh* directory.

8[.]{.c19}The *ssh-copy-id* command is used to distribute the public key
to remote systems.

9[.]{.c19}The *rsync* command is used to maintain a copy of source files
at remote location.

10[.]{.c23}The public key-based and password-based authentication
methods are more prevalent.

11[.]{.c23}The *ssh-keygen* command is used to generate public/private
key combination for use with ssh.

12[.]{.c23}The default algorithm used with ssh is RSA.

13[.]{.c23}The *\~/.ssh/known_hosts* file stores fingerprints of remote
servers.

14[.]{.c23}The two encryption techniques are symmetric (secret key) and
asymmetric (public key).

15[.]{.c23}The default port used by the secure shell service is 22.

16[.]{.c23}The **var*log/secure* file stores authentication messages.

17[.]{.c23}False. The *ssh* command provides a secure tunnel over a
network.

18[.]{.c23}The client-side SSH configuration file is *ssh_config* and it
is located in the **etc*ssh* directory.

19[.]{.c23}The command provided will run the *ls* command on the
specified remote ssh server without the need for the user to log in.

**[Do-]{#part0031_split_001.html#id_521 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

**[Lab]{#part0031_split_001.html#id_522 .calibre10} 19-1: Establish
Key-Based Authentication**

As *user1* with *sudo* on *server10* and *server20*, create user account
*user20* and assign a password. As *user20* on *server10*, generate a
private/public key pair without a passphrase using the *ssh-keygen*
command. Distribute the public key to *server20* with the *ssh-copy-id*
command. Log on to *server20* as *user20* and accept the fingerprints
for the server if presented. On subsequent log in attempts from
*server10* to *server20*, *user20* should not be prompted for their
password. (Hint: System Access and File Transfer).

**[Lab]{#part0031_split_001.html#id_523 .calibre10} 19-2: Test the
Effect of PermitRootLogin Directive**

As *user1* with *sudo* on *server20*, edit the **etc*ssh/sshd_config*
file and change the value of the directive PermitRootLogin to "no". Use
the *systemctl* command to activate the change. As *root* on *server10*,
run **ssh server20** (or its IP address). You'll get permission denied
message. Reverse the change on *server20* and retry **ssh server20**.
You should be able to log in. (Hint: The OpenSSH Service).

[]{#part0032_split_000.html}

## Chapter 20 {#part0032_split_000.html#calibre_pb_0 .calibre11}

**The Linux Firewall**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Describe Linux firewall for
host-based security control

[![](images/00001.jpeg){.c37}]{.c36}Overview of the firewalld service

[![](images/00001.jpeg){.c37}]{.c36}Understand the concepts of firewalld
zones and services

[![](images/00001.jpeg){.c37}]{.c36}Analyze zone and service
configuration files

[![](images/00001.jpeg){.c37}]{.c36}Control access to network services
and ports through firewalld

[![](images/00001.jpeg){.c37}]{.c36}Use firewall-cmd command to manage
firewall rules

[RHCSA Objectives:]{.c39}

[50]{#part0032_split_000.html#id_815 .calibre10}. Restrict network
access using firewall-cmd/firewall

55\. Configure firewall settings using firewall-cmd/firewalld

::: {#part0032_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0032_split_001.html}

[ R]{.c42}[unning]{.c43} a system in a networked or an Internet-facing
environment requires that some security measures be taken to tighten
access to the system in general and individual services in particular.
This can be accomplished by implementing a firewall and restricting
inbound traffic to allowed ports from valid source IP addresses only. We
discuss a host-based firewall solution in this chapter.

**[Firewall]{#part0032_split_001.html#id_524 .calibre10} Overview**

A *firewall* is a protective layer implemented at the network or server
level to secure, monitor, and control inbound and outbound traffic flow.
Firewalls employed at the network level use either dedicated hardware or
sophisticated software appliances to form a shield around the network.
Server level firewalls are referred to as *host-based firewalls* and
they run in a computer operating system to monitor and manage traffic in
and out. Firewalls defend a network or an individual server from
undesired traffic.

RHEL is shipped with a host-based firewall solution that works by
filtering data packets. A data packet is formed as a result of a process
called *encapsulation* whereby the header information is attached to a
message (called *payload*) during packet formation. The header includes
information such as source and destination IP addresses, port, and type
of data. Based on predefined *rules*, a firewall intercepts each inbound
and outbound data packet, inspects its header, and decides whether to
allow the packet to pass through.

Ports are defined in the **etc*services* file for common network
services that are standardized across all network operating systems,
including RHEL. Some common services and the ports they listen on are
FTP (*File Transfer Protocol*) on port 21, SSH (*Secure Shell*) 22,
Postfix (an email service) 25, HTTP (*HyperText Transfer Protocol*) 80,
and NTP (*Network Time Protocol*) on port 123.

The host-based firewall solution employed in RHEL uses a kernel module
called *netfilter* together with a filtering and packet classification
framework called *nftables* for policing the traffic movement. It also
supports other advanced features such as *Network Address Translation*
(NAT) and *port forwarding*. This firewall solution inspects, modifies,
drops, or routes incoming, outgoing, and forwarded network packets based
on defined rulesets.

**[Overview]{#part0032_split_001.html#id_525 .calibre10} of firewalld**

*firewalld* is the default host-based firewall management service in
RHEL 8. One of the major advantages is its ability to add, modify, or
delete firewall rules immediately without disrupting current network
connections or restarting the service process. This is especially useful
in testing and troubleshooting scenarios. *firewalld* also allows to
save rules persistently so that they are activated automatically at
system reboots.

The *firewalld* service lets you perform management operations at the
command line using the *firewall-cmd* command, graphically using the web
console, or manually by editing rules files. *firewalld* stores the
default rules in files located in the **usr*lib/firewalld* directory,
and those that contain custom rules in the **etc*firewalld* directory.
The default rules files may be copied to the custom rules directory and
modified.

**[firewalld]{#part0032_split_001.html#id_526 .calibre10} Zones**

*firewalld* uses the concept of *zones* for easier and transparent
traffic management. Zones define policies based on the trust level of
network connections and source IP addresses. A network connection can be
part of only one zone at a time; however, a zone can have multiple
network connections assigned to it. Zone configuration may include
services, ports, and protocols that may be open or closed. It may also
include rules for advanced configuration items such as masquerading,
port forwarding, NATting, ICMP filters, and rich language. Rules for
each zone are defined and manipulated independent of other zones.

*firewalld* inspects each incoming packet to determine the source IP
address and applies the rules of the zone that has a match for the
address. In the event no zone configuration matches the address, it
associates the packet with the zone that has the network connection
defined, and applies the rules of that zone. If neither works,
*firewalld* associates the packet with the default zone, and enforces
the

rules of the default zone on the packet.

The *firewalld* software installs several predefined zone files that may
be selected or customized. These files include templates for traffic
that must be blocked or dropped, and for traffic that is public-facing,
internal, external, home, public, trusted, and work-related. Of these,
the *public* zone is the default zone, and it is activated by default
when the *firewalld* service is started. [Table
20-1](#part0032_split_001.html#id_757){.calibre5} lists and describes
the predefined zones sorted based on the trust level from trusted to
untrusted.

::: c49
  ---------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Zone**   **Description**
  trusted    Allow all incoming
  internal   Reject all incoming traffic except for what is allowed. Intended for use on internal networks.
  home       Reject all incoming traffic except for what is allowed. Intended for use in homes.
  work       Reject all incoming traffic except for what is allowed. Intended for use at workplaces.
  dmz        Reject all incoming traffic except for what is allowed. Intended for use in publicly-accessible demilitarized zones.
  external   Reject all incoming traffic except for what is allowed. Outgoing IPv4 traffic forwarded through this zone is masqueraded to look like it originated from the IPv4 address of an outgoing network interface. Intended for use on external networks with masquerading enabled.
  public     Reject all incoming traffic except for what is allowed. It is the default zone for any newly added network interfaces. Intended for us in public places.
  block      Reject all incoming traffic with icmp-host-prohibited message returned. Intended for use in secure places.
  drop       Drop all incoming traffic without responding with ICMP errors. Intended for use in highly secure places.
  ---------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0032_split_001.html#id_757} 20-1 firewalld Default
Zones**
:::

For all the predefined zones, outgoing traffic is allowed by default.

**[Zone]{#part0032_split_001.html#id_527 .calibre10} Configuration
Files**

*firewalld* stores zone rules in XML format at two locations: the
system-defined rules in the **usr*lib/firewalld/zones* directory, and
the user-defined rules in the **etc*firewalld/zones* directory. The
files at the former location can be used as templates for adding new
rules, or applied instantly to any available network connection. A
system zone configuration file is automatically copied to the
**etc*firewalld/zones* directory if it is modified with a management
tool. Alternatively, you can copy the required zone file to the
**etc*firewalld/zones* directory manually, and make the necessary
changes. The *firewalld* service reads the files saved in this location,
and applies the rules defined in them. A listing of the system zone
files is presented below:

![](images/00946.jpeg){.image2}

The default *public* zone file is displayed below:

![](images/00947.jpeg){.image2}

As depicted in the screenshot, the zone has a name and description, and
it contains a list of all the allowed services---*ssh*, *dhcpv6-client*,
and *cockpit*. See the manual pages for *firewalld.zone* for details on
zone configuration files.

**[firewalld]{#part0032_split_001.html#id_528 .calibre10} Services**

In addition to the concept of zones, *firewalld* also uses the idea of
*services* for easier activation and deactivation of specific rules.
*firewalld* services are preconfigured firewall rules delineated for
various services and stored in different files. The rules consist of
necessary settings, such as the port number, protocol, and possibly
helper modules, to support the loading of the service. *firewalld*
services can be added to a zone. By default, *firewalld* blocks all
traffic unless a service or port is explicitly opened.

**[Service]{#part0032_split_001.html#id_529 .calibre10} Configuration
Files**

*firewalld* stores service rules in XML format at two locations: the
system-defined rules in the **usr*lib/firewalld/services* directory, and
the user-defined rules in the **etc*firewalld/services* directory. The
files at the former location can be used as templates for adding new
service rules, or activated instantly. A system service configuration
file is automatically copied to the **etc*firewalld/services* directory
if it is modified with a management tool. Alternatively, you can copy
the required service file to the **etc*firewalld/services* directory
manually, and make the necessary changes. The *firewalld* service reads
the files saved in this location, and applies the rules defined in them.
A listing of the system service files is presented below:

![](images/00948.jpeg){.image2}

The following shows the content of the *ssh* service file:

![](images/00949.jpeg){.image2}

As depicted in the screenshot, the service has a name and description,
and it defines the port and protocol for the service. See the manual
pages for *firewalld.service* for details on service configuration
files.

**[Firewall]{#part0032_split_001.html#id_530 .calibre10} Management**

Managing the *firewalld* service involves a number of operations, such
as listing, querying, adding, changing, and removing zones, services,
ports, IP sources, and network connections. There are three methods
available in RHEL 8 to perform the management tasks. They include the
*firewall-cmd* command for those who prefer to work at the command line
and the web interface for graphical administration. The third management
option is to make use of the zone and service templates, and edit them
as desired. We use the command line method in this book.

**The firewall-cmd Command**

The *firewall-cmd* command is a powerful tool to manage the *firewalld*
service at the command prompt. This tool can be used to add or remove
rules from the runtime configuration, or save any modifications to
service configuration for persistence. It supports numerous options for
the management of zones, services, ports, connections, and so on; [Table
20-2](#part0032_split_001.html#id_758){.calibre5} lists and describes
the common options only.

::: c49
  ------------------------- -------------------------------------------------------------------------------------------------
  **Option**                **Description**
  **General**               
  \--state                  Displays the running status of firewalld
  \--reload                 Reloads firewall rules from zone files. All runtime changes are lost.
  \--permanent              Stores a change persistently. The change only becomes active after a service reload or restart.
  **Zones**                 
  \--get-default-zone       Shows the name of the default/active zone
  \--set-default-zone       Changes the default zone for both runtime and permanent configuration
  \--get-zones              Prints a list of available zones
  \--get-active-zones       Displays the active zone and the assigned interfaces
  \--list-all               Lists all settings for a zone
  \--list-all-zones         Lists the settings for all available zones
  \--zone                   Specifies the name of the zone to work on. Without this option, the default zone is used.
  **Services**              
  \--get-services           Prints predefined services
  \--list-services          Lists services for a zone
  \--add-service            Adds a service to a zone
  \--remove-service         Removes a service from a zone
  \--query-service          Queries for the presence of a service
  **Ports**                 
  \--list-ports             Lists network ports
  \--add-port               Adds a port or a range of ports to a zone
  \--remove-port            Removes a port from a zone
  \--query-port             Queries for the presence of a port
  **Network Connections**   
  \--list-interfaces        Lists network connections assigned to a zone
  \--add-interface          Binds a network connection to a zone
  \--change-interface       Changes the binding of a network connection to a different zone
  \--remove-interface       Unbinds a network connection from a zone
  **IP Sources**            
  \--list-sources           Lists IP sources assigned to a zone
  \--add-source             Adds an IP source to a zone
  \--change-source          Changes an IP source
  \--remove-source          Removes an IP source from a zone
  ------------------------- -------------------------------------------------------------------------------------------------

**[Table]{#part0032_split_001.html#id_758} 20-2 Common firewall-cmd
Options**
:::

With all the \--add and \--remove options, the \--permanent switch may
be specified to ensure the rule is stored in the zone configuration file
under the **etc*firewalld/zones* directory for persistence. Some of the
options from [Table 20-2](#part0032_split_001.html#id_758){.calibre5}
are used in the upcoming exercises; the rest are beyond the scope of
this book. Consult the manual pages of the command for details on the
usage of these and other options.

**[Querying]{#part0032_split_001.html#id_531 .calibre10} the Operational
Status of firewalld**

You can check the running status of the *firewalld* service using either
the *systemctl* or the *firewall-cmd* command. Both commands will
produce different outputs, but the intent here is to ensure the service
is in the running state.

![](images/00950.jpeg){.image2}

The output indicates that the *firewalld* service is in the running
state on *server10*. The other command outcome also reports that the
service is marked for autostart at system reboots. In case *firewalld*
is not enabled or is inactive, issue **sudo systemctl \--now enable
firewalld** to start it immediately, and mark it for autostart on
reboots.

You are ready to perform the exercises presented next.

**[Exercise]{#part0032_split_001.html#id_532 .calibre10} 20-1: Add
Services and Ports, and Manage Zones**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will determine the current active zone. You will
add and activate a permanent rule to allow HTTP traffic on port 80, and
then add a runtime rule for traffic intended for TCP port 443 (the HTTPS
service). You will add a permanent rule to the *internal* zone for TCP
port range 5901 to 5910. You will confirm the changes and display the
contents of the affected zone files. Lastly, you will switch the default
zone to the *internal* zone and activate it.

1[.]{.c19}Determine the name of the current default zone:

![](images/00951.jpeg){.image2}

2[.]{.c19}Add a permanent rule to allow HTTP traffic on its default
port:

![](images/00952.jpeg){.image2}

The command made a copy of the *public.xml* file from
**usr*lib/firewalld/zones* directory into the **etc*firewalld/zones*
directory, and added the rule for the HTTP service.

3[.]{.c19}Activate the new rule:

![](images/00953.jpeg){.image2}

4[.]{.c19}Confirm the activation of the new rule:

![](images/00954.jpeg){.image2}

5[.]{.c19}Display the content of the default zone file to confirm the
addition of the permanent rule:

![](images/00955.jpeg){.image2}

6[.]{.c19}Add a runtime rule to allow traffic on TCP port 443 and
verify:

![](images/00956.jpeg){.image2}

7[.]{.c19}Add a permanent rule to the *internal* zone for TCP port range
5901 to 5910:

![](images/00957.jpeg){.image2}

8[.]{.c19}Display the content of the *internal* zone file to confirm the
addition of the permanent rule:

![](images/00958.jpeg){.image2}

The *firewall-cmd* command makes a backup of the affected zone file with
a *.old* extension whenever an update is made to a zone.

9[.]{.c19}Switch the default zone to internal and confirm:

![](images/00959.jpeg){.image2}

10[.]{.c23}Activate the rules defined in the *internal* zone and list
the port range added earlier:

![](images/00960.jpeg){.image2}

This completes the exercise.

**[Exercise]{#part0032_split_001.html#id_533 .calibre10} 20-2: Remove
Services and Ports, and Manage Zones**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will remove the two permanent rules that were
added in [Exercise 20-1](#part0032_split_001.html#id_532){.calibre5}.
You will switch the *public* zone back as the default zone, and confirm
the changes.

1[.]{.c19}Remove the permanent rule for HTTP from the *public* zone:

![](images/00961.jpeg){.image2}

Notice the equal sign (=) used with the \--remove-service option. The
*firewall-cmd* command supports the specification of add, remove,
change, and zone options with and without an equal sign (=). The \--zone
option is used to specify the *public* zone as it is currently the
non-default.

2[.]{.c19}Remove the permanent rule for ports 5901 to 5910 from the
*internal* zone:

![](images/00962.jpeg){.image2}

The \--zone option is not used, as 'internal' is currently the default
zone.

3[.]{.c19}Switch the default zone to *public* and validate:

![](images/00963.jpeg){.image2}

4[.]{.c19}Activate the *public* zone rules, and list the current
services:

![](images/00964.jpeg){.image2}

The *public* zone reflects the removal of the *http* service. This
concludes the exercise.

**[Exercise]{#part0032_split_001.html#id_534 .calibre10} 20-3: Test the
Effect of Firewall Rule**

This exercise should be done on *server10* and *server20* as *user1*
with *sudo* where required.

In this exercise, you will remove the *sshd* service rule from the
runtime configuration on *server10*, and try to access the server from
*server20* using the *ssh* command.

1[.]{.c19}Remove the rule for the *sshd* service on *server10*:

![](images/00965.jpeg){.image2}

2[.]{.c19}Issue the *ssh* command on *server20* to access *server10*:

![](images/00966.jpeg){.image2}

The error displayed is because the firewall on *server10* blocked the
access. Put the rule back on *server10* and try to access it from
*server20* again:

3[.]{.c19}Add the rule back for *sshd* on *server10*:

![](images/00967.jpeg){.image2}

4[.]{.c19}Issue the *ssh* command on *server20* to access *server10*.
Enter "yes" if prompted and the password for *user1*.

![](images/00968.jpeg){.image2}

This brings the exercise to an end.

**[Chapter]{#part0032_split_001.html#id_535 .calibre10} Summary**

We discussed a native host-based firewall solution for system protection
in this chapter. We explored the concepts around firewall and described
how it works. We looked at the firewalld service and examined the
concepts of zones and services. We reviewed predefined zones and
services, and analyzed their configuration files. We studied the lone
firewall management command and reviewed options for listing and
administering zones, services, ports, network connections, and source IP
addresses.

We learned how to change and check the operational state of the
firewalld service. We performed exercises to add and remove services and
ports persistently and non-persistently, and manage zones. Finally, we
tested the effect of deleting a port from the firewall configuration and
adding it back.

**[Review]{#part0032_split_001.html#id_536 .calibre10} Questions**

1[.]{.c19}A firewall can be configured between two networks but not
between two hosts. True or False?

2[.]{.c19}Which directory stores the configuration file for modified
firewalld zones?

3[.]{.c19}After changing the default firewalld zone to internal, what
would you run to verify?

4[.]{.c19}What is the process of data packet formation called?

5[.]{.c19}What would the command *firewall-cmd \--permanent
\--add-service=nfs \--zone=external* do?

6[.]{.c19}If you have a set of firewall rules defined for a service
stored under both **etc*firewalld* and **usr*lib/firewalld* directories,
which of the two sets will take precedence?

7[.]{.c19}What is the kernel module in RHEL 8 that implements the
host-level protection called?

8[.]{.c19}firewalld is the firewall management solution in RHEL 8. True
or False?

9[.]{.c19}Name the default firewalld zone.

10[.]{.c23}What is the purpose of firewalld service configuration files?

11[.]{.c23}What would the command *firewall-cmd \--remove-port=5000/tcp*
do?

12[.]{.c23}What is the primary command line tool for managing firewalld
called?

**[Answers]{#part0032_split_001.html#id_537 .calibre10} to Review
Questions**

1[.]{.c19}False. A firewall can also be configured between two host
computers.

2[.]{.c19}The modified *firewalld* zone files are stored under
**etc*firewalld/zones* directory.

3[.]{.c19}You run *firewall-cmd \--get-default-zone* for validation.

4[.]{.c19}The process of data packet formation is called encapsulation.

5[.]{.c19}The command provided will add the *nfs* service to *external*
firewalld zone persistently.

6[.]{.c19}The ruleset located in the **etc*firewalld* directory will
have precedence.

7[.]{.c19}The kernel module that implements the host-level protection is
called netfilter.

8[.]{.c19}True.

9[.]{.c19}The default firewalld zone is the *public* zone.

10[.]{.c23}firewalld service configuration files store service-specific
port, protocol, and other details, which makes it easy to activate and
deactivate them.

11[.]{.c23}The command provided will remove the runtime firewall rule
for TCP port 5000.

12[.]{.c23}The primary command line tool for managing *firewalld* is
called *firewall-cmd*.

**[Do-]{#part0032_split_001.html#id_538 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

**[Lab]{#part0032_split_001.html#id_539 .calibre10} 20-1: Add Service to
Firewall**

As *user1* with *sudo* on *server10*, add and activate a permanent rule
for HTTPs traffic to the default zone. Confirm the change by viewing the
zone's XML file and running the *firewall-cmd* command. (Hint: Firewall
Management).

**[Lab]{#part0032_split_001.html#id_540 .calibre10} 20-2: Add Port Range
to Firewall**

As *user1* with *sudo* on *server10*, add and activate a permanent rule
for the UDP port range 8000 to 8005 to the *trusted* zone. Confirm the
change by viewing the zone's XML file and running the *firewall-cmd*
command. (Hint: Firewall Management).

[]{#part0033_split_000.html}

## Chapter 21 {#part0033_split_000.html#calibre_pb_0 .calibre11}

**Security Enhanced Linux**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Describe Security Enhanced Linux and
its terminology

[![](images/00001.jpeg){.c37}]{.c36}Understand SELinux contexts for
users, processes, files, and ports

[![](images/00001.jpeg){.c37}]{.c36}Copy, move, and archive files with
and without SELinux context

[![](images/00001.jpeg){.c37}]{.c36}How domain transitioning works

[![](images/00001.jpeg){.c37}]{.c36}Overview of SELinux Booleans

[![](images/00001.jpeg){.c37}]{.c36}Query and manage SELinux via
management tools

[![](images/00001.jpeg){.c37}]{.c36}Modify SELinux contexts for files
and ports

[![](images/00001.jpeg){.c37}]{.c36}Add SELinux rules to policy database

[![](images/00001.jpeg){.c37}]{.c36}View and analyze SELinux alerts

[RHCSA Objectives:]{.c39}

[58]{#part0033_split_000.html#id_822 .calibre10}. Set enforcing and
permissive modes for SELinux

[59]{#part0033_split_000.html#id_823 .calibre10}. List and identify
SELinux file and process context

[60]{#part0033_split_000.html#id_824 .calibre10}. Restore default file
contexts

[61]{#part0033_split_000.html#id_825 .calibre10}. Use Boolean settings
to modify system SELinux settings

[62]{#part0033_split_000.html#id_826 .calibre10}. Diagnose and address
routine SELinux policy violations

::: {#part0033_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0033_split_001.html}

[ S]{.c42}[ecurity]{.c43} Enhanced Linux is a mechanism that controls
who can access and do what on the system. It is part and parcel of the
Linux kernel. It handles access beyond what the traditional access
control system delivers including file and directory permissions, user
and group-level permissions, shadow password and password aging
mechanisms, and ACLs. The goal of SELinux is to limit the possible
damage that could occur to the system due to unauthorized user or
program access. This chapter covers SELinux in reasonable detail.

**[Security]{#part0033_split_001.html#id_541 .calibre10} Enhanced
Linux**

*Security Enhanced Linux* (SELinux) is an implementation of the
*Mandatory Access Control* (MAC) architecture developed by the U.S.
*National Security Agency* (NSA) in collaboration with other
organizations and the Linux community for flexible, enriched, and
granular security controls in Linux. MAC is integrated into the Linux
kernel as a set of patches using the *Linux Security Modules* (LSM)
framework that allows the kernel to support various security
implementations, including SELinux.

MAC provides an added layer of protection above and beyond the standard
Linux *Discretionary Access Control* (DAC) security architecture. DAC
includes the traditional file and directory permissions, ACLs, extended
attribute settings, setuid/setgid bits, su/sudo privileges, and other
controls. MAC limits the ability of a *subject* (Linux user or process)
to access an *object* (file, directory, file system, device, network
interface/connection, port, pipe, socket, etc.) to reduce or eliminate
the potential damage the subject may be able to inflict on the system if
compromised due to the exploitation of vulnerabilities in services,
programs, or applications.

MAC controls are fine-grained; they protect other services in the event
one service is negotiated. For instance, if the HTTP service process is
compromised, the attacker can only damage the files the hacked process
will have access to, and not the other processes running on the system,
or the objects the other processes will have access to. To ensure this
coarse-grained control, MAC uses a set of defined authorization rules
called *policy* to examine security attributes associated with subjects
and objects when a subject tries to access an object, and decides
whether to permit the access attempt. These attributes are stored in
*contexts* (a.k.a. *labels*), and are applied to both subjects and
objects.

SELinux decisions are stored in a special cache area called *Access
Vector Cache* (AVC). This cache area is checked for each access attempt
by a process to determine whether the access attempt was previously
allowed. With this mechanism in place, SELinux does not have to check
the policy ruleset repeatedly, thus improving performance.

By default, SELinux controls are enabled at the time of RHEL
installation with the default configuration, which confines the
processes to the bare minimum privileges that they need to function.

**[Terminology]{#part0033_split_001.html#id_542 .calibre10}**

To comprehend SELinux, an understanding of some key terms is essential.
These terms are useful in explaining the concepts and SELinux
functionality in the remainder of this chapter.

**Subject**

A *subject* is any user or process that accesses an object. Examples
include system_u for the SELinux system user, and unconfined_u for
subjects that are not bound by the SELinux policy. The subject is stored
in field 1 of the context.

**Object**

An *object* is a resource, such as a file, directory, hardware device,
network interface/connection, port, pipe, or socket, that a subject
accesses. Examples include object_r for general objects, system_r for
system-owned objects, and unconfined_r for objects that are not bound by
the SELinux policy.

**Access**

An *access* is an action performed by the subject on an object. Examples
include creating, reading, or updating a file, creating or navigating a
directory, and accessing a network port or socket.

**Policy**

A *policy* is a defined ruleset that is enforced system-wide, and is
used to analyze security attributes assigned to subjects and objects.
This ruleset is referenced to decide whether to permit a subject's
access attempt to an object, or a subject's attempt to interact with
another subject. The default behavior of SELinux in the absence of a
rule is to deny the access. Two standard preconfigured policies are
*targeted* and *mls* with targeted being the default.

The targeted policy dictates that any process that is targeted runs in a
confined domain, and any process that is not targeted runs in an
unconfined domain. For instance, SELinux runs logged-in users in the
unconfined domain, and the *httpd* process in a confined domain by
default. Any subject running unconfined is more vulnerable than the one
running confined.

The mls policy places tight security controls at deeper levels.

A third preconfigured policy called *minimum* is a light version of the
targeted policy, and it is designed to protect only selected processes.

**Context**

A *context* (a.k.a. *label*) is a tag to store security attributes for
subjects and objects. In SELinux, every subject and object has a context
assigned, which consists of a SELinux user, role, type (or domain), and
sensitivity level. SELinux uses this information to make access control
decisions.

**Labeling**

*Labeling* is the mapping of files with their stored contexts.

**SELinux User**

SELinux policy has several predefined SELinux user identities that are
authorized for a particular set of roles. SELinux policy maintains Linux
user to SELinux user identity mapping to place SELinux user restrictions
on Linux users. This controls what roles and levels a process (with a
particular SELinux user identity) can enter. A Linux user, for instance,
cannot run the *su* and *sudo* commands or the programs located in their
home directories if they are mapped to the SELinux user *user_u*.

**Role**

A *role* is an attribute of the *Role-Based Access Control* (RBAC)
security model that is part of SELinux. It classifies who (subject) is
allowed to access what (domains or types). SELinux users are authorized
for roles, and roles are authorized for domains and types. Each subject
has an associated role to ensure that the system and user processes are
separated. A subject can transition into a new role to gain access to
other domains and types. Examples roles include user_r for ordinary
users, sysadm_r for administrators, and system_r for processes that
initiate under the system_r role. The role is stored in field 2 of the
context.

**Type Enforcement**

*Type enforcement* (TE) identifies and limits a subject's ability to
access domains for processes, and types for files. It references the
contexts of the subjects and objects for this enforcement.

**Type and Domain**

A *type* is an attribute of type enforcement. It is a group of objects
based on uniformity in their security requirements. Objects such as
files and directories with common security requirements, are grouped
within a specific type. Examples of types include user_home_dir_t for
objects located in user home directories, and usr_t for most objects
stored in the */usr* directory. The type is stored in field 3 of a file
context.

A *domain* determines the type of access that a process has. Processes
with common security requirements are grouped within a specific domain
type, and they run confined within that domain. Examples of domains
include init_t for the *systemd* process, firewalld_t for the
*firewalld* process, and unconfined_t for all processes that are not
bound by SELinux policy. The domain is stored in field 3 of a process
context.

SELinux policy rules outline how types can access each other, domains
can access types, and domains can access each other.

**Level**

A *level* is an attribute of *Multi-Level Security* (MLS) and
*Multi-Category Security* (MCS). It is a pair of sensitivity:category
values that defines the level of security in the context. A category may
be defined as a single value or a range of values, such as c0.c4 to
represent c0 through c4. In RHEL 8, the targeted policy is used as the
default, which enforces MCS (MCS supports only one sensitivity level
(s0) with 0 to 1023 different categories).

**[SELinux]{#part0033_split_001.html#id_543 .calibre10} Contexts for
Users**

SELinux contexts define security attributes placed on subjects and
objects. Each context contains a type and a security level with subject
and object information. Use the *id* command with the -Z option to view
the context set on Linux users. The following example shows the context
for *user1*:

![](images/00969.jpeg){.image2}

The output indicates that *user1* is mapped to the SELinux unconfined_u
user, and that there are no SELinux restrictions placed on this user.
You'll get the same result if you run this command for other users. This
entails that all Linux users, including *root*, run unconfined by
default, which gives them full access to the system.

In addition to the unconfined user with unlimited privileges, SELinux
includes seven confined user identities with restricted access to
objects. These accounts are mapped to Linux users via SELinux policy.
This regulated access helps safeguard the system from potential damage
that Linux users might inflict on the system.

You can use the *seinfo* query command to list the SELinux users;
however, the *setools-console* software package must be installed before
doing so.

![](images/00970.jpeg){.image2}

The output shows the eight predefined SELinux users. You can use the
*semanage* command to view the mapping between Linux and SELinux users:

![](images/00971.jpeg){.image2}

The output displays Linux users in column 1 (Login Name) and SELinux
users they are mapped to in column 2 (SELinux User). Columns 3 and 4
show the associated security level (MLS/MCS Range), and the context for
the Linux user (the \* represents all services). By default, all
non-*root* Linux users are represented as \_\_default\_\_, which is
mapped to the unconfined_u user in the policy.

**[SELinux]{#part0033_split_001.html#id_544 .calibre10} Contexts for
Processes**

You can determine the context for processes using the *ps* command with
the -Z flag. The following example shows only the first two lines from
the command output:

![](images/00972.jpeg){.image2}

In the output, the subject (system_u) is a SELinux username (mapped to
Linux user *root*), object is system_r, domain (init_t) reveals the type
of protection applied to the process, and level of security (s0). Any
process that is unprotected will run in the unconfined_t domain.

**SELinux Contexts for Files**

You can spot the context for files and directories. To this end, use the
*ls* command with the -Z switch. The following shows the four attributes
set on the **etc*passwd* file:

![](images/00973.jpeg){.image2}

The outcome indicates the subject (system_u), object (object_r), type
(passwd_file_t), and security level (s0) for the *passwd* file. Contexts
for system-installed and user-created files are stored in the
*file_contexts* and *file_contexts.local* files located in the
**etc*selinux/targeted/contexts/files* directory. These policy files can
be updated using the *semanage* command.

**[Copying,]{#part0033_split_001.html#id_545 .calibre10} Moving, and
Archiving Files with SELinux Contexts**

As mentioned, all files in RHEL are labeled with an SELINUX security
context by default. New files inherit the parent directory's context at
the time of creation. However, three common file management
operations---copy, move, and archive---require special attention. There
are certain rules to be kept in mind during their use to ensure correct
contexts on affected files. These rules are:

1[.]{.c19}If a file is copied to a different directory, the destination
file will receive the destination directory's context, unless the
\--preserve=context switch is specified with the *cp* command to retain
the source file's original context.

2[.]{.c19}If a copy operation overwrites the destination file in the
same or different directory, the file being copied will receive the
context of the overwritten file, unless the \--preserve=context switch
is specified with the *cp* command to preserve the source file's
original context.

3[.]{.c19}If a file is moved to the same or different directory, the
SELinux context will remain intact, which may differ from the
destination directory's context.

4[.]{.c19}If a file is archived with the *tar* command, use the
\--selinux option to preserve the context.

Later in the chapter, we will perform an exercise to confirm the
behavior of the three operations.

**[SELinux]{#part0033_split_001.html#id_546 .calibre10} Contexts for
Ports**

SELinux contexts define security attributes for network ports, which can
be viewed with the *semanage* command . The following illustrates a few
entries from the output of this command:

![](images/00974.jpeg){.image2}

The output is displayed in three columns. Column 1 shows the SELinux
type, column 2 depicts the protocol, and column 3 indicates the port
number(s). By default, SELinux allows services to listen on a restricted
set of network ports only. This is evident from the above output.

**Domain Transitioning**

SELinux allows a process running in one domain to enter another domain
to execute an application that is restricted to run in that domain only,
provided a rule exists in the policy to support such transition. SELinux
defines a permission setting called *entrypoint* in its policy to
control processes that can transition into another domain. To understand
how this works, a basic example is provided below that shows what
happens when a Linux user attempts to change their password using the
**usr*bin/passwd* command.

The *passwd* command is labeled with the passwd_exec_t type, which can
be confirmed as follows:

![](images/00975.jpeg){.image2}

The *passwd* command requires access to the **etc*shadow* file in order
to modify a user password. The *shadow* file has a different type set on
it (shadow_t):

![](images/00976.jpeg){.image2}

The SELinux policy has rules that specifically allow processes running
in domain passwd_t to read and modify the files with type shadow_t, and
allow them entrypoint permission into domain passwd_exec_t. This rule
enables the user's shell process executing the *passwd* command to
switch into the passwd_t domain and update the *shadow* file~.~

Open two terminal windows. In window 1, issue the *passwd* command as
*user1* and wait at the prompt:

![](images/00977.jpeg){.image2}

In window 2, run the *ps* command:

![](images/00978.jpeg){.image2}

As you can see, the *passwd* command (process) transitioned into the
passwd_t domain to change the user password. A process running in this
domain is allowed to modify the content of the **etc*shadow* file.

**[SELinux]{#part0033_split_001.html#id_547 .calibre10} Booleans**

*Booleans* are on/off switches that SELinux uses to determine whether to
permit an action. Booleans activate or deactivate certain rule in the
SELinux policy immediately and without the need to recompile or reload
the policy. For instance, the ftpd_anon_write Boolean can be turned on
to enable anonymous users to upload files. This privilege can be revoked
by turning this Boolean off. Boolean values are stored in virtual files
located in the **sys*fs/selinux/booleans* directory. The filenames match
the Boolean names. A sample listing of this directory is provided below:

![](images/00979.jpeg){.image2}

On a typical server, you'll see hundreds of Boolean files in the output.

The manual pages of the Booleans are available through the
*selinux-policy-doc* package. Once installed, use the -K option with the
*man* command to bring the pages up for a specific Boolean. For
instance, issue **man -K abrt_anon_write** to view the manual pages for
the abrt_anon_write Boolean.

Boolean values can be viewed, and flipped temporarily or for permanence.
The new value takes effect right away. Temporary changes are stored as a
"1" or "0" in the corresponding Boolean file in the
**sys*fs/selinux/booleans* directory and permanent changes are saved in
the policy database.

One of the exercises in the next section demonstrates how to display and
change Boolean values.

**[SELinux]{#part0033_split_001.html#id_548 .calibre10} Administration**

Managing SELinux involves plentiful tasks, including controlling the
activation mode, checking operational status, setting security contexts
on subjects and objects, and switching Boolean values. RHEL provides a
set of commands to perform these operations. These commands are
available through multiple packages, such as *libselinux-utils* provides
*getenforce*, *getenforce*, and *getsebool* commands, *policycoreutils*
contains *sestatus*, *setsebool*, and *restorecon* commands,
*policycoreutils-python-utils* provides the *semanage* command, and
*setools-console* includes the *seinfo* and *sesearch* commands.

For viewing alerts and debugging SELinux issues, a graphical tool called
*SELinux Alert Browser* is available, which is part of the
*setroubleshoot-server* package. In order to fully manage SELinux, you
need to ensure that all these packages are installed on the system.
Besides this toolset, there are additional utilities available to
accomplish specific SELinux administration tasks, but their use is not
as frequent.

**[Management]{#part0033_split_001.html#id_549 .calibre10} Commands**

SELinux delivers a variety of commands for effective administration.
[Table 21-1](#part0033_split_001.html#id_759){.calibre5} lists and
describes the commands mentioned above plus a few more under various
management categories.

::: c49
  ------------------------ -------------------------------------------------------------------------------------------------------------------
  **Command**              **Description**
  **Mode Management**      
  getenforce               Displays the current mode of operation
  sestatus                 Shows SELinux runtime status and Boolean values
  setenforce               Switches the operating mode between enforcing and permissive temporarily
  **Context Management**   
  chcon                    Changes context on files (changes do not survive file system relabeling)
  restorecon               Restores default contexts on files by referencing the files in the *etc*selinux/targeted/contexts/files directory
  semanage                 Changes context on files with the fcontext subcommand (changes survive file system relabeling)
  **Policy Management**    
  seinfo                   Provides information on policy components
  semanage                 Manages policy database
  sesearch                 Searches rules in the policy database
  **Boolean Management**   
  getsebool                Displays Booleans and their current settings
  setsebool                Modifies Boolean values temporarily, or in the policy database
  semanage                 Modifies Boolean values in the policy database with the boolean subcommand
  **Troubleshooting**      
  sealert                  The graphical troubleshooting tool
  ------------------------ -------------------------------------------------------------------------------------------------------------------

**[Table]{#part0033_split_001.html#id_759} 21-1 SELinux Management
Commands**
:::

Most of these commands are employed in this chapter.

**[Viewing]{#part0033_split_001.html#id_550 .calibre10} and Controlling
SELinux Operational State**

One of the key configuration files that controls the SELinux operational
state, and sets its default type is the *config* file located in the
**etc*selinux* directory. The default content of the file are displayed
below:

![](images/00980.jpeg){.image2}

The SELINUX directive in the file sets the activation mode for SELinux.
*Enforcing* activates it and allows or denies actions based on the
policy rules. *Permissive* activates SELinux, but permits all actions.
It records all security violations. This mode is useful for
troubleshooting and in developing or tuning the policy. The third option
is to completely turn SELinux off. When running in enforcing mode, the
SELINUXTYPE directive dictates the type of policy to be enforced. The
default SELinux type is targeted.

Issue the *getenforce* command to determine the current operating mode:

![](images/00981.jpeg){.image2}

The output returns enforcing as the current active policy. You may flip
the state to permissive using the *setenforce* command, and verify the
change with *getenforce*:

![](images/00982.jpeg){.image2}

![](images/00002.jpeg){.image} You may alternatively use a "0" for
permissive and a "1" for enforcing.

The change takes effect at once; however, it will be lost at the next
system reboot. To make it persistent, edit the **etc*selinux/config*
file and set the SELINUX directive to the desired mode.

[**EXAM TIP:**]{.c56} You may switch SELinux to permissive for
troubleshooting a non-functioning service. Change it back to enforcing
when the issue is resolved.

To disable SELinux completely, the SELINUX directive needs to be set to
disabled in the *config* file, and the system must be rebooted.
Reactivation in the future to either enforcing or permissive will
require the mode resetting in the *config* file followed by a system
reboot. The reboot will take longer than normal, as SELinux will go
through the process of relabeling all the files.

**[Querying]{#part0033_split_001.html#id_551 .calibre10} Status**

The current runtime status of SELinux can be viewed with the *sestatus*
command. This command also displays the location of principal
directories, the policy in effect, and the activation mode.

![](images/00983.jpeg){.image2}

The output reveals that SELinux is enabled (SELinux status), and it is
running in permissive mode (Current mode) with the targeted policy in
effect (Loaded policy name). It also indicates the current mode setting
in the *config* file (Mode from config file) along with other
information.

The *sestatus* command may be invoked with the -v switch to report on
security contexts set on files and processes, as listed in the
**etc*sestatus.conf* file. The default content of this file is shown
below:

![](images/00984.jpeg){.image2}

Run the *sestatus* command with -v:

![](images/00985.jpeg){.image2}

With -v included, the command reports the contexts for the current
process (Current context) and the *init* (*systemd*) process (Init
context) under Process Contexts. It also reveals the file contexts for
the controlling terminal and associated files under File Contexts.

**[Exercise]{#part0033_split_001.html#id_552 .calibre10} 21-1: Modify
SELinux File Context**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will create a directory *sedir1* under */tmp* and
a file *sefile1* under *sedir1*. You will check the context on the
directory and file. You will change the SELinux user and type to user_u
and public_content_t on both and verify.

1[.]{.c19}Create the hierarchy *sedir1/sefile1* under */tmp*:

![](images/00986.jpeg){.image2}

2[.]{.c19}Determine the context on the new directory and file:

![](images/00987.jpeg){.image2}

The directory and the file get unconfined_u and user_tmp_t as the
SELinux user and type.

3[.]{.c19}Modify the SELinux user (-u) on the directory to user_u and
type (-t) to public_content_t recursively (-R) with the *chcon* command:

![](images/00988.jpeg){.image2}

4[.]{.c19}Validate the new context:

![](images/00989.jpeg){.image2}

This concludes the exercise.

**[Exercise]{#part0033_split_001.html#id_553 .calibre10} 21-2: Add and
Apply File Context**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will add the current context on *sedir1* to the
SELinux policy database to ensure a relabeling will not reset it to its
previous value (see [Exercise
21-1](#part0033_split_001.html#id_552){.calibre5}). Next, you will
change the context on the directory to some random values. You will
restore the default context from the policy database back to the
directory recursively.

1[.]{.c19}Determine the current context:

![](images/00990.jpeg){.image2}

The output indicates the current SELinux user (user_u) and type
(public_content_t) set on the directory and the file.

2[.]{.c19}Add (-a) the directory recursively to the policy database
using the *semanage* command with the *fcontext* subcommand:

![](images/00991.jpeg){.image2}

The regular expression (/.\*)? instructs the command to include all
files and subdirectories under **tmp*sedir1*. This expression is needed
only if recursion is required.

The above command added the context to the
**etc*selinux/targeted/contexts/files/file_contexts.local* file. You can
use the *cat* command to view the content.

3[.]{.c19}Validate the addition by listing (-l) the recent changes (-C)
in the policy database:

![](images/00992.jpeg){.image2}

4[.]{.c19}Change the current context on *sedir1* to something random
(staff_u/etc_t) with the *chcon* command:

![](images/00993.jpeg){.image2}

5[.]{.c19}The security context is changed successfully. Confirm with the
*ls* command:

![](images/00994.jpeg){.image2}

6[.]{.c19}Reinstate the context on the *sedir1* directory recursively
(-R) as stored in the policy database using the *restorecon* command:

![](images/00995.jpeg){.image2}

The output confirms the restoration of the default context on the
directory and the file.

[**EXAM TIP:**]{.c56} Use the combination of semanage and restorecon
commands to add a file context to the SELinux policy and then apply it.
This will prevent the context on file to reset to the original value in
the event of SELinux relabeling (disabled to enforcing/permissive).

**Exercise 21-3: Add and Delete Network Ports**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will add a non-standard network port 8010 to the
SELinux policy database for the *httpd* service and confirm the
addition. You will then remove the port from the policy and verify the
deletion.

1[.]{.c19}List (-l) the ports for the *httpd* service as defined in the
SELinux policy database:

![](images/00996.jpeg){.image2}

The output reveals eight network ports the *httpd* process is currently
allowed to listen on.

2[.]{.c19}Add (-a) port 8010 with type (-t) http_port_t and protocol
(-p) tcp to the policy:

![](images/00997.jpeg){.image2}

3[.]{.c19}Confirm the addition:

![](images/00998.jpeg){.image2}

The new network port is visible in the outcome.

4[.]{.c19}Delete (-d) port 8010 from the policy and confirm:

![](images/00999.jpeg){.image2}

The port is removed from the policy database.

[**EXAM TIP:**]{.c56} Any non-standard port you want to use for any
service, make certain to add it to the SELinux policy database with the
correct type.

**[Exercise]{#part0033_split_001.html#id_554 .calibre10} 21-4: Copy
Files with and without Context**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will create a file called *sefile2* under */tmp*
and display its context. You will copy this file to the **etc*default*
directory, and observe the change in the context. You will remove
se*file2* from **etc*default*, and copy it again to the same
destination, ensuring that the target file receives the source file's
context.

1[.]{.c19}Create file *sefile2* under */tmp* and show context:

![](images/01000.jpeg){.image2}

The context on the file is unconfined_u, object_r, and user_tmp_t.

2[.]{.c19}Copy this file to the **etc*default* directory, and check the
context again:

![](images/01001.jpeg){.image2}

The target file (**etc*default/sefile2*) received the default context of
the destination directory (**etc*default*).

3[.]{.c19}Erase the **etc*default/sefile2* file, and copy it again with
the \--preserve=context option:

![](images/01002.jpeg){.image2}

4[.]{.c19}List the file to view the context:

![](images/01003.jpeg){.image2}

The original context (user_tmp_t) is preserved on the target file after
the copy operation has finished.

**[Exercise]{#part0033_split_001.html#id_555 .calibre10} 21-5: View and
Toggle SELinux Boolean Values**

This exercise should be done on *server10* as *user1* with *sudo* where
required.

In this exercise, you will display the current state of the Boolean
nfs_export_all_rw. You will toggle its value temporarily, and reboot the
system. You will flip its value persistently after the system has been
back up.

1[.]{.c19}Display the current setting of the Boolean nfs_export_all_rw
using three different commands---*getsebool*, *sestatus*, and
*semanage*:

![](images/01004.jpeg){.image2}

2[.]{.c19}Turn off the value of nfs_export_all_rw using the *setsebool*
command by simply furnishing "off" or "0" with it and confirm:

![](images/01005.jpeg){.image2}

3[.]{.c19}Reboot the system and rerun the *getsebool* command to check
the Boolean state:

![](images/01006.jpeg){.image2}

The value reverted to its previous state.

4[.]{.c19}Set the value of the Boolean persistently (-P or -m as needed)
using either of the following:

![](images/01007.jpeg){.image2}

5[.]{.c19}Validate the new value using the *getsebool*, *sestatus*, or
*semanage* command:

![](images/01008.jpeg){.image2}

The command outputs confirm the permanent change.

**[Monitoring]{#part0033_split_001.html#id_556 .calibre10} and Analyzing
SELinux Violations**

SELinux generates alerts for system activities when it runs in enforcing
or permissive mode. It writes the alerts to the
**var*log/audit/audit.log* file if the *auditd* daemon is running, or to
the **var*log/messages* file via the *rsyslog* daemon in the absence of
*auditd*. SELinux also logs the alerts that are generated due to denial
of an action, and identifies them with a type tag AVC (*Access Vector
Cache*) in the *audit.log* file. It also writes the rejection in the
*messages* file with a message ID, and how to view the message details.

![](images/00002.jpeg){.image} If it works with SELinux in permissive
mode and not in enforcing, something needs to be adjusted in SELinux.

SELinux denial messages are analyzed and the audit data is examined to
identify the potential cause of the rejection. The results of the
analysis are recorded with recommendations on how to fix it. These
results can be reviewed to aid in troubleshooting, and recommended
actions taken to address the issue. SELinux runs a service daemon called
*setroubleshootd* that performs this analysis and examination in the
background. This service also has a client interface called *SELinux
Troubleshooter* (the *sealert* command) that reads the data and displays
it for assessment. The client tool has both text and graphical
interfaces. The server and client components are part of the
*setroubleshoot-server* software package that must be installed on the
system prior to using this service.

The following shows a sample allowed record in raw format from the
**var*log/audit/audit.log* file:

![](images/01009.jpeg){.image2}

The record shows a successful *user1* attempt to *su* into the *root*
user account on *server10*.

The following is a sample denial record from the same file in raw
format:

![](images/01010.jpeg){.image2}

The message has the AVC type, and it is related to the *passwd* command
(comm) with source context (scontext)
"unconfined_u:unconfined_r:passwd_t:s0-s0:c0.c1023", and the *nshadow*
file (name) with file type (tclass) "file", and target context
(tcontext) "system_u:object_r:etc_t:s0". It also indicates the SELinux
operating mode, which is enforcing (permissive=0). This message
indicates that the **etc*shadow* file does not have the correct context
set on it, and that's why SELinux prevented the *passwd* command from
updating the user's password.

You can also use the *sealert* command to analyze (-a) all AVC records
in the *audit.log* file. This command produces a formatted report with
all relevant details:

![](images/01011.jpeg){.image2}

The above SELinux denial was due to the fact that I produced the
scenario by changing the SELinux type on the *shadow* file to something
random (etc_t). I then issued the *passwd* command as *user1* to modify
the password. As expected, SELinux disallowed the *passwd* command to
write the new password to the *shadow* file, and it logged the password
rejection attempt to the audit log. I then restored the type on the
*shadow* file with **restorecon *etc*shadow**. I re-tried the password
change and it worked.

**[Chapter]{#part0033_split_001.html#id_557 .calibre10} Summary**

In this chapter, we discussed security enhanced Linux in fair detail. We
looked at the concepts, features, and terminology at length. We examined
how security contexts are associated with users, processes, files, and
ports, and viewed and modified contexts for them. We analyzed the
configuration file that controls its state and defines the policy to be
enforced. We examined how domain transitioning works. We learned several
SELinux administrative commands and performed tasks such as checking and
switching activation mode and operational status. We studied the concept
of Booleans and learned how to modify certain parts of the SELinux
policy temporarily and persistently. Finally, we reviewed the SELinux
Troubleshooter program and used it to view and analyze SELinux related
messages.

**[Review]{#part0033_split_001.html#id_558 .calibre10} Questions**

1[.]{.c19}What is the name and location of the SELinux configuration
file?

2[.]{.c19}What would the command *semanage fcontext -Cl* do?

3[.]{.c19}Which command can be used to ensure modified contexts will
survive file system relabeling?

4[.]{.c19}What would the command *semanage login -a -s user_u user10*
do?

5[.]{.c19}Name the two commands that can be used to modify a Boolean
value.

6[.]{.c19}Which option is used with the *ps* command to view the
security contexts for processes?

7[.]{.c19}What would the command *restorecon -F *etc*sysconfig* do?

8[.]{.c19}What is the name of the default SELinux policy used in RHEL 8?

9[.]{.c19}What would the command *sestatus -b \| grep nfs_export_all_rw*
do?

10[.]{.c23}Name the directory that stores SELinux Boolean files.

11[.]{.c23}What are the two commands to display and modify the SELinux
mode?

12[.]{.c23}Name the two SELinux subjects.

13[.]{.c23}SELinux is an implementation of discretionary access control.
True or False?

14[.]{.c23}Where are SELinux denial messages logged in the absence of
the *auditd* daemon?

15[.]{.c23}Name the four parts of a process context.

16[.]{.c23}What one task must be done to change the SELinux mode from
enforcing to disabled?

17[.]{.c23}Which option with the *cp* command must be specified to
preserve SELinux contexts?

18[.]{.c23}What is the purpose of the command *sestatus*?

19[.]{.c23}With SELinux running in enforcing mode and one of the
services on the system is compromised, all other services will be
affected. True or False?

20[.]{.c23}Name the command that starts the SELinux Troubleshooter
program.

**[Answers]{#part0033_split_001.html#id_559 .calibre10} to Review
Questions**

1[.]{.c19}The SELinux configuration filename is *config* and it is
located in the **etc*selinux* directory.

2[.]{.c19}The command provided will show recent changes made to the
SELinux policy database.

3[.]{.c19}The *semanage* command.

4[.]{.c19}This command provided will map Linux user *user10* with
SELinux user user_u.

5[.]{.c19}The *semanage* and *setsebool* commands.

6[.]{.c19}The -Z option.

7[.]{.c19}The command provided will restore the default SELinux contexts
on the specified directory.

8[.]{.c19}The default SELinux policy used in RHEL 8 is targeted.

9[.]{.c19}The command provided will display the current value of the
specified Boolean.

10[.]{.c23}The **sys*fs/selinux/booleans* directory.

11[.]{.c23}The *getenforce* and *setenforce* commands.

12[.]{.c23}User and process are two SELinux subjects.

13[.]{.c23}False. SELinux is an implementation of mandatory access
control.

14[.]{.c23}The SELinux denial messages are logged to the
**var*log/messages* file.

15[.]{.c23}The four parts of a process context are user, role,
type/domain, and sensitivity level.

16[.]{.c23}The system must be rebooted.

17[.]{.c23}The \--preserve=context option.

18[.]{.c23}The *sestatus* command displays SELinux status information.

19[.]{.c23}False.

20[.]{.c23}The command name is *sealert*.

**[Do-]{#part0033_split_001.html#id_560 .calibre10}It-Yourself Challenge
Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform the labs
without external help. A step-by-step guide is not supplied, as the
knowledge and skill required to implement the lab has already been
disseminated in the chapter; however, hints to the relevant major
topic(s) are included.

**[Lab]{#part0033_split_001.html#id_561 .calibre10} 21-1: Disable and
Enable the SELinux Operating Mode**

As *user1* with *sudo* on *server10*, check and make a note of the
current SELinux operating mode. Modify the configuration file and set
the mode to disabled. Reboot the system to apply the change. Run **sudo
getenforce** to confirm the change when the system is up. Restore the
directive's value to enforcing in the configuration file, and reboot to
apply the new mode. Run **sudo getenforce** to confirm the mode when the
system is up. (Hint: SELinux Administration).

**[Lab]{#part0033_split_001.html#id_562 .calibre10} 21-2: Modify Context
on Files**

As *user1* with *sudo* on *server10*, create directory hierarchy
**tmp*d1/d2*. Check the contexts on **tmp*d1* and **tmp*d1/d2*. Change
the SELinux type on **tmp*d1* to etc_t recursively with the *chcon*
command and confirm. Add **tmp*d1* to the policy database with the
*semanage* command to ensure the new context is persistent on the
directory hierarchy. (Hint: SELinux Administration).

**[Lab]{#part0033_split_001.html#id_563 .calibre10} 21-3: Add Network
Port to Policy Database**

As *user1* with *sudo* on *server10*, add network port 9001 to the
SELinux policy database for the secure HTTP service using the *semanage*
command. Verify the addition. (Hint: SELinux Administration).

**[Lab]{#part0033_split_001.html#id_564 .calibre10} 21-4: Copy Files
with and without Context**

As *user1* with *sudo* on *server10*, create file *sef1* under */tmp*.
Copy the file to the **usr*local* directory. Check and compare the
contexts on both source and destination files. Create another file
*sef2* under */tmp* and copy it to the **var*local* directory using the
\--preserve=context option with the *cp* command. Check and compare the
contexts on both source and destination files. (Hint: SELinux
Administration).

**Lab 21-5: Flip SELinux Booleans**

As *user1* with *sudo* on *server10*, check the current value of Boolean
*ssh_use_tcpd* using the *getsebool* and *sestatus* commands. Use the
*setsebool* command and toggle the value of the directive. Confirm the
new value with the *getsebool*, *semanage*, or *sestatus* command.
(Hint: SELinux Administration).

[]{#part0034_split_000.html}

## Chapter 22 {#part0034_split_000.html#calibre_pb_0 .calibre11}

**Shell Scripting**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Overview of shell scripts

[![](images/00001.jpeg){.c37}]{.c36}Write scripts to display basic
system information, employ shell and environment variables, use command
substitution, and manipulate special and positional parameters

[![](images/00001.jpeg){.c37}]{.c36}Execute and debug scripts

[![](images/00001.jpeg){.c37}]{.c36}Know exit codes and test conditions

[![](images/00001.jpeg){.c37}]{.c36}Understand logical constructs:
if-then-fi, if-then-else-fi, and if-then-elif-fi

[![](images/00001.jpeg){.c37}]{.c36}Write scripts using logical
statements

[![](images/00001.jpeg){.c37}]{.c36}Know arithmetic test conditions

[![](images/00001.jpeg){.c37}]{.c36}Comprehend looping construct:
for-do-done

[![](images/00001.jpeg){.c37}]{.c36}Write scripts using looping
statements

[RHCSA Objectives:]{.c39}

[12]{#part0034_split_000.html#id_778 .calibre10}. Conditionally execute
code (use of: if, test, \[\], etc.)

[13]{#part0034_split_000.html#id_779 .calibre10}. Use Looping constructs
(for, etc.) to process file, command line input

[14]{#part0034_split_000.html#id_780 .calibre10}. Process script inputs
(\$1, \$2, etc.)

[15]{#part0034_split_000.html#id_781 .calibre10}. Processing output of
shell commands within a script

[16]{#part0034_split_000.html#id_782 .calibre10}. Processing shell
command exit codes

::: {#part0034_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0034_split_001.html}

[ S]{.c42}[hell]{.c43} scripts are essentially a group of Linux commands
along with control structures and optional comments stored in a text
file. Their primary purpose of creation is the automation of long and
repetitive tasks. Scripts may include any simple to complex command and
can be executed directly at the Linux command prompt. They do not need
to be compiled as they are interpreted by the shell line by line. This
chapter presents example scripts and analyzes them to solidify the
learning. These scripts begin with simple programs and advance to more
complicated ones. As with any other programming language, the scripting
skill develops over time as more and more scripts are read, written, and
examined. This chapter also discusses a debug technique that may be
helpful in troubleshooting the code.

**[Shell]{#part0034_split_001.html#id_565 .calibre10} Scripts**

Shell scripts (a.k.a. *shell programs* or simply *scripts*) are text
files that contain Linux commands and control structures to automate
lengthy, complex, or repetitive tasks, such as managing packages and
users, administering partitions and file systems, monitoring file system
utilization, trimming log files, archiving and compressing files,
finding and removing unnecessary files, starting and stopping database
services and applications, and producing reports. Commands in a script
are interpreted and run by the shell one at a time in the order in which
they are listed. Each line is executed as if it is typed and run at the
command prompt. Control structures are utilized for creating and
managing conditional and looping constructs. Comments are also generally
included to add information about the script such as the author name,
creation date, previous modification dates, purpose, and usage. If the
script encounters an error during execution, the error message is
printed on the screen.

Scripts presented in this chapter are written in the bash shell and may
be used in other Linux shells with slight modifications.

You can use any available text editor to write the scripts; however, it
is suggested to use the *vim* editor so that you have an opportunity to
practice it as you learn scripting. To quickly identify where things are
in your scripts, you can use the *nl* command to enumerate the lines.
You can store your scripts in the **usr*local/bin* directory, which is
included in the PATH of all users by default.

**[Script]{#part0034_split_001.html#id_566 .calibre10}01: Displaying
System Information**

Let's create the first script called *sys_info.sh* on *server10* in the
**usr*local/bin* directory and examine it line by line. Use the *vim*
editor with *sudo* to write the script. Type what you see below. Do not
enter the line numbers, as they are used for explanation and reference.

![](images/01012.jpeg){.image2}

![](images/00002.jpeg){.image} Within vim, press the ESC key and then
type :set nu to view line numbers associated with each line entry.

In this script, comments and commands are used as follows:

The first line indicates the shell that will run the commands in the
script. This line must start with the "#!" character combination (called
*shebang*) followed by the full pathname to the shell file.

The next three lines contain comments: the script name, author name,
creation time, default directory location for storage, and purpose. The
number sign (#) implies that anything to the right of it is
informational and will be ignored during script execution. Note that the
first line also uses the number character (#), but it is followed by the
exclamation mark (!); that combination has a special meaning to the
shell.

The fifth line has the first command of the script. The *echo* command
prints on the screen whatever follows it ("Display Basic System
Information" in this case). This may include general comments, errors,
or script usage.

The sixth line will highlight the text "Display Basic System
Information" by underlining it.

The seventh line has the *echo* command followed by nothing. This will
insert an empty line in the output.

The eighth line will print "The hostname, hardware, and OS information
is:".

The ninth line will execute the *hostnamectl* command to display basic
information about the system.

The tenth line will insert an empty line.

The eleventh line will print "The following users are currently logged
in:" on the screen.

The twelfth line will execute the *who* command to list the logged-in
users.

Here is the listing of the *sys_info.sh* file created in the
**usr*local/bin* directory:

![](images/01013.jpeg){.image2}

**[Executing]{#part0034_split_001.html#id_567 .calibre10} a Script**

The script created above does not have the execute permission bit since
the default umask for the *root* user is set to 0022, which allows
read/write access to the owner, and read-only access to the rest. You
will need to run the *chmod* command on the file and add an execute bit
for everyone:

![](images/01014.jpeg){.image2}

Any user on the system can now run this script using either its name or
the full path.

Let's run the script and see what the output will look like:

![](images/01015.jpeg){.image2}

The output reflects the execution of the commands as scripted.

The *hostnamectl* command displays the hostname of the system, type of
platform (physical, virtual) it is running on, hypervisor vendor name,
operating system name and version, current kernel version, hardware
architecture, and other information.

**[Debugging]{#part0034_split_001.html#id_568 .calibre10} a Script**

Before you have a perfectly working script in place, you may have to run
and modify it more than once. You can use a debugging technique that
will help identify where the script might have failed or did not
function as expected. You can either append the -x option to the
"#!/bin/bash" at the beginning of the script to look like "#!/bin/bash
-x", or execute the script as follows:

![](images/01016.jpeg){.image2}

The above output now also includes the actual lines from the script
prefixed by the + sign and followed by the command execution result. It
also shows the line number of the problem line in the output if there is
any. This way you can identify any issues pertaining to the path,
command name, use of special characters, etc., and address it quickly.
Try changing any of the *echo* commands in the script to "iecho" and
re-run the script in the debug mode to confirm what has just been said.

**[Script]{#part0034_split_001.html#id_569 .calibre10}02: Using Local
Variables**

You had worked with variables earlier in the book and seen their usage.
To recap, there are two types of variables: *local* (also called
*private* or *shell*) and *environment*. Both can be defined and used in
scripts and at the command line.

The following script called *use_var.sh* will define a local variable
and display its value on the screen. You will re-check the value of this
variable after the script execution has completed. The comments have
been excluded for brevity.

![](images/01017.jpeg){.image2}

Add the execute bit to the script. The following output will be
generated when you run it:

![](images/01018.jpeg){.image2}

If you run the *echo* command to see what is stored in the SYSNAME
variable, you will get nothing:

![](images/01019.jpeg){.image2}

The output is self-explanatory.

**[Script]{#part0034_split_001.html#id_570 .calibre10}03: Using
Pre-Defined Environment Variables**

The following script called *pre_env.sh* will display the values of
SHELL and LOGNAME environment variables:

![](images/01020.jpeg){.image2}

Add the execute bit to the script, and run to view the result:

![](images/01021.jpeg){.image2}

The output is self-explanatory.

**[Script]{#part0034_split_001.html#id_571 .calibre10}04: Using Command
Substitution**

During the execution of a script, you can use the command substitution
feature of the bash shell and store the output generated by the command
into a variable. For example, the following script called *cmd_out.sh*
will run the *hostname* and *uname* commands and save their outputs in
variables. This script shows two different ways to use command
substitution. Make sure to use the backticks (normally located with the
\~ character on the keyboard) to enclose the *uname* command.

![](images/01022.jpeg){.image2}

Add the execute bit and run the script:

![](images/01023.jpeg){.image2}

The output is self-explanatory.

**[Understanding]{#part0034_split_001.html#id_572 .calibre10} Shell
Parameters**

A *shell parameter* (or simply a *parameter*) is an entity that holds a
value such as a name, special character, or number. The parameter that
holds a name is referred to as a variable; a special character is
referred to as a *special parameter*; and one or more digits, except for
0 is referred to as a *positional parameter* (a.k.a. a *command line
argument*). A special parameter represents the command or script itself
(\$0), count of supplied arguments (\$\* or \$@), all arguments (\$#),
and PID of the process (\$\$). A positional parameter (\$1, \$2, \$3 . .
.) is an argument supplied to a script at the time of its invocation,
and its position is determined by the shell based on its location with
respect to the calling script. [Figure
22-1](#part0034_split_001.html#page_487){.calibre5} gives a pictorial
view of the special and positional parameters.

::: c49
::: width_
![](images/01024.jpeg){.calibre13}
:::

**Figure 22-1 Shell Parameters**
:::

[Figure 22-1](#part0034_split_001.html#page_487){.calibre5} also shows
that positional parameters beyond 9 are to be enclosed in curly
brackets. Just like the variable and command substitutions, the shell
uses the dollar (\$) sign for special and positional parameter
expansions as well.

**[Script]{#part0034_split_001.html#id_573 .calibre10}05: Using Special
and Positional Parameters**

The script *com_line_arg.sh* below will show the supplied arguments,
total count, value of the first argument, and PID of the script:

![](images/01025.jpeg){.image2}

The result will be as follows when the script is executed with four
arguments. Do not forget to add the execute bit prior to running it.

![](images/01026.jpeg){.image2}

The output is self-explanatory.

**[Script]{#part0034_split_001.html#id_574 .calibre10}06: Shifting
Command Line Arguments**

The *shift* command is used to move arguments one position to the left.
During this move, the value of the first argument is lost. The
*com_line_arg_shift.sh* script below is an extension to the
*com_line_arg.sh* script. It uses the *shift* command to show what
happens when arguments are moved.

![](images/01027.jpeg){.image2}

Let's execute the script with the same four arguments. Notice that a new
value is assigned to \$1 after each shift.

![](images/01028.jpeg){.image2}

Multiple shifts in a single attempt may be performed by furnishing a
count of desired shifts to the *shift* command as an argument. For
example, "*shift* 2" will carry out two shifts, "*shift* 3" will make
three shifts, and so on.

**[Logical]{#part0034_split_001.html#id_575 .calibre10} Constructs**

So far, we have talked about simple scripts that run the code line by
line. The shell lets us employ logical constructs to control the flow of
scripts. It does this by allowing us to use test conditions, which
decides what to do next based on the true or false status of the
condition.

The shell offers two logical constructs: the *if-then-fi* construct and
the *case* construct. The if-then-fi construct has a few variations and
those will be covered as well. A discussion on the case construct is
beyond the scope.

Before starting to look at the example scripts and see how logical
constructs are used, let's discuss exit codes and various test
conditions. You will use them later in the example scripts.

**[Exit]{#part0034_split_001.html#id_576 .calibre10} Codes**

*Exit codes*, or *exit values*, refer to the value returned by a command
when it finishes execution. This value is based on the outcome of the
command. If the command runs successfully, you typically get a zero exit
code, otherwise you get a non-zero value. This code is also referred to
as a *return code,* and it is stored in a special shell parameter called
*?* (question mark). Let's look at the following two examples to
understand their usage:

![](images/01029.jpeg){.image2}

In the first example, the *pwd* command ran successfully and it produced
the desired result, hence a zero exit code was returned and stored in
the ?. In the second example, the *man* command did not run successfully
because of a missing argument, therefore a non-zero exit code was
returned and stored in the ?.

You can define exit codes within a script at different locations to help
debug the script by knowing where exactly it terminated.

**[Test]{#part0034_split_001.html#id_577 .calibre10} Conditions**

Test conditions are used in logical constructs to decide what to do
next. They can be set on integer values, string values, or files using
the *test* command or by enclosing them within the square brackets \[\].
[Table 22-1](#part0034_split_001.html#id_760){.calibre5} describes
various test condition operators.

::: c49
  ---------------------------------- -----------------------------------------------------------------------------------------------------------------------------------------
  **Operation on Integer Value**     **Description**
  integer1 -eq (-ne) integer2        Integer1 is equal (not equal) to integer2
  integer1 -lt (-gt) integer2        Integer1 is less (greater) than integer2
  integer1 -le (-ge) integer2        Integer1 is less (greater) than or equal to integer2
  **Operation on String Value**      **Description**
  string1=(!=)string2                Tests whether the two strings are identical (not identical)
  -l string or -z string             Tests whether the string length is zero
  string or -n string                Tests whether the string length is non-zero
  **Operation on File**              **Description**
  -b (-c) file                       Tests whether the file is a block (character) device file
  -d (-f) file                       Tests whether the file is a directory (normal file)
  -e (-s) file                       Tests whether the file exists (non-empty)
  -L file                            Tests whether the file is a symlink
  -r (-w) (-x) file                  Tests whether the file is readable (writable) (executable)
  -u (-g) (-k) file                  Tests whether the file has the setuid (setgid) (sticky) bit
  file1 -nt (-ot) file2              Tests whether file1 is newer (older) than file2
  **Logical Operators**              **Description**
  !                                  The logical NOT operator
  -a or && (two ampersand            The logical AND operator. Both operands must be true for
  characters)                        the condition to be true. Syntax: \[ -b file1 && -r file1 \]
  -o or \|\| (two pipe characters)   The logical OR operator. Either of the two or both operands must be true for the condition to be true. Syntax: \[ (x == 1 -o y == 2) \]
  ---------------------------------- -----------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0034_split_001.html#id_760} 22-1 Test Conditions**
:::

Having described the exit codes and test conditions, let's look at a few
example scripts and observe their effects.

**[The]{#part0034_split_001.html#id_578 .calibre10} if-then-fi
Construct**

The if-then-fi statement evaluates the condition for true or false. It
executes the specified action if the condition is true; otherwise, it
exits the construct. The if-then-fi statement begins with an "if" and
ends with a "fi", as depicted in the flow diagram in [Figure
22-2](#part0034_split_001.html#id_681){.calibre5}:

::: c49
::: width_
![](images/01030.jpeg){.calibre14}
:::

**Figure 22-2 The if-then-fi Construct**
:::

The general syntax of this statement is as follows:

  ------ -----------
  if     condition
  then   
         action
  fi     
  ------ -----------

This construct is used in the following script.

**[Script]{#part0034_split_001.html#id_579 .calibre10}07: The if-then-fi
Construct**

You saw earlier how to check the number of arguments supplied at the
command line. The following script called *if_then_fi.sh* determines the
number of arguments and prints an error message if there are none
provided:

![](images/01031.jpeg){.image2}

This script will display the following messages on the screen if it is
executed without exactly two arguments specified at the command line:

![](images/01032.jpeg){.image2}

A value of 2 will appear upon examining the return code as follows. This
value reflects the exit code that you defined in the script on line 6.

![](images/01033.jpeg){.image2}

Conversely, the return code will be 0 and the message will be as follows
if you supply a pair of arguments:

![](images/01034.jpeg){.image2}

**[The]{#part0034_split_001.html#id_580 .calibre10} if-then-else-fi
Construct**

The if-then-fi statement has a limitation and it can execute an action
only if the specified condition is true. It quits the statement if the
condition is untrue. The if-then-else-fi statement, in contrast, is more
advanced in the sense that it can execute an action if the condition is
true and another action if the condition is false. The flow diagram for
this structure is shown in [Figure
22-3](#part0034_split_001.html#page_492){.calibre5}:

::: c49
::: width_
![](images/01035.jpeg){.calibre14}
:::

**Figure 22-3 The if-then-else-fi Construct**
:::

The general syntax of this statement is as follows:

  ------ -----------
  if     condition
  then   
         action1
  else   
         action2
  fi     
  ------ -----------

action1 or action2 is performed based on the true or false evaluation of
the condition.

**[Script]{#part0034_split_001.html#id_581 .calibre10}08: The
if-then-else-fi Construct**

The following script called *if_then_else_fi.sh* will accept an integer
value as an argument and tell if the value is positive or negative:

![](images/01036.jpeg){.image2}

Run this script one time with a positive integer value and the next time
with a negative value:

![](images/01037.jpeg){.image2}

Try the script again but with a non-integer value and see what it does.

**[The]{#part0034_split_001.html#id_582 .calibre10} if-then-elif-fi
Construct**

The if-then-elif-fi is a more sophisticated construct than the other two
conditional statements. You can define multiple conditions and associate
an action with each one of them. During the evaluation, the action
corresponding to the true condition is performed. The flow diagram for
this structure is shown in [Figure
22-4](#part0034_split_001.html#id_682){.calibre5}:

::: c49
::: width_
![](images/01038.jpeg){.calibre13}
:::

**Figure 22-4 The if-then-elif-fi Construct**
:::

The general syntax of this statement is as follows:

  -------------- ------------
  if             condition1
  then           
                 action1
  elif           condition2
  then           
                 action2
  elif           condition3
  then           
                 action3
  ............   
  else           
                 action(n)
  fi             
  -------------- ------------

Let's use the if-then-elif-fi construct in the following two example
scripts.

**[Script]{#part0034_split_001.html#id_583 .calibre10}09: The
if-then-elif-fi Construct (Example 1)**

The *if_then_elif_fi.sh* script is an enhanced version of the
*if_then_else_fi.sh* script. It accepts an integer value as an argument
and tells if it is positive, negative, or zero. If a non-integer value
or no argument is supplied, the script will complaint. Notice that the
script employs the *exit* command after each action to help you identify
where it exited.

![](images/01039.jpeg){.image2}

Run this script four times: the first time with a positive integer, the
second time with 0, the third time with a negative integer, and the
fourth time with a non-integer value. Check the exit code after each
execution to know where the script exited.

![](images/01040.jpeg){.image2}

The outputs and exit values reflect the program code.

**[Script]{#part0034_split_001.html#id_584 .calibre10}10: The
if-then-elif-fi Construct (Example 2)**

The script *ex200_ex294.sh* will display the name of the Red Hat exam
RHCSA or RHCE in the output based on the input argument (ex200 or
ex294). If a random or no argument is provided, it will print "Usage:
Acceptable values are ex200 and ex294". Make sure to add white spaces in
the conditions as shown.

![](images/01041.jpeg){.image2}

Run this script three times: the first time with argument ex200, the
second time with argument ex294, and the third time with something
random as an argument:

![](images/01042.jpeg){.image2}

The results are as expected.

[**EXAM TIP:**]{.c56} A good understanding of the usage of logical
statements is important.

**[Looping]{#part0034_split_001.html#id_585 .calibre10} Constructs**

As a Linux user and administrator, you often want to perform certain
task on a number of given elements or repeatedly until a specified
condition becomes true or false. For instance, if plenty of disks need
to be initialized for use in LVM, you can either run the *pvcreate*
command on each disk one at a time manually or employ a loop to do it
for you. Likewise, based on a condition, you may want a program to
continue to run until that condition becomes true or false.

There are three constructs---*for-do-done*, *while-do-done*, and
*until-do-done*---that you can use to implement looping.

![](images/00002.jpeg){.image} The for loop is also referred to as the
foreach loop.

The for loop iterates on a list of given values until the list is
exhausted. The while loop runs repeatedly until the specified condition
becomes false. The until loop does just the opposite of the while loop;
it performs an operation repeatedly until the specified condition
becomes true. This chapter will discuss the for loop only; the other two
are out of scope.

**[Test]{#part0034_split_001.html#id_586 .calibre10} Conditions**

The *let* command is used in looping constructs to evaluate a condition
at each iteration. It compares the value stored in a variable against a
pre-defined value. Each time the loop does an iteration, the variable
value is altered. You can enclose the condition for arithmetic
evaluation within a pair of parentheses (( )) or quotation marks (" ")
instead of using the *let* command explicitly.

[Table 22-2](#part0034_split_001.html#id_761){.calibre5} lists operators
that can be used in test conditions.

  -------------- --------------------------------------------------
  **Operator**   **Description**
  !              Negation
  \+ *--* \*     Addition *subtraction* multiplication / division
  \%             Remainder
  \< / \<=       Less than / less than or equal to
  \> / \>=       Greater than / greater than or equal to
  =              Assignment
  == / !=        Comparison for equality / non-equality
  -------------- --------------------------------------------------

**[Table]{#part0034_split_001.html#id_761} 22-2 Arithmetic Operators**

Having described various test condition operators, let's look at the
syntax of the for loop and a few example scripts and observe the
implications of some of the test conditions.

**[The]{#part0034_split_001.html#id_587 .calibre10} for Loop**

The for loop is executed on an array of elements until all the elements
in the array are consumed. Each element is assigned to a variable one
after the other for processing. The flow diagram for this construct is
displayed in [Figure 22-5](#part0034_split_001.html#id_683){.calibre5}:

::: c49
::: width_
![](images/01043.jpeg){.calibre14}
:::

**Figure 22-5 The for Loop** The general syntax of this construct is as
follows:
:::

  ----------------- --------
  for VAR in list   
  do                
                    action
  done              
  ----------------- --------

**[Script]{#part0034_split_001.html#id_588 .calibre10}11: Print
Alphabets Using for Loop**

The *for_do_done.sh* script initializes the variable COUNT to 0. The for
loop will read each letter sequentially from the range placed within
curly brackets (no spaces before the letter A and after the letter Z),
assign it to another variable LETTER, and display the value on the
screen. The *expr* command is an arithmetic processor and it is used
here to increment the COUNT by 1 at each loop iteration.

![](images/01044.jpeg){.image2}

The output of the script will be:

![](images/01045.jpeg){.image2}

**[Script]{#part0034_split_001.html#id_589 .calibre10}12: Create Users
Using for Loop**

The *create_user.sh* script can create several Linux user accounts. As
each account is created, the value of the variable ? is checked. If the
value is 0, a message saying the account is created successfully will be
displayed, otherwise the script will terminate. In case of a successful
account creation, the *passwd* command will be invoked to assign the
user the same password as their username.

![](images/01046.jpeg){.image2}

The result of the script execution below confirms the addition of three
new user accounts:

![](images/01047.jpeg){.image2}

If this script is re-executed without modifying the list of elements
(user names), the following will appear:

![](images/01048.jpeg){.image2}

[**EXAM TIP:**]{.c56} A good understanding of the looping construct will
help on the exam.

**[Chapter]{#part0034_split_001.html#id_590 .calibre10} Summary**

In this chapter, we learned the basics of bash shell scripting. This
chapter began with an overview of scripting and then demonstrated how to
write and analyze test scripts using various built-in features of the
bash shell. We wrote and examined simple code and gradually advanced to
more advanced scripts, including those that employed logical and looping
constructs. We learned how to identify problem lines in our scripts.
After understanding and practicing the scripts presented in this
chapter, you should be able to write your own programs, debug them, and
examine those authored by others.

**[Review]{#part0034_split_001.html#id_591 .calibre10} Questions**

1[.]{.c19}What are the three major components in a shell script?

2[.]{.c19}Which looping construct can be used to perform an action on
listed items?

3[.]{.c19}What is the function of the *shift* command?

4[.]{.c19}You can script the startup and shutdown of a database. True or
False?

5[.]{.c19}What does the *echo* command do without any arguments?

6[.]{.c19}What would the command *echo \$?* do?

7[.]{.c19}When would you want to use an exit code in your script?

8[.]{.c19}What would you modify in a shell script to run it in the debug
mode?

9[.]{.c19}What are the two types of logical constructs mentioned in this
chapter?

10[.]{.c23}What would != imply in a looping condition?

11[.]{.c23}What is the difference between a line in a script starting
with a "#" and a "#!"?

12[.]{.c23}What comments may you want to include in a shell script?
Write any six.

13[.]{.c23}What is one benefit of writing shell scripts?

14[.]{.c23}What would the command *bash -x *usr*local/bin/script1.sh*
do?

**[Answers]{#part0034_split_001.html#id_592 .calibre10} to Review
Questions**

1[.]{.c19}The three major components in a shell script are commands,
control structures, and comments.

2[.]{.c19}The for loop.

3[.]{.c19}The *shift* command moves an argument to the left.

4[.]{.c19}True.

5[.]{.c19}The *echo* command inserts an empty line in the output when
used without arguments.

6[.]{.c19}This command will display the exit code of the last command
executed.

7[.]{.c19}The purpose of using an exit code is to determine exactly
where the script quits.

8[.]{.c19}We would specify -x as an argument to the shell path.

9[.]{.c19}The if and case constructs.

10[.]{.c23}!= would check the value for non-equality.

11[.]{.c23}The former is used to include general comments in the script
and the latter combination dictates the full path to the shell file that
is to be used to execute the script.

12[.]{.c23}The author name, creation date, last modification date,
location, purpose, and usage.

13[.]{.c23}One major benefit of writing shell scripts is to automate
lengthy and repetitive tasks.

14[.]{.c23}This command will execute *script1.sh* in debug mode.

**[DIY]{#part0034_split_001.html#id_593 .calibre10} Challenge Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform these
labs without any additional help. A step-by-step guide is not provided,
as the implementation of these labs requires the knowledge that has been
presented in this chapter. Use defaults or your own thinking for missing
information.

**[Lab]{#part0034_split_001.html#id_594 .calibre10} 22-1: Write a Script
to Create Logical Volumes**

For this lab, present 2x1GB virtual disks to your system in VirtualBox
VM Manager. Write a single bash shell script to create 2x800MB
partitions on each disk using *parted* and then bring both partitions
into LVM control with the *pvcreate* command. Create a volume group
called *vgscript* and add both PVs to it. Create three logical volumes
each of size 500MB and name them *lvscript1*, *lvscript2*, and
*lvscript3*.

**[Lab]{#part0034_split_001.html#id_595 .calibre10} 22-2: Write a Script
to Create File Systems**

This lab is a continuation of Lab 22-1. Write another bash shell script
to create xfs, ext4, and vfat file system structures in each logical
volume, respectively. Create mount points **mnt*xfs*, **mnt*ext4*, and
**mnt*vfat*, and mount the file systems. Include the *df* command with
-h to list the mounted file systems.

**[Lab]{#part0034_split_001.html#id_596 .calibre10} 22-3: Write a Script
to Configure a New Network Profile**

For this lab, present a new network interface to your system in
VirtualBox VM Manager. Write a single bash shell script to run the
*nmcli* command to configure custom IP assignments (choose your own
settings) on the new network device and ensure that they are
automatically applied to it on future system reboots. Make a copy of the
**etc*hosts* file as part of this script. Choose a hostname of your
choice and add a mapping to the **etc*hosts* file without overwriting
existing file content.

[]{#part0035_split_000.html}

## Chapter 23 {#part0035_split_000.html#calibre_pb_0 .calibre11}

**Containers**

This chapter describes the following major topics:

[![](images/00001.jpeg){.c37}]{.c36}Understand container technology

[![](images/00001.jpeg){.c37}]{.c36}Identify key Linux features that
establish the foundation to run containers

[![](images/00001.jpeg){.c37}]{.c36}Analyze container benefits

[![](images/00001.jpeg){.c37}]{.c36}A better home for containers

[![](images/00001.jpeg){.c37}]{.c36}Grasp the concepts of container
images and registries

[![](images/00001.jpeg){.c37}]{.c36}Compare pros and cons of root and
rootless containers

[![](images/00001.jpeg){.c37}]{.c36}Examine registry configuration file

[![](images/00001.jpeg){.c37}]{.c36}Work with container images (find,
inspect, pull, list, and delete)

[![](images/00001.jpeg){.c37}]{.c36}Administer basic containers (start,
list, stop, remove, interact with, run commands from outside, attach to,
run custom entry point commands, etc.)

[![](images/00001.jpeg){.c37}]{.c36}Implement advanced container
features (port mapping, environment variables, and persistent storage)

[![](images/00001.jpeg){.c37}]{.c36}Control container operational states
via systemd

[RHCSA Objectives:]{.c39}

[63]{#part0035_split_000.html#id_827 .calibre10}. Find and retrieve
container images from a remote registry

[64]{#part0035_split_000.html#id_828 .calibre10}. Inspect container
images

[65]{#part0035_split_000.html#id_829 .calibre10}. Perform container
management using commands such as podman and skopeo

[66]{#part0035_split_000.html#id_830 .calibre10}. Perform basic
container management such as running, starting, stopping, and listing
running containers

[67]{#part0035_split_000.html#id_831 .calibre10}. Run a service inside a
container

[68]{#part0035_split_000.html#id_832 .calibre10}. Configure a container
to start automatically as a systemd service

[69]{#part0035_split_000.html#id_833 .calibre10}. Attach persistent
storage to a container

::: {#part0035_split_000.html#calibre_pb_1 .calibre12}
:::

[]{#part0035_split_001.html}

[ C]{.c42}[ontainers]{.c43} and containerization technologies such as
Docker and Kubernetes have received an overwhelming appreciation and
massive popularity in recent years. They are now part of many new
deployments. Containers offer an improved method to package distributed
applications, deploy them in a consistent manner, and run them in
isolation from one another on the same or different virtual or physical
server(s). Containers take advantage of the native virtualization
features available in the Linux kernel. Each container typically
encapsulates one self-contained application that includes all
dependencies such as library files, configuration files, software
binaries, and services.

This chapter presents an overview of container images, container
registries, and containers. It shows how to interact with images and
registries. It demonstrates how to launch, manage, and interact with
containers. It discusses advanced topics such as mapping a host port
with a container port, passing and setting environment variables, and
attaching host storage for data persistence. The chapter ends with a
detailed look at controlling the operational states of containers via
the systemd service. There are numerous exercises in the chapter to
support the concepts learned.

**[Introduction]{#part0035_split_001.html#id_597 .calibre10} to
Containers**

A *container* is essentially a set of processes that runs in complete
seclusion on a Linux system. A single Linux system running on bare metal
hardware or in a virtual machine may have tens or hundreds of containers
running. The underlying hardware may be located either on the ground or
in the cloud.

Traditionally, one or more software applications are deployed on a
single server. These applications may have conflicting requirements in
terms of shared library files, package dependencies, and software
version. Moreover, patching or an update to the operating system may
result in breaking an application functionality. To address these and
other potential challenges, developers perform an analysis on their
current deployments before they decide whether to collocate a new
application with an existing one or to go with a new server without
taking the risk of breaking the current operation.

Fortunately, there is a better choice available now in the form of
containers. Developers can now package their application alongside
dependencies, shared library files, environment variables, and other
specifics in a single image file and use that file to run the
application in a unique, isolated "virtual computer" called container.
Each container is treated as a complete whole, which can be tagged,
started, stopped, restarted, or even transported to another server
independent of other containers. This way any conflicts that may exist
among applications, within application components, or with the operating
system can be evaded. Applications encapsulated to run inside containers
are called *containerized applications*, and this is a growing trend for
architecting and deploying applications, application components, and
databases in real world environments.

**[Containers]{#part0035_split_001.html#id_598 .calibre10} and the Linux
Features**

The container technology employs some of the core features available in
the Linux kernel. These include control groups, namespaces, seccomp
(*secure computing mode*), and SELinux. A short description of each
feature is provided below:

**Control Groups**

*Control groups* (abbreviated as *cgroups*) splits processes into groups
to set limits on their consumption of compute resources---CPU, memory,
disk, and network I/O. These restrictions result in controlling
individual processes from over utilizing available resources.

**Namespaces**

*Namespaces* restrict the ability of process groups from seeing or
accessing system resources---PIDs, network interfaces, mount points,
hostname, etc.---thus instituting a layer of isolation between process
groups and the rest of the system. This feature guarantees a secure,
performant, and stable environment for containerized applications as
well as the host operating system.

**Secure Computing Mode (seccomp) and SELinux**

These Linux features impose security constraints thereby protecting
processes from one another and the host operating system from running
processes.

The container technology employs these characteristics to run processes
isolated in a highly secure environment with full control over what they
can or cannot do.

**[Benefits]{#part0035_split_001.html#id_599 .calibre10} of Using
Containers**

There are several benefits linked with using containers. These benefits
range from security to manageability and from independence to velocity.
The following provides a quick look at common containerization benefits:

**Isolation**

Containers are not affected due to changes in the host operating system
or in other hosted or containerized applications, as they run fully
isolated from the rest of the environment.

**Loose Coupling**

Containerized applications are loosely coupled with the underlying
operating system due to their self-containment and minimal level of
dependency.

**Maintenance Independence**

Maintenance is performed independently on individual containers.

**Less Overhead**

Containers require fewer system resources than do bare metal and virtual
servers.

**Transition Time**

Containers require a few seconds to start and stop.

**Transition Independence**

Transitioning from one state to another (start or stop) is independent
of other containers, and it does not affect or require a restart of any
underlying host operating system service.

**Portability**

Containers can be migrated to other servers without modifications to the
contained applications. The target servers may be bare metal or virtual
and located on-premises or in the cloud.

**Reusability**

The same container image can be used to run identical containers in
development, test, preproduction, and production environments. There is
no need to rebuild the image.

**Rapidity**

The container technology allows for accelerated application development,
testing, deployment, patching, and scaling. Also, there is no need for
an exhaustive testing.

**Version Control**

Container images can be version-controlled, which gives users the
flexibility in choosing the right version to run a container.

**[Container]{#part0035_split_001.html#id_600 .calibre10} Home: Bare
Metal or Virtual Machine**

A hypervisor software such as VMware ESXi, Oracle VirtualBox, Microsoft
Hyper-V, or KVM allows multiple virtual machines to run on the same bare
metal physical server. Each virtual machine runs an isolated,
independent instance of its own guest operating system. All virtual
machines run in parallel and share the resources of the underlying
hardware of the bare metal server. Each virtual machine may run multiple
applications that share the virtualized resources allocated to it. The
hypervisor runs a set of services on the bare metal server to enable
virtualization and support virtual machines, which introduces management
and operational overheads. Besides, any updates to the guest operating
system may require a reboot or result in an application failure.

Containers, in contrast, run directly on the underlying operating system
whether it be running on a bare metal server or in a virtual machine.
They share hardware and operating system resources securely among
themselves. This allows containerized applications to stay lightweight
and isolated, and run in parallel. Containers share the same Linux
kernel and require far fewer hardware resources than do virtual
machines, which contributes to their speedy start and stop. Given the
presence of an extra layer of hypervisor services, it may be more
beneficial and economical to run containers directly on non-virtualized
physical servers.

[Figure 23-1](#part0035_split_001.html#page_505){.calibre5} depicts how
applications are placed on traditional bare metal hardware, in virtual
machines, and to run as containers on bare metal servers.

::: c49
::: width_
![](images/01049.jpeg){.calibre13}
:::

**Figure 23-1 Container Home: Bare Metal or Virtual Machine**
:::

The added layer of hypervisor is shown in the middle stack. Any of the
above implementation can run multiple applications concurrently. A
decision as to which option to go with requires a careful use case
study; however, the benefits of running containers on bare metal servers
are obvious.

**[Container]{#part0035_split_001.html#id_601 .calibre10} Images and
Container Registries**

Launching a container requires a pre-packaged image to be available. A
container *image* is essentially a file that is built with all necessary
components---application binaries, library files, configuration
settings, environment variables, static data files, etc.---required by
an application to run smoothly, securely, and successfully included.
RHEL follows the *open container initiative* (OCI) to allow users to
build images based on industry standard specifications. An OCI-compliant
image can be executed and managed with OCI-compliant tools such as
*podman* (*pod manager*) and Docker. Images can be version-controlled
giving users the suppleness to use the latest or any of the previous
versions to launch their containers. A single image can be used to run
several containers at once.

Container images adhere to a standard naming convention for
identification. This is referred to as *fully qualified image name*
(FQIN). An FQIN is comprised of four components: (1) the storage
location (registry_name), (2) the owner or organization name
(user_name), (3) a unique repository name (repo_name), and (4) an
optional version (tag). The syntax of an FQIN is
*registry_name/user_name/repo_name:tag*.

Images are stored and maintained in public or private *registries*;
however, they need to be downloaded and made locally available for
consumption. There are several registries available on the Internet.
These include Red Hat Container Catalog at *registry.redhat.io* (or
*[registry.access.redhat.com](http://registry.access.redhat.com){.calibre5}*[),](http://registry.access.redhat.com){.calibre5}
Red Hat Quay at *quay.io*, and Docker Hub at
*[hub.docker.com](http://hub.docker.com.Additional){.calibre5}*[.Additional](http://hub.docker.com.Additional){.calibre5}
registries may be added as required. Private registries may require
authentication for access.

**Root vs. Rootless Containers**

Containers can be launched with the *root* user privileges. This gives
containers full access to perform administrative functions including the
ability to map privileged network ports (1024 and below). However,
launching containers with superuser rights opens a gate to potential
unauthorized access if a container is compromised due to a vulnerability
or misconfiguration.

To secure containers and the underlying operating system, the concept of
*rootless* container was instituted. Rootless containers allow normal,
unprivileged Linux users to run containers and interact with them just
like the *root* user, but without the ability to perform tasks that
require privileged access. Running containers without *root* is gaining
traction.

**[Working]{#part0035_split_001.html#id_602 .calibre10} with Images and
Containers**

To work with images and containers, a good comprehension of the
management commands and the configuration files involved is crucial. It
is also important to have the minimum required version of the necessary
software installed on the system to ensure the features you're looking
to implement are available. RHEL offers two commands---*podman* and
*skopeo*---to manage and interact with images, registries, and
containers. These tools can also be used to troubleshoot issues and
perform advanced management tasks; however, a discussion on those is
beyond the scope of this book.

**[Exercise]{#part0035_split_001.html#id_603 .calibre10} 23-1: Install
Necessary Container Support**

This exercise should be done on *server20* as *user1* with *sudo* where
required.

In this exercise, you will install the necessary software to set the
foundation for completing the exercises in the remainder of the chapter.
The standard RHEL 8.0 (and RHEL 8.2) image includes a module called
*container-tools* that consists of all the required components and
commands; however, a newer version is available in the Red Hat
Subscription Management (RHSM) service and that's what you will be
installing here to take advantage of the latest features and bug fixes.
You will use the standard *dnf* command to install the module.

1[.]{.c19}Register the system with RHSM to access the latest version of
the *container-tools* module. Use the credentials you created in
[Chapter 01](#part0013_split_000.html#page_1){.calibre5} "Local
Installation" to connect to the service. Enter your username at the
command line with the \--username option. Enter the *root* user password
first if you have invoked the *subscription-manager* command as a normal
user and then your Red Hat password.

![](images/01050.jpeg){.image2}

The system has been registered as indicated.

2[.]{.c19}Next, attach the server with the RHEL subscription using the
*attach* subcommand:

![](images/01051.jpeg){.image2}

*server20* is attached to the indicated subscription (Product Name) and
it has access to the Red Hat-maintained BaseOS and AppStream
repositories, which contain newer versions of the packages that the
corresponding repositories on the DVD image provide. The Red Hat repos
also incorporate a lot more software packages than do the DVD repos.

3[.]{.c19}Pull the metadata information for the Red Hat repositories:

![](images/01052.jpeg){.image2}

4[.]{.c19}Install the *container-tools* module:

![](images/01053.jpeg){.image2}

![](images/01054.jpeg){.image2}

See the *skopeo* and *podman* commands and their versions on the list of
installed packages above. They are now available on the system for
managing images and containers.

5[.]{.c19}Verify the module installation:

![](images/01055.jpeg){.image2}

The output confirms a successful installation (\[i\] beside the common
profile) of the *container-tools* module from RHSM.

This concludes the exercise.

**[The]{#part0035_split_001.html#id_604 .calibre10} podman Command**

Managing images and containers involves several operations such as
finding, inspecting, retrieving, and deleting images and running,
stopping, listing, and deleting containers. The *podman* command is used
for most of these operations. It supports numerous subcommands and
options. [Table 23-1](#part0035_split_001.html#id_762){.calibre5}
describes the ones that are used in this chapter.

::: c49
  -------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Subcommand**             **Description**
  **Image Management**       
  images                     Lists downloaded images from local storage
  inspect                    Examines an image and displays its details
  login/logout               Logs in/out to/from a container registry. A login may be required to access private and protected registries.
  pull                       Downloads an image to local storage from a registry
  rmi                        Removes an image from local storage
  search                     Searches for an image. The following options can be included with this subcommand:
                             1\. A partial image name in the search will produce a list of all images containing the partial name.
                             2\. The \--no-trunc option makes the command exhibit output without truncating it.
                             3\. The \--limit \<number\> option limits the displayed results to the specified number.
  tag                        Adds a name to an image. The default is 'latest' to classify the image as the latest version. Older images may have specific version identifiers.
  **Container Management**   
  attach                     Attaches to a running container
  exec                       Runs a process in a running container
  generate                   Generates a systemd unit configuration file that can be used to control the operational state of a container. The \--new option is important and is employed in later exercises.
  info                       Reveals system information, including the defined registries
  inspect                    Exhibits the configuration of a container
  ps                         Lists running containers (includes stopped containers with the -a option)
  rm                         Removes a container
  run                        Launches a new container from an image. Some options such as -d (detached), -i (interactive), and -t (terminal) are important and are employed in exercises where needed.
  start/stop/restart         Starts, stops, or restarts a container
  -------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

**[Table]{#part0035_split_001.html#id_762} 23-1 Common podman
Subcommands**
:::

All the subcommands described in [Table
23-1](#part0035_split_001.html#id_762){.calibre5} are used in the
upcoming exercises. Consult the manual pages of the *podman* command for
details on the usage of these and other subcommands.

[**EXAM TIP:**]{.c56} A solid understanding of the usage of the podman
command is key to completing container tasks.

**[The]{#part0035_split_001.html#id_605 .calibre10} skopeo Command**

The *skopeo* command is utilized for interacting with local and remote
images and registries. It has numerous subcommands available; however,
you will be using only the *inspect* subcommand to examine the details
of an image stored in a remote registry. View the manual pages of
*skopeo* for details on the usage of the *inspect* and other
subcommands.

**[The]{#part0035_split_001.html#id_606 .calibre10} registries.conf
File**

The system-wide configuration file for image registries is the
*registries.conf* file and it resides in the **etc*containers*
directory. Normal Linux users may store a customized copy of this file,
if required, under the *\~/.config/containers* directory. The settings
stored in the peruser file will take precedence over those stored in the
system-wide file. This is especially useful for running rootless
containers.

This file defines searchable and blocked registries. There are three
sections---registries.search, registries.insecure, and registries.block,
as evident from the below output:

![](images/01056.jpeg){.image2}

The registries.search section lists the registries that are searched if
an FQIN is not specified at the command line. By default, there are
three registries on the list (see the above output). You will be using
*registry.redhat.io* (old name
*[registry.access.redhat.com](http://registry.access.redhat.com){.calibre5}*)
primarily and the other two if needed. If access to an additional
registry is necessary, simply add it to the list and place it according
to the preference. For instance, if you want a private registry called
*registry.private.myorg.io* to be added with the highest priority, edit
the *registries.conf* file and make the following change:

\[registries.search\]

registries = \['registry.private.myorg.io', 'registry.redhat.io',
'quay.io', 'docker.io'\]

If this private registry is the only one to be used, you can take the
rest of the registry entries out of the list. In that case, the line
entry will look like: registries = registry.private.myorg.io.

The registries.insecure section lists the registries that do not have
valid SSL/TLS certificates. Insecure registries may also be added to the
registries.search section. There are none included in the file by
default.

The regsitries.block section lists registries that must not be used.

[**EXAM TIP:**]{.c56} As there is no Internet access provided during Red
Hat exams, you may have to access a network-based registry to download
images.

The default content of the file is good for many use cases; however, you
may see additional or different entries on busy systems.

**[Viewing]{#part0035_split_001.html#id_607 .calibre10} Podman
Configuration and Version**

The *podman* command references various system runtime and configuration
files and runs certain Linux commands in the background to gather and
display information. For instance, it looks for registries and storage
data in the system-wide and peruser configuration files, pulls memory
information from the **proc*meminfo* file, executes **uname -r** to
obtain the kernel version, and so on. *podman*'s *info* subcommand shows
all this information. Here is a sample when this command is executed as
a normal user (*user1*):

![](images/01057.jpeg){.image2}

![](images/01058.jpeg){.image2}

The above output is self-explanatory.

Now, re-run the command as *root* (precede by *sudo* if running as
*user1*) and compare the values for the settings "rootless" under host
and "ConfigFile" and "ImageStore" under store. The differences lie
between where the root and rootless (normal) users store and obtain
configuration data, the number of container images they have locally
available, and so on.

Similarly, you can run the *podman* command as follows to check its
version:

![](images/01059.jpeg){.image2}

The output reveals the version (1.9.3) of the *podman* utility and this
is what you will be using to perform the forthcoming exercises.

**Image Management**

Images are stored in private or public registries and are pulled locally
for consumption. They can be searched through one or more registries and
their metadata can be examined before download. Downloaded images can be
removed when no longer needed to conserve local storage. The same pair
of commands---*podman* and *skopeo*---is employed for these operations.

**[Exercise]{#part0035_split_001.html#id_608 .calibre10} 23-2: Search,
Examine, Download, and Remove an Image**

This exercise should be done on *server20* as *user1* with *sudo* where
required.

In this exercise, you will look for an image called *mysql* in the
*quay.io* registry, examine its details, pull it to your system, confirm
the retrieval, and finally erase it from the local storage. You will use
the *podman* and *skopeo* commands as required for these operations.

1[.]{.c19}Find the *mysql* image in the specified registry (*quay.io*)
using *podman search*. You may add the \--no-trunc option to view full
output.

![](images/01060.jpeg){.image2}

There are several images containing the searched name. You will pick
app-sre/mysql for use in this exercise.

2[.]{.c19}Inspect the image without downloading it using *skopeo
inspect*. A long output will be generated. The command uses the
docker:// mechanism to access the image.

![](images/01061.jpeg){.image2}

![](images/01062.jpeg){.image2}

The output shows older versions under RepoTags, creation timestamp for
the latest version, the Docker version that was used to build this
image, and other information. It is a good practice to analyze the
metadata of an image prior to downloading and consuming it.

3[.]{.c19}Download the image by specifying the fully qualified image
name using *podman pull*:

![](images/01063.jpeg){.image2}

4[.]{.c19}List the image to confirm the retrieval using *podman images*:

![](images/01064.jpeg){.image2}

The output indicates the FQIN of the image, version (tag), image ID,
when it was created, and size.

5[.]{.c19}Display this image's details using *podman inspect*. The
command will generate a long output. You may pipe the output to the
*less* command to view one screenful of information at a time.

![](images/01065.jpeg){.image2}

6[.]{.c19}Remove the mysql image from local storage using *podman rmi*:

![](images/01066.jpeg){.image2}

The *podman* command shows the ID of the image after deletion.

7[.]{.c19}Confirm the removal using *podman images*:

![](images/01067.jpeg){.image2}

The image is no longer available in the local storage, which confirms
the removal.

This concludes the exercise.

**[Basic]{#part0035_split_001.html#id_609 .calibre10} Container
Management**

Managing containers involves common tasks such as starting, stopping,
listing, viewing information about, and deleting them. Depending on the
use case, containers can be launched in different ways. They can have a
name assigned or be nameless, have a terminal session opened for
interaction, execute an entry point command (the command specified at
the launch time) and be auto-terminated right after, and so on. Running
containers can be stopped and restarted, or discarded if no longer
needed. The *podman* command is utilized to start containers and manage
their lifecycle. This command is also employed to list stopped and
running containers, and view their details.

**Exercise 23-3: Run, Interact with, and Remove a Named Container**

This exercise should be done on *server20* as *user1* with *sudo* where
required.

In this exercise, you will run a container based on the latest version
of a *universal base image* (ubi) for RHEL 8 available in the Red Hat
Container Registry. This image provides the base operating system layer
for the deployment of containerized applications. You will assign this
container a name and run a few native Linux commands in a terminal
window interactively. You will exit out of the container and invoke a
command inside the container from *server20*. Finally, you will
reconnect to the container, exit out, and delete it to mark the
completion of the exercise.

1[.]{.c19}Delete the directory **etc*docker* (if it exists) to avoid
permission issues:

![](images/01068.jpeg){.image2}

2[.]{.c19}Launch a container using ubi8 (RHEL 8 image). Name this
container *rhel8-base-os* and open a terminal session (-t) for
interaction (-i). Use the *podman run* command.

![](images/01069.jpeg){.image2}

The above command downloaded the latest version of the specified image
automatically even though no FQIN was provided. This is because it
searched through the registries listed in the
**etc*containers/registries.conf* file and retrieved the image from
wherever it found it first (*registry.redhat.io*). It also opened a
terminal session inside the container as the *root* user to interact
with the containerized RHEL 8 OS. The container ID is reflected as the
hostname in the container's command prompt (last line in the output).
This is an auto-generated ID.

3[.]{.c19}Run a few basic commands such as *pwd*, *ls*, *cat*, and
*date* inside the container for verification:

![](images/01070.jpeg){.image2}

4[.]{.c19}Close the terminal session when done:

![](images/01071.jpeg){.image2}

5[.]{.c19}Execute a command inside the container directly from the host
using *podman exec*:

![](images/01072.jpeg){.image2}

6[.]{.c19}Reconnect to the container using *podman attach*:

![](images/01073.jpeg){.image2}

7[.]{.c19}Close the terminal session to return to the host system's
command prompt:

![](images/01074.jpeg){.image2}

8[.]{.c19}Delete the container using *podman rm*:

![](images/01075.jpeg){.image2}

Confirm the removal with **podman ps**.

This concludes the exercise.

**[Exercise]{#part0035_split_001.html#id_610 .calibre10} 23-4: Run a
Nameless Container and Auto-Remove it After Entry Point Command
Execution**

This exercise should be done on *server20* as *user1*.

In this exercise, you will launch a container based on the latest
version of a *universal base image* (ubi) for RHEL 7 available in the
Red Hat Container Registry. This image provides the base operating
system layer to deploy containerized applications. You will enter a
Linux command at the command line for execution inside the container as
an entry point command and ensure that the container is deleted right
after that.

1[.]{.c19}Start a container using ubi7 (RHEL 7 image) and run *ls* as an
entry point command. Use *podman run* with the \--rm option to remove
the container as soon as the entry point command has finished running.

![](images/01076.jpeg){.image2}

The *ls* command was executed successfully and the container was
terminated thereafter.

2[.]{.c19}Confirm the container removal with *podman ps*:

![](images/01077.jpeg){.image2}

The container no longer exists.

This concludes the exercise.

**[Advanced]{#part0035_split_001.html#id_611 .calibre10} Container
Management**

Containers run a variety of applications and databases that may need to
communicate with applications or databases running in other containers
on the same or different host system over certain network ports. Preset
environment variables may be passed when launching containers or new
variables may be set for containerized applications to consume for
proper operation. Information stored during an application execution is
lost when a container is restarted or erased. This behavior can be
overridden by making a directory on the host available inside the
container for saving data persistently. Containers may be configured to
start and stop with the transitioning of the host system via the systemd
service. These advanced tasks are also performed with the *podman*
command. Let's take a closer look.

**[Containers]{#part0035_split_001.html#id_612 .calibre10} and Port
Mapping**

Applications running in different containers often need to exchange data
for proper operation. For instance, a containerized Apache web server
may need to talk to a MySQL database instance running in a different
container. It may also need to talk to the outside world over a port
such as 80 or 8080. To support this traffic flow, appropriate port
mappings are established between the host system and each container.

[**EXAM TIP:**]{.c56} As a normal user, you cannot map a host port below
1024 to a container port.

**Exercise 23-5: Configure Port Mapping**

This exercise should be done on *server20* as *root*.

In this exercise, you will launch a container called *rhel7-port-map* in
detached mode (as a daemon) with host port 10000 mapped to port 8000
inside the container. You will use a version of the RHEL 7 image with
Apache web server software pre-installed. This image is available in the
Red Hat Container Registry. You will list the running container and
confirm the port mapping.

1[.]{.c19}Search for an Apache web server image for RHEL 7 using *podman
search*:

![](images/01078.jpeg){.image2}

The Red Hat Container Registry has an Apache web server image called
*httpd-24-rhel7* available as evident from the output.

2[.]{.c19}Log in to *registry.redhat.io* using the Red Hat credentials
to access the image:

![](images/01079.jpeg){.image2}

3[.]{.c19}Download the latest version of the Apache image using *podman
pull*:

![](images/01080.jpeg){.image2}

4[.]{.c19}Verify the download using *podman images*:

![](images/01081.jpeg){.image2}

5[.]{.c19}Launch a container named (\--name) *rhel7-port-map* in
detached mode (-d) to run the containerized Apache web server with host
port 10000 mapped to container port 8000 (-p). Use the *podman run*
command with appropriate flags.

![](images/01082.jpeg){.image2}

6[.]{.c19}Verify that the container was launched successfully using
*podman ps*:

![](images/01083.jpeg){.image2}

The container is running (up for 50 seconds and created 51 seconds ago).
The output also indicates the port mapping.

7[.]{.c19}You can also use *podman port* to view the mapping:

![](images/01084.jpeg){.image2}

Now any inbound web traffic on host port 10000 will be redirected to the
container.

This concludes the exercise.

**[Exercise]{#part0035_split_001.html#id_613 .calibre10} 23-6: Stop,
Restart, and Remove a Container**

This exercise should be done on *server20* as *root*.

This exercise is a continuation of [Exercise
23-5](#part0035_split_000.html#page_518){.calibre5} in which a container
named *rhel7-port-map* was launched.

In this exercise, you will stop the container, restart it, stop it
again, and then erase it. You will use appropriate *podman* subcommands
and verify each transition.

1[.]{.c19}Verify the current operational state of the container
*rhel7-port-map*:

![](images/01085.jpeg){.image2}

The container has been up and running for 8 minutes. It was created 8
minutes ago.

2[.]{.c19}Stop the container and confirm using the *stop* and *ps*
subcommands (the -a option with *ps* also includes the stopped
containers in the output):

![](images/01086.jpeg){.image2}

The container is in stopped (Exited) state for the past 5 seconds. It
was created 10 minutes ago.

3[.]{.c19}Start the container and confirm with the *start* and *ps*
subcommands:

![](images/01087.jpeg){.image2}

Observe the creation and running times.

4[.]{.c19}Stop the container and remove it using the *stop* and *rm*
subcommands:

![](images/01088.jpeg){.image2}

5[.]{.c19}Confirm the removal using the -a option with *podman ps* to
also include any stopped containers:

![](images/01089.jpeg){.image2}

The container no longer exists.

This concludes the exercise.

**[Containers]{#part0035_split_001.html#id_614 .calibre10} and
Environment Variables**

Many a times, it is necessary to pass a host's pre-defined environment
variable, such as PATH, to a containerized application for consumption.
Moreover, it may also be necessary at times to set new variables to
inject debugging flags or sensitive information such as passwords,
access keys, or other secrets for use inside containers. Passing host
environment variables and setting new environment variables is done at
the time of launching a container. The *podman* command allows multiple
variables to be passed or set with the -e option.

[**EXAM TIP:**]{.c56} Use the -e option with each variable that you want
to pass or set.

Refer to [Chapter 07](#part0019_split_000.html#page_151){.calibre5} "The
Bash Shell" for more information on variables, their types, and how to
define and view them.

**[Exercise]{#part0035_split_001.html#id_615 .calibre10} 23-7: Pass and
Set Environment Variables**

This exercise should be done on *server20* as *user1*.

In this exercise, you will launch a container using the latest version
of a ubi for RHEL 8 available in the Red Hat Container Registry. You
will inject the HISTSIZE environment variable and a variable called
SECRET with the value "secret123". You will name this container
*rhel8-env-vars* and have a shell terminal opened to check the variable
settings. Finally, you will remove this container.

1[.]{.c19}Launch a container with an interactive terminal session (-it)
and inject (-e) variables HISTSIZE and SECRET as directed. Use the
specified container image.

![](images/01090.jpeg){.image2}

The container is launched with a terminal opened for interaction.

2[.]{.c19}Verify both variables using the *echo* command:

![](images/01091.jpeg){.image2}

Both variables are set and show their respective values.

3[.]{.c19}Disconnect from the container using the *exit* command, and
stop and remove it using the *stop* and *rm* subcommands:

![](images/01092.jpeg){.image2}

At this point, you may want to confirm the deletion by running **podman
ps -a**.

This concludes the exercise.

**[Containers]{#part0035_split_001.html#id_616 .calibre10} and
Persistent Storage**

Containers are normally launched for a period of time to run an
application and are then stopped or deleted when their job is finished.
Any data that is produced during runtime is lost on their restart,
failure, or termination. This data may be saved for persistence on a
host directory by attaching it to a container. The containerized
application will see the attached directory just like any other local
directory and will use it to store data if it is configured to do so.
Any data that is saved on the directory will be available even after the
container is rebooted or removed. Later, this directory can be
re-attached to other containers to give them access to the stored data
or to save their own data. The source directory on the host may itself
exist on any local or remote file system.

[**EXAM TIP:**]{.c56} Proper ownership, permissions, and SELinux file
type must be set to ensure persistent storage is accessed and allows
data writes without issues.

There are a few simple steps that should be performed to configure a
directory before it can be attached to a container. These steps include
the correct ownership, permissions, and SELinux type (container_file_t).
The special SELinux file type is applied to prevent containerized
applications (especially those running in root containers) from gaining
undesired privileged access to host files and processes, or other
running containers on the host if compromised.

**[Exercise]{#part0035_split_001.html#id_617 .calibre10} 23-8: Attach
Persistent Storage and Access Data Across Containers**

This exercise should be done on *server20* as *user1* with *sudo* where
required.

In this exercise, you will set up a directory on *server20* and attach
it to a new container. You will write some data to the directory while
in the container. You will delete the container and launch another
container with the same directory attached. You will observe that the
saved data is available in the new container and is accessible. You will
remove the container to mark the completion of this exercise.

1[.]{.c19}Create a directory called */host_data*, set full permissions
on it, and confirm:

![](images/01093.jpeg){.image2}

2[.]{.c19}Launch a root container called *rhel7-persistent-data*
(\--name) in interactive mode (-it) using the latest ubi7 image. Specify
the attachment point (*/container_data*) to be used inside the container
for the host directory (*/host_data*) with the -v (volume) option. Add
:Z as shown to ensure the SELinux type container_file_t is automatically
set on the directory and files within.

![](images/01094.jpeg){.image2}

3[.]{.c19}Confirm the presence of the directory inside the container
using the *ls* command on */container_host*:

![](images/01095.jpeg){.image2}

The host directory is available as */container_data* inside the
container with the correct SELinux type.

4[.]{.c19}Create a file called *testfile* with the *echo* command under
*/container_host*:

![](images/01096.jpeg){.image2}

5[.]{.c19}Verify the file creation and the SELinux type on it:

![](images/01097.jpeg){.image2}

The file inherits the SELinux type from the parent directory.

6[.]{.c19}Exit out of the container and check the presence of the file
in the host directory:

![](images/01098.jpeg){.image2}

The output indicates the exact same attributes on the file.

7[.]{.c19}Stop and remove the container using the *stop* and *rm*
subcommands:

![](images/01099.jpeg){.image2}

8[.]{.c19}Launch a new root container called *rhel8-persistent-data*
(\--name) in interactive mode (-it) using the latest ubi8 image from any
of the defined registries. Specify the attachment point
(*/container_data2*) to be used inside the container for the host
directory (*/host_data*) with the -v (volume) option. Add :Z as shown to
ensure the SELinux type container_file_t is automatically set on the
directory and files within.

![](images/01100.jpeg){.image2}

9[.]{.c19}Confirm the presence of the directory inside the container
using the *ls* command on */container_host2*:

![](images/01101.jpeg){.image2}

The host directory is available as */container_data2* inside this new
container with the correct SELinux type. The output also confirms the
persistence of the *testfile* data that was written in the previous
container.

10[.]{.c23}Create a file called *testfile2* with the *echo* command
under */container_data2*:

![](images/01102.jpeg){.image2}

11[.]{.c23}Exit out of the container and confirm the existence of both
files in the host directory:

![](images/01103.jpeg){.image2}

12[.]{.c23}Stop and remove the container using the *stop* and *rm*
subcommands:

![](images/01104.jpeg){.image2}

13[.]{.c23}Re-check the presence of the files in the host directory:

![](images/01105.jpeg){.image2}

Both files still exist and they can be shared with other containers if
required.

This concludes the exercise.

**[Container]{#part0035_split_001.html#id_618 .calibre10} State
Management with systemd**

So far you have seen how containers are started, stopped, and deleted by
hand using the management command. In real life environments multiple
containers run on a single host and it becomes a challenging task to
change their operational state or delete them manually. In RHEL 8, these
administrative functions can be automated via the systemd service (refer
to [Chapter 12](#part0024_split_000.html#page_271){.calibre5} "System
Initialization, Message Logging, and System Tuning" for details on
systemd).

There are several steps that need to be completed to configure container
state management via systemd. These steps vary for root and rootless
container setups and include the creation of service unit files and
their storage in appropriate directory locations
(*\~/.config/systemd/user* for rootless containers and
**etc*systemd/system* for root containers). Once setup and enabled, the
containers will start and stop automatically as a systemd service with
the host state transition or manually with the *systemctl* command.

![](images/00002.jpeg){.image} The *podman* command to start and stop
containers is no longer needed if the systemd setup is in place. You may
experience issues if you continue to use *podman* for container state
transitioning.

The start and stop behavior for rootless containers differs slightly
from that of root containers. For the rootless setup, the containers are
started when the relevant user logs in to the host and stopped when that
user logs off from all their open terminal sessions; however, this
default behavior can be altered by enabling lingering for that user with
the *loginctl* command.

![](images/00002.jpeg){.image} User lingering is a feature that, if
enabled for a particular user, spawns a user manager for that user at
system startup and keeps it running in the background to support
long-running services configured for that user. The user need not log
in.

[**EXAM TIP:**]{.c56} Make sure that you use a normal user to launch
rootless containers and the root user (or sudo) to launch root
containers.

Rootless setup does not require elevated privileges of the *root* user.

**[Exercise]{#part0035_split_001.html#id_619 .calibre10} 23-9: Configure
a Root Container as a systemd Service**

This exercise should be done on *server20* as *user1* with *sudo* where
required.

In this exercise, you will create a systemd unit configuration file for
managing the state of your root containers. You will launch a new
container and use it as a template to generate a service unit file. You
will stop and remove the launched container to avoid conflicts with new
containers that will start. You will use the *systemctl* command to
verify the automatic container start, stop, and deletion.

1[.]{.c19}Launch a new container called *root-container* (\--name) in
detached mode (-d) using the latest ubi8:

![](images/01106.jpeg){.image2}

2[.]{.c19}Confirm the new container using *podman ps*. Note the
container ID.

![](images/01107.jpeg){.image2}

3[.]{.c19}Create (*generate*) a service unit file called
*root-container.service* under **etc*systemd/system* while ensuring that
the next new container that will be launched based on this configuration
file will not require the source container (\--new) to work. The *tee*
command will show the generated file content on the screen as well as
store it in the specified file.

![](images/01108.jpeg){.image2}

The unit file has the same syntax as any other systemd service
configuration file. There are three sections---Unit, Service, and
Install. (1) The unit section provides a short description of the
service, the manual page location, and the dependencies (wants and
after). (2) The service section highlights the full commands for
starting (ExecStart) and stopping (ExecStop) containers. It also
highlights the commands that will be executed before the container start
(ExecStartPre) and after the container stop (ExecStopPost). There are a
number of options and arguments with the commands to ensure a proper
transition. The restart on-failure stipulates that systemd will try to
restart the container in the event of a failure. (3) The install section
identifies the operational target the host needs to be running in before
this container service can start. See [Chapter
12](#part0024_split_000.html#page_271){.calibre5} "System
Initialization, Message Logging, and System Tuning" for details on
systemd units and targets. Also check out the manual pages for
*systemd.unit*, *systemd.target*, and *systemd.service* if interested.

4[.]{.c19}Stop and delete the source container (*root-container*) using
the *stop* and *rm* subcommands:

![](images/01109.jpeg){.image2}

Verify the removal by running **podman ps -a**.

5[.]{.c19}Update systemd to bring the new service to its control (reboot
the system if required):

![](images/01110.jpeg){.image2}

6[.]{.c19}Enable (enable) and start (\--now) the container service using
the *systemctl* command:

![](images/01111.jpeg){.image2}

7[.]{.c19}Check the running status of the new service with the
*systemctl* command:

![](images/01112.jpeg){.image2}

8[.]{.c19}Verify the launch of a new container (compare the container ID
with that of the source root container):

![](images/01113.jpeg){.image2}

9[.]{.c19}Restart the container service using the *systemctl* command:

![](images/01114.jpeg){.image2}

10[.]{.c23}Check the status of the container again. Observe the removal
of the previous container and the launch of a new container (compare
container IDs).

![](images/01115.jpeg){.image2}

Each time the *root-container* service is restarted or *server20* is
rebooted, a new container will be launched. You can verify this by
comparing their container IDs.

This concludes the exercise.

**[Exercise]{#part0035_split_001.html#id_620 .calibre10} 23-10:
Configure a Rootless Container as a systemd Service**

This exercise should be done on *server20* as *user1* with *sudo* where
required. Log in as *conuser1* as directed.

In this exercise, you will create a systemd unit configuration file for
managing the state of your rootless containers. You will launch a new
container as *conuser1* (create this user) and use it as a template to
generate a service unit file. You will stop and remove the launched
container to avoid conflicts with new containers that will start. You
will use the *systemctl* command as *conuser1* to verify the automatic
container start, stop, and deletion.

1[.]{.c19}Create a user account called *conuser1* and assign a simple
password:

![](images/01116.jpeg){.image2}

2[.]{.c19}Open a new terminal window on *server20* and log in as
*conuser1*. Create directory *\~/.config/systemd/user* to store a
service unit file:

![](images/01117.jpeg){.image2}

3[.]{.c19}Launch a new container called *rootless-container* (\--name)
in detached mode (-d) using the latest ubi8:

![](images/01118.jpeg){.image2}

4[.]{.c19}Confirm the new container using *podman ps*. Note the
container ID.

![](images/01119.jpeg){.image2}

5[.]{.c19}Create (*generate*) a service unit file called
*rootless-container.service* under *\~/.config/systemd/user* while
ensuring that the next new container that will be launched based on this
configuration file will not require the source container (\--new) to
work:

![](images/01120.jpeg){.image2}

6[.]{.c19}Display the content of the unit file:

![](images/01121.jpeg){.image2}

See [Exercise 23-9](#part0035_split_001.html#id_619){.calibre5} earlier
for an analysis of the file content.

7[.]{.c19}Stop and delete the source container *rootless-container*
using the *stop* and *rm* subcommands:

![](images/01122.jpeg){.image2}

Verify the removal by running **podman ps -a**.

8[.]{.c19}Update systemd to bring the new service to its control (reboot
the system if required). Use the \--user option with the *systemctl*
command.

![](images/01123.jpeg){.image2}

9[.]{.c19}Enable (enable) and start (\--now) the container service using
the *systemctl* command:

![](images/01124.jpeg){.image2}

10[.]{.c23}Check the running status of the new service using the
*systemctl* command:

![](images/01125.jpeg){.image2}

11[.]{.c23}Verify the launch of a new container (compare the container
ID with that of the source rootless container):

![](images/01126.jpeg){.image2}

12[.]{.c23}Enable the container service to start and stop with host
transition using the *loginctl* command (systemd login manager) and
confirm:

![](images/01127.jpeg){.image2}

13[.]{.c23}Restart the container service using the *systemctl* command:

![](images/01128.jpeg){.image2}

14[.]{.c23}Check the status of the container again. Observe the removal
of the previous container and the launch of a new container (compare
container IDs).

![](images/01129.jpeg){.image2}

Each time the *rootless-container* service is restarted or *server20* is
rebooted, a new container will be launched. You can verify this by
comparing their container IDs.

This concludes the exercise.

**[Chapter]{#part0035_split_001.html#id_621 .calibre10} Summary**

In this last chapter, we discussed the container technology that has
gained tremendous popularity and been deployed in massive numbers over
the recent years. It offers benefits that are unmatched with any
previous virtualization technology.

We analyzed the core components of the technology: images, registries,
and containers. We interacted with images located in remote registries
and local storage. We looked at a variety of ways to launch containers,
with and without a name. We examined the benefits and use cases for
mapping network ports, injecting environment variables, and making host
storage available inside containers for persistent data storage. The
last topic expounded on controlling the operational states of root and
rootless containers via systemd and ensuring that they are auto-started
and auto-stopped with the host startup and shutdown. The chapter
presented several exercises to reinforce the learning.

**[Review]{#part0035_split_001.html#id_622 .calibre10} Questions**

1[.]{.c19}How would you ensure that data stored inside a container will
not be deleted automatically when the container is restarted?

2[.]{.c19}What is the name of the primary tool that is used for
container management?

3[.]{.c19}Which option must be included with the *systemctl* command to
manage the state of a rootless container?

4[.]{.c19}Rootless containers are less secure than root containers. True
or False?

5[.]{.c19}What is one thing that you do not get with rootless
containers? Select from: security, ability to map privileged ports,
ability to pass environment variables, or can be managed via systemd.

6[.]{.c19}What is the significance of a tag in a container image?

7[.]{.c19}Which command would you use to inspect a container image
sitting in a remote registry?

8[.]{.c19}What would you run to remove all locally stored images in one
shot?

9[.]{.c19}Containers can be run on a bare metal server or in a virtual
machine. True or False?

10[.]{.c23}Which feature would you use to allow traffic flow for your
Apache web server running inside a container?

11[.]{.c23}It is mandatory to download an image prior to running a
container based off it. True or False?

12[.]{.c23}How would you connect to a container running in the
background?

13[.]{.c23}By default, any data saved inside a container is lost when
the container is restarted. True or False?

14[.]{.c23}Can a single host directory be attached to multiple
containers at the same time?

15[.]{.c23}Which of these---isolation, security, high transition time,
less overhead---is not a benefit of the container technology?

16[.]{.c23}What is the name and location of the system-wide file that
stores a list of insecure registries?

17[.]{.c23}How would you ensure that a rootless container for a specific
user is automatically started with the host startup without the need for
the user to log in?

18[.]{.c23}What would you run to remove all stopped containers?

19[.]{.c23}What is the default tag used with container images?

20[.]{.c23}The peruser systemd service configuration file is stored
under **etc*systemd/user* directory. True or False?

**Answers to Review Questions**

1[.]{.c19}Attach a host directory to the container and use it for
storing data that requires persistence.

2[.]{.c19}The primary container management tool is called *podman*.

3[.]{.c19}The \--user option is required with the *systemctl* command to
control the state of a rootless container.

4[.]{.c19}False.

5[.]{.c19}The ability to map privileged ports is not directly supported
with rootless containers.

6[.]{.c19}A tag is typically used to identify a version of the image.

7[.]{.c19}The *skopeo* command can be used to inspect an image located
in a remote registry.

8[.]{.c19}Execute *podman rmi -a* to remove all locally stored images.

9[.]{.c19}True.

10[.]{.c23}Map a host port with a container port.

11[.]{.c23}False. The specified image is automatically downloaded when
starting a container.

12[.]{.c23}Use the *podman attach* command to connect to a running
container.

13[.]{.c23}True.

14[.]{.c23}Yes, a single host directory can be attached to multiple
containers simultaneously.

15[.]{.c23}High transition time is not a container benefit.

16[.]{.c23}The file that stores insecure registry list is called
*registries.conf* and it lives in the **etc*containers* directory.

17[.]{.c23}Use the *loginctl* command as that user and enable lingering.

18[.]{.c23}Run *podman rm -a* to remove all stopped containers.

19[.]{.c23}The default tag is 'latest' that identifies the latest
version of an image.

20[.]{.c23}False. The peruser systemd unit configuration file is stored
under the user's home directory at *\~/.config/systemd/user*.

**[DIY]{#part0035_split_001.html#id_623 .calibre10} Challenge Labs**

The following labs are useful to strengthen most of the concepts and
topics learned in this chapter. It is expected that you perform these
labs without any additional help. A step-by-step guide is not provided,
as the implementation of these labs requires the knowledge that has been
presented in this chapter. Use defaults or your own thinking for missing
information.

**[Lab]{#part0035_split_001.html#id_624 .calibre10} 23-1: Prepare to
Launch Containers**

Create a new user account called *conadm* on *server10* and give them
full *sudo* rights. As *conadm*, register *server10* with RHSM using
your Red Hat credentials. Install the module *container-tools* from RHSM
and ensure that podman version is 1.9. (Hint: Chapters 05, 06, and 10).

**[Lab]{#part0035_split_001.html#id_625 .calibre10} 23-2: Launch a Named
Root Container with Port Mapping**

As *conadm* with *sudo* (where required) on *server10*, inspect the
latest version of RHEL 7 image and then download it to your computer.
Launch a container called *root-cont-port* in attached terminal mode
(-it) with host port 80 mapped to container port 8080. Run a few basic
Linux commands such as *ls*, *pwd*, *df*, *cat *etc*redhat-release*, and
*os-release* while in the container. Check to confirm the port mapping
from *server10*. (Hint: use the *skopeo* and *podman* commands).
**Note:** Do not remove the container yet.

**Lab 23-3: Launch a Nameless Rootless Container with Two Variables**

As *conadm* on *server10*, launch a container using the latest version
of ubi8 image in detached mode (-d) with two environment variables
VAR1=lab1 and VAR2=lab2 defined. Use the *exec* subcommand to check the
settings for the variables directly from *server10*. Delete the
container and the image when done. (Hint: use the *podman* command).

**[Lab]{#part0035_split_001.html#id_626 .calibre10} 23-4: Launch a Named
Rootless Container with Persistent Storage**

As *conadm* with *sudo* (where required) on *server10*, create a
directory called */host_perm1* with full permissions, and a file called
*str1* in it. Launch a container called *rootless-cont-str* in attached
terminal mode (-it) with the created directory mapped to */cont_perm1*
inside the container. While in the container, check access to the
directory and the presence of the file. Create a sub-directory and a
file under */cont_perm1* and exit out of the container shell. List
*/host_perm1* on *server10* to verify the sub-directory and the file.
Stop and delete the container. Remove */host_perm1*. (Hint: use the
*podman* command).

**[Lab]{#part0035_split_001.html#id_627 .calibre10} 23-5: Launch a Named
Rootless Container with Port Mapping, Environment Variables, and
Persistent Storage**

As *conadm* with *sudo* (where required) on *server10*, launch a named
rootless container called *rootless-cont-adv* in attached mode (-it)
with two variables (HISTSIZE=100 and MYNAME=RedHat), host port 9000
mapped to container port 8080, and */host_perm2* mounted at
*/cont_perm2*. Check and confirm the settings while inside the
container. Exit out of the container. **Note:** Do not remove the
container yet. (Hint: use the *podman* command).

**[Lab]{#part0035_split_001.html#id_628 .calibre10} 23-6: Control
Rootless Container States via systemd**

As *conadm* on *server10*, use the *rootless-cont-adv* container
launched in Lab 23-5 as a template and generate a systemd service
configuration file and store the file in the appropriate directory. Stop
and remove the source container *rootless-cont-adv*. Add the support for
the new service to systemd and enable the new service to auto-start at
system reboots. Perform the required setup to ensure the container is
launched without the need for the *conadm* user to log in. Reboot
*server10* and confirm a successful start of the container service and
the container. (Hint: use the *podman*, *systemctl*, and *loginctl*
commands).

**[Lab]{#part0035_split_001.html#id_629 .calibre10} 23-7: Control Root
Container States via systemd**

As *conadm* with *sudo* where required on *server10*, use the
*root-cont-port* container launched in Lab 23-2 as a template and
generate a systemd service configuration file and store the file in the
appropriate directory. Stop and remove the source container
*root-cont-port*. Add the support for the new service to systemd and
enable the service to auto-start at system reboots. Reboot *server10*
and confirm a successful start of the container service and the
container. (Hint: use the *podman* and *systemctl* commands).

[]{#part0036.html}

**[]{#part0036.html#page_535 .calibre2}Appendix A: Sample RHCSA Exam 1**

  -------------------- ----------------------------------------------------------------
  **Time Duration:**   3 hours
  **Passing Score:**   70% (210 out of 300)
  **Instructions:**    The RHCSA exam, EX200, is offered electronically on a physical
  -------------------- ----------------------------------------------------------------

computer running RHEL 8. The computer has two virtual machines with RHEL
8 running in each one of them. The exam presents two sets of tasks that
are to be completed within the stipulated time in the identified virtual
machine. Firewall and SELinux need to be considered. All settings
performed in the virtual machines must survive reboots, or you will lose
marks. Access to the Internet, printed and electronic material, and
electronic devices is prohibited during the exam.

**[Setup for Sample Exam 1:]{.underline1}**

Build a virtual machine with RHEL 8 Server with GUI (Exercises 1-1 and
1-2). Use a 10GB disk for the OS with default partitioning. Add 2x300MB
disks and a network interface. Do not configure the network interface or
create a normal user account during installation.

**[Instructions:]{.underline1}**

**01:** The following tasks are in addition to the exercises and labs
presented in the book. No solutions are furnished, but hints to
applicable exercises, chapters, or topics are provided in parentheses
for reference.

**02:** Please do not browse the Internet or seek help from other
sources. However, you can refer to the manual pages, and the
documentation under the *usr*share/doc directory. This rule does not
apply to the kernel download task if included.

**03:** The exam tasks should be completed in a terminal window using
only the command line interface (no GUI).

**04:** You can reboot the VM whenever you want during this exam, but
retest the configuration after each reboot for verification.

**05:** Use your knowledge and judgement for any missing configuration
in task description.

**[Tasks:]{.underline1}**

**Task 01:** Assuming the root user password is lost, and your system is
running in multi-user target with no current root session open. Reboot
the system into an appropriate target level, and reset the root user
password to root1234. ([Exercise
11-2](#part0023_split_001.html#id_317){.calibre2}). After completing
this task, log in as the root user and perform the remaining tasks
presented below.

**Task 02:** Using a manual method (create/modify files by hand),
configure a network connection on the primary network device with IP
address 192.168.0.241/24, gateway 192.168.0.1, and nameserver
192.168.0.1. Use different IP assignments based on your lab setup.
([Exercise 16-3](#part0028_split_001.html#id_452){.calibre5}).

**[]{#part0036.html#page_536 .calibre5}Task 03:** Using a manual method
(modify file by hand), set the system hostname to
[rhcsa1.example.com](http://rhcsa1.example.com){.calibre5} and alias
rhcsa1. Make sure that the new hostname is reflected in the command
prompt. ([Exercises 16-1](#part0028_split_001.html#id_437){.calibre5}
and [16-5](#part0028_split_001.html#id_458){.calibre5}).

**Task 04:** Set the default boot target to multi-user. ([Chapter
12](#part0024_split_000.html#page_271){.calibre5}, topic: Managing
Target Units).

**Task 05:** Set SELinux to permissive mode. ([Chapter
21](#part0033_split_000.html#page_461){.calibre5}, topic: Viewing and
Controlling SELinux Operational State).

**Task 06:** Perform a case-insensitive search for all lines in the
*usr*share/dict/linux.words file that begin with the pattern
"essential". Redirect the output to *var*tmp/pattern.txt file. Make sure
that empty lines are omitted. ([Chapter
07](#part0019_split_000.html#page_151){.calibre5}, topic: Regular
Expressions).

**Task 07:** Change the primary command prompt for the root user to
display the hostname, username, and current working directory
information in that order. Update the peruser initialization file for
permanence. ([Exercise
7-1](#part0019_split_001.html#id_201){.calibre5}).

**Task 08:** Create user accounts called user10, user20, and user30. Set
their passwords to Temp1234. Make user10 and user30 accounts to expire
on December 31, 2021. ([Exercises
5-1](#part0017_split_001.html#id_163){.calibre5}, and
[6-1](#part0018_split_001.html#id_177){.calibre5} or
[6-2](#part0018_split_000.html#page_139){.calibre5}).

**Task 09:** Create a group called group10 and add user20 and user30 as
secondary members. ([Exercise
6-4](#part0018_split_001.html#id_182){.calibre5}).

**Task 10:** Create a user account called user40 with UID 2929. Set the
password to user1234. ([Exercise
5-2](#part0017_split_001.html#id_164){.calibre5}).

**Task 11:** Create a directory called dir1 under *var*tmp with
ownership and owning group set to root. Configure default ACLs on the
directory and give user10 read, write, and execute permissions.
([Exercise 4-8](#part0016_split_000.html#page_110){.calibre5}).

**Task 12:** Attach the RHEL 8 ISO image to the VM and mount it
persistently to *mnt*cdrom. Define access to both repositories and
confirm. ([Exercise 10-1](#part0022_split_001.html#id_280){.calibre5}).

**Task 13:** Create a logical volume called lvol1 of size 280MB in
vgtest volume group. Mount the ext4 file system persistently to
*mnt*mnt1. ([Exercises
14-1](#part0026_split_001.html#id_384){.calibre5},
[14-2](#part0026_split_001.html#id_385){.calibre5}, and
[15-3](#part0027_split_001.html#id_419){.calibre5}).

**Task 14:** Change group membership on *mnt*mnt1 to group10. Set
read/write/execute permissions on *mnt*mnt1 for group members, and
revoke all permissions for public. ([Exercises
6-4](#part0018_split_001.html#id_182){.calibre5},
[6-6](#part0018_split_001.html#id_188){.calibre5}, and [either
4-1](#part0016_split_001.html#id_121){.calibre5} or
[4-2](#part0016_split_000.html#page_93){.calibre5}).

**Task 15:** Create a logical volume called lvswap of size 280MB in
vgtest volume group. Initialize the logical volume for swap use. Use the
UUID and place an entry for persistence. ([Exercise
15-6](#part0027_split_001.html#id_425){.calibre5}).

**Task 16:** Use the combination of tar and bzip2 commands to create a
compressed archive of the *usr*lib directory. Store the archive under
*var*tmp as usr.tar.bz2. ([Exercise
3-1](#part0015_split_001.html#id_84){.calibre5}).

**Task 17:** Create a directory hierarchy *dir1*dir2/dir3/dir4 and apply
SELinux contexts of /etc on it recursively. ([Chapter
03](#part0015_split_000.html#page_61){.calibre5}, topic: Creating Files
and Directories, and [Exercise
21-2](#part0033_split_001.html#id_553){.calibre5}).

**Task 18:** Enable access to the atd service for user20 and deny for
user30. ([Chapter 08](#part0020_split_000.html#page_175){.calibre5},
topic: Controlling User Access).

**Task 19:** Add a custom message "This is RHCSA sample exam on \$(date)
by \$LOGNAME" to the *var*log/messages file as the root user. Use
regular expression to confirm the message entry to the log file.
([Chapter 07](#part0019_split_000.html#page_151){.calibre5}, topic:
Regular Expressions, and [Chapter
12](#part0024_split_000.html#page_271){.calibre5}, topic: Logging Custom
Messages).

**[]{#part0036.html#page_537 .calibre5}Task 20:** Allow user20 to use
sudo without being prompted for their password. ([Chapter
06](#part0018_split_000.html#page_135){.calibre5}, topic: Doing as
Superuser (or Doing as Substitute User)).

**Task 21:** Write a bash shell script to create three user
accounts---user555, user666, and user777---with no login shell and
passwords matching their usernames. The script should also extract the
three usernames from the *etc*passwd file and redirect them into
*var*tmp/newusers. ([Chapter
22](#part0034_split_000.html#page_481){.calibre5}: Script12 and [Chapter
07](#part0019_split_000.html#page_151){.calibre5}, topics: Regular
Expressions, and Input, Output, and Error Redirections).

**Task 22:** Launch a simple container as user20 using the latest
version of ubi7 image. Configure the container to auto-start at system
reboots without the need for user20 to log in. ([Exercise
23-10](#part0035_split_001.html#id_620){.calibre5}).

**Task 23:** Launch another container as user20 using the latest version
of ubi8 image with two environment variables SHELL and HOSTNAME.
Configure the container to auto-start via systemd without the need for
user20 to log in. Connect to the container and verify variable settings.
([Exercise 23-7](#part0035_split_001.html#id_615){.calibre5} and
[23-10](#part0035_split_001.html#id_620){.calibre5}).

**Reboot the system and validate the configuration.**

[]{#part0037.html}

**[]{#part0037.html#page_539 .calibre2}Appendix B: Sample RHCSA Exam 2**

  -------------------- ----------------------------------------------------------------
  **Time Duration:**   3 hours
  **Passing Score:**   70% (210 out of 300)
  **Instructions:**    The RHCSA exam, EX200, is offered electronically on a physical
  -------------------- ----------------------------------------------------------------

computer running RHEL 8. The computer has two virtual machines with RHEL
8 running in each one of them. The exam presents two sets of tasks that
are to be completed within the stipulated time in the identified virtual
machine. Firewall and SELinux need to be considered. All settings
performed in the virtual machines must survive reboots, or you will lose
marks. Access to the Internet, printed and electronic material, and
electronic devices is prohibited during the exam.

**[Setup for Sample Exam 2:]{.underline1}**

Build a virtual machine with RHEL 8 Server with GUI (Exercises 1-1 and
1-2). Use a 10GB disk for the OS with default partitioning. Add 1x400MB
disk and a network interface. Do not configure the network interface or
create a normal user account during installation.

**[Instructions:]{.underline1}**

**01:** The following tasks are in addition to the exercises and labs
presented in the book. No solutions are furnished, but hints to
applicable exercises, chapters, or topics are provided in parentheses
for reference.

**02:** Please do not browse the Internet or seek help from other
sources. However, you can refer to the manual pages, and the
documentation under the *usr*share/doc directory. This rule does not
apply to the kernel download task if included.

**03:** The exam tasks should be completed in a terminal window using
only the command line interface (no GUI).

**04:** You can reboot the VM whenever you want during this exam, but
retest the configuration after each reboot for verification.

**05:** Use your knowledge and judgement for any missing configuration
in task description.

**[Tasks:]{.underline1}**

**Task 01:** Using the nmcli command, configure a network connection on
the primary network device with IP address 192.168.0.242/24, gateway
192.168.0.1, and nameserver 192.168.0.1. Use different IP assignments
based on your lab environment. ([Exercise
16-4](#part0028_split_001.html#id_455){.calibre2}).

**Task 02:** Using the hostnamectl command, set the system hostname to
[rhcsa2.example.com](http://rhcsa2.example.com){.calibre5} and alias
rhcsa2. Make sure that the new hostname is reflected in the command
prompt. ([Exercises 16-1](#part0028_split_001.html#id_437){.calibre5}
and [16-5](#part0028_split_001.html#id_458){.calibre5}).

**Task 03:** Create a user account called user70 with UID 7000 and
comments "I am user70". Set the maximum allowable inactivity for this
user to 30 days. ([Exercises
5-2](#part0017_split_001.html#id_164){.calibre5}, and
[6-1](#part0018_split_001.html#id_177){.calibre5} or
[6-2](#part0018_split_000.html#page_139){.calibre5}).

**Task 04:** Create a user account called user50 with a non-interactive
shell. ([Exercise 5-4](#part0017_split_001.html#id_166){.calibre5}).

**[]{#part0037.html#page_540 .calibre5}Task 05:** Create a file called
testfile1 under *var*tmp with ownership and owning group set to root.
Configure access ACLs on the file and give user10 read and write access.
Test access by logging in as user10 and editing the file. ([Chapter
03](#part0015_split_000.html#page_61){.calibre5}, topic: Creating Files
and Directories, and [Exercise
4-7](#part0016_split_001.html#id_140){.calibre5}).

**Task 06:** Attach the RHEL 8 ISO image to the VM and mount it
persistently to *mnt*dvdrom. Define access to both repositories and
confirm. ([Exercise 10-1](#part0022_split_001.html#id_280){.calibre5}).

**Task 07:** Create a logical volume called lv1 of size equal to 10 LEs
in vg1 volume group (create vg1 with PE size 8MB in a partition on the
400MB disk). Initialize the logical volume with XFS type and mount it on
*mnt*lvfs1. Create a file called lv1file1 in the mount point. Set the
file system to automatically mount at each system reboot. ([Exercises
14-1](#part0026_split_001.html#id_384){.calibre5},
[14-2](#part0026_split_001.html#id_385){.calibre5}, and
[15-3](#part0027_split_001.html#id_419){.calibre5}).

**Task 08:** Add a group called group20 and change group membership on
*mnt*lvfs1 to group20. Set read/write/execute permissions on *mnt*lvfs1
for the owner, group members, and others. ([Exercises
6-4](#part0018_split_001.html#id_182){.calibre5},
[6-6](#part0018_split_001.html#id_188){.calibre5}, and [either
4-1](#part0016_split_001.html#id_121){.calibre5} or
[4-2](#part0016_split_000.html#page_93){.calibre5}).

**Task 09:** Extend the file system in the logical volume lv1 by 64MB
without unmounting it and without losing any data. Confirm the new size
for the logical volume and the file system. ([Exercise
15-4](#part0027_split_001.html#id_420){.calibre5}).

**Task 10:** Create a swap partition of size 85MB on the 400MB disk. Use
its UUID and ensure it is activated after every system reboot.
([Exercise 15-6](#part0027_split_001.html#id_425){.calibre5}).

**Task 11:** Create a disk partition of size 100MB on the 400MB disk and
format it with Ext4 file system structures. Assign label stdlabel to the
file system. Mount the file system on *mnt*stdfs1 persistently using the
label. Create file stdfile1 in the mount point. ([Exercise
13-2](#part0025_split_001.html#id_363){.calibre5} or
[13-4](#part0025_split_001.html#id_366){.calibre5}, [Chapter
15](#part0027_split_000.html#page_345){.calibre5}, topic: Labeling a
File System, and [Exercise
15-1](#part0027_split_000.html#page_356){.calibre5}).

**Task 12:** Use the tar and gzip command combination to create a
compressed archive of the /etc directory. Store the archive under
*var*tmp using a filename of your choice. ([Exercise
3-1](#part0015_split_001.html#id_84){.calibre5}).

**Task 13:** Create a directory *direct01 and apply SELinux contexts
for* root to it. ([Exercise
21-2](#part0033_split_001.html#id_553){.calibre5}).

**Task 14:** Set up a cron job for user70 to search for files by the
name "core" in the /var directory and copy them to the directory
*var*tmp/coredir1. This job should run every Monday at 1:20 a.m.
([Chapter 04](#part0016_split_000.html#page_89){.calibre5}, topics:
Using the find Command, and Using find with -exec and -ok Flags, and
[Exercise 8-4](#part0020_split_001.html#id_237){.calibre5}).

**Task 15:** Search for all files in the entire directory structure that
have been modified in the past 30 days and save the file listing in the
*var*tmp/modfiles.txt file. ([Chapter
04](#part0016_split_000.html#page_89){.calibre5}, topics: Using the find
Command and Using find with -exec and -ok Flags).

**Task 16:** Modify the bootloader program and set the default autoboot
timer value to 2 seconds. ([Exercise
11-1](#part0023_split_001.html#id_315){.calibre5}).

**Task 17:** Determine the recommended tuning profile for the system and
apply it. ([Exercise 12-2](#part0024_split_001.html#id_348){.calibre5}).

**Task 18:** Configure Chrony to synchronize system time with the
hardware clock. Remove all other NTP sources. ([Exercise
18-1](#part0030_split_001.html#id_491){.calibre5}).

**Task 19:** Install package group called "Development Tools" and
capture its information in *var*tmp/systemtools.out file. ([Chapter
03](#part0015_split_000.html#page_61){.calibre5}, topic: Regular
Expressions, and [Exercise
10-3](#part0022_split_001.html#id_290){.calibre5}).

**Task 20:** Lock user account user70. Use regular expressions to
capture the line that shows the lock and store the output in file
*var*tmp/user70.lock. ([Chapter
03](#part0015_split_000.html#page_61){.calibre5}, topic: Regular
Expressions, and [Exercise
6-3](#part0018_split_001.html#id_180){.calibre5}).

**[]{#part0037.html#page_541 .calibre5}Task 21:** Write a bash shell
script so that it prints RHCSA when RHCE is passed as an argument, and
vice versa. If no argument is provided, the script should print a usage
message and quit with exit value 5. ([Chapter
22](#part0034_split_000.html#page_481){.calibre5}: Script10).

**Task 22:** Launch a root container and configure it to auto-start via
systemd. ([Exercise 23-9](#part0035_split_001.html#id_619){.calibre5}).

**Task 23:** Launch a container as user80 with *data01 mapped to* data01
using the latest version of the ubi8 image. Configure a systemd service
to auto-start the container on system reboots without the need for
user80 to log in. Create files under the shared mount point and validate
data persistence. ([Exercise
23-7](#part0035_split_001.html#id_615){.calibre5} and
[23-10](#part0035_split_001.html#id_620){.calibre5}).

**Reboot the system and validate the configuration.**

[]{#part0038.html}

**[]{#part0038.html#page_543 .calibre2}Appendix C: Sample RHCSA Exam 3**

  -------------------- ----------------------------------------------------------------
  **Time Duration:**   3 hours
  **Passing Score:**   70% (210 out of 300)
  **Instructions:**    The RHCSA exam, EX200, is offered electronically on a physical
  -------------------- ----------------------------------------------------------------

computer running RHEL 8. The computer has two virtual machines with RHEL
8 running in each one of them. The exam presents two sets of tasks that
are to be completed within the stipulated time in the identified virtual
machine. Firewall and SELinux need to be considered. All settings
performed in the virtual machines must survive reboots, or you will lose
marks. Access to the Internet, printed and electronic material, and
electronic devices is prohibited during the exam.

**[Setup for Sample Exam 3:]{.underline1}**

Build two virtual machines with RHEL 8 Server with GUI (Exercises 1-1
and 1-2). Use a 10GB disk for the OS with default partitioning. Add
1x4GB disk to VM1, 2x1GB disks to VM2, and a network interface to both
virtual machines. Do not configure the network interfaces or create a
normal user account during installation.

**[Instructions:]{.underline1}**

**01:** The following tasks are in addition to the exercises and labs
presented in the book. No solutions are furnished, but hints to
applicable exercises, chapters, or topics are provided in parentheses
for reference.

**02:** Please do not browse the Internet or seek help from other
sources. However, you can refer to the manual pages, and the
documentation under the *usr*share/doc directory. This rule does not
apply to the kernel download task if included.

**03:** The exam tasks should be completed in a terminal window using
only the command line interface (no GUI).

**04:** You can reboot the VM whenever you want during this exam, but
retest the configuration after each reboot for verification.

**05:** Use your knowledge and judgement for any missing configuration
in task description.

**[Tasks:]{.underline1}**

**Task 01:** On VM1, set the system hostname to
[rhcsa3.example.com](http://rhcsa3.example.com){.calibre2} and alias
rhcsa3 using the hostnamectl command. Make sure that the new hostname is
reflected in the command prompt. ([Exercises
16-1](#part0028_split_001.html#id_437){.calibre2} and
[16-5](#part0028_split_001.html#id_458){.calibre2}).

**Task 02:** On rhcsa3, configure a network connection on the primary
network device with IP address 192.168.0.243/24, gateway 192.168.0.1,
and nameserver 192.168.0.1 using the nmcli command (use different IP
assignments based on your lab environment). ([Exercise
16-4](#part0028_split_001.html#id_455){.calibre5}).

**[]{#part0038.html#page_544 .calibre5}Task 03:** On VM2, set the system
hostname to [rhcsa4.example.com](http://rhcsa4.example.com){.calibre5}
and alias rhcsa4 using a manual method (modify file by hand). Make sure
that the new hostname is reflected in the command prompt. ([Exercises
16-1](#part0028_split_001.html#id_437){.calibre5} and
[16-5](#part0028_split_001.html#id_458){.calibre5}).

**Task 04:** On rhcsa4, configure a network connection on the primary
network device with IP address 192.168.0.244/24, gateway 192.168.0.1,
and nameserver 192.168.0.1 using a manual method (create/modify file by
hand). Use different IP assignments based on your lab environment.
([Exercise 16-3](#part0028_split_001.html#id_452){.calibre5}).

**Task 05:** Run "ping -c2 rhcsa4" on rhcsa3. Run "ping -c2 rhcsa3" on
rhcsa4. You should see 0% loss in both outputs. ([Exercise
16-5](#part0028_split_001.html#id_458){.calibre5}).

**Task 06:** On rhcsa3 and rhcsa4, attach the RHEL 8 ISO image to the VM
and mount it persistently to *mnt*sr0. Define access to both
repositories and confirm. ([Exercise
10-1](#part0022_split_001.html#id_280){.calibre5}).

**Task 07:** On rhcsa3, add HTTP port 8300/tcp to the SELinux policy
database persistently. ([Exercise
21-3](#part0033_split_000.html#page_474){.calibre5}).

**Task 08:** On rhcsa3, create VDO volume called vdo1 on the 4GB disk
with logical size 16GB and mounted with Ext4 structures on *mnt*vdo1.
([Exercises 13-6](#part0025_split_001.html#id_370){.calibre5} and
[13-7](#part0025_split_000.html#page_316){.calibre5}).

**Task 09:** Configure NFS service on rhcsa3 and share *rh_share3 with
rhcsa4. Configure AutoFS direct map on rhcsa4 to mount* rh_share3 on
*mnt*rh_share4. User user80 (create on both systems) should be able to
create files under the share on the NFS server as well as under the
mount point on the NFS client. ([Exercises
5-1](#part0017_split_001.html#id_163){.calibre5},
[17-1](#part0029_split_001.html#id_469){.calibre5}, and
[17-3](#part0029_split_001.html#id_476){.calibre5}).

**Task 10:** Configure NFS service on rhcsa4 and share the home
directory for user60 (create user60 on both systems) with rhcsa3.
Configure AutoFS indirect map on rhcsa3 to automatically mount the home
directory under /nfsdir when user60 logs on to rhcsa3. ([Exercises
5-1](#part0017_split_001.html#id_163){.calibre5},
[17-1](#part0029_split_001.html#id_469){.calibre5},
[17-4,](#part0029_split_001.html#id_477){.calibre5} and
[17-5](#part0029_split_001.html#id_479){.calibre5}).

**Task 11:** On rhcsa4, create Stratis pool pool1 and volume str1 on a
1GB disk and mount it to *mnt*str1. ([Exercise
15-5](#part0027_split_000.html#page_365){.calibre5}).

**Task 12:** On rhcsa4, expand Stratis pool pool1 using the other 1GB
disk. Confirm that *mnt*str1 sees the storage expansion. ([Exercise
15-5](#part0027_split_000.html#page_365){.calibre5}).

**Task 13:** On rhcsa3, create a group called group30 with GID 3000, and
add user60 and user80 to this group. Create a directory called *sdata,
enable setgid bit on it, and add write permission bit for group members.
Set ownership and owning group to root and group30. Create a file called
file1 under* sdata as user60 and modify the file as user80 successfully.
([Exercises 4-5](#part0016_split_001.html#id_130){.calibre5},
[6-4](#part0018_split_001.html#id_182){.calibre5}, and
[6-6](#part0018_split_001.html#id_188){.calibre5}).

**Task 14:** On rhcsa3, create directory *var*dir1 with full permissions
for everyone. Disallow non-owners to remove files. Test by creating file
*var*dir1/stkfile1 as user60 and removing it as user80. ([Exercise
4-6](#part0016_split_001.html#id_132){.calibre5}).

**Task 15:** On rhcsa3, search for all manual pages for the description
containing the keyword "password" and redirect the output to file
*var*tmp/man.out. ([Chapter
02](#part0014_split_000.html#page_35){.calibre5}, topic Searching by
Keyword, and [Chapter 07](#part0019_split_000.html#page_151){.calibre5},
topic: Input, Output, and Error Redirections).

**Task 16:** On rhcsa3, create file lnfile1 under *var*tmp and create
one hard link *var*tmp/lnfile2 and one soft link *boot*file1. Edit
lnfile1 using one link at a time and confirm. (Exercises 3-2 and 3-3).

**Task 17:** On rhcsa3, install module postgresql version 9.6 (select a
different non-default version if 9.6 is not available). ([Exercise
10-5](#part0022_split_001.html#id_297){.calibre5}).

**[]{#part0038.html#page_545 .calibre5}Task 18:** On rhcsa3, add the
http service to the "external" firewalld zone persistently. ([Exercise
201](#part0032_split_001.html#id_532){.calibre5}).

**Task 19:** On rhcsa3, set SELinux type shadow_t on a new file
testfile1 in /usr and ensure that the context is not affected by a
SELinux relabeling. ([Exercises
21-1](#part0033_split_001.html#id_552){.calibre5} and
[21-2](#part0033_split_001.html#id_553){.calibre5}).

**Task 20:** Configure password-less ssh access for user60 from rhcsa3
to rhcsa4. ([Exercise
19-2](#part0031_split_001.html#id_513){.calibre5}).

**Task 21:** Write a bash shell script that checks for the existence of
files (not directories) under the *usr*bin directory that begin with the
letters "ac" and display their statistics (the stat command). ([Chapter
22](#part0034_split_000.html#page_481){.calibre5}: [Table
22-1](#part0034_split_001.html#id_760){.calibre5} and Script07).

**Task 22:** On rhcsa3, launch a named container as user60 with host
port 10000 mapped to container port 80. Employ the latest version of the
ubi7 image. Configure a systemd service to auto-start the container
without the need for user60 to log in. Validate port mapping using an
appropriate podman subcommand. ([Exercises
23-5](#part0035_split_000.html#page_518){.calibre5} and
[23-10](#part0035_split_001.html#id_620){.calibre5}).

**Task 23:** On rhcsa3, launch another named container as user60 with
*host_data01 mapped to* container_data01, one variable ENVIRON=Exam, and
host port 1050 mapped to container port 1050. Use the latest version of
the ubi8 image. Configure a separate systemd service to auto-start the
container without the need for user60 to log in. Create a file under the
shared directory and validate data persistence. Verify port mapping and
variable settings using appropriate podman subcommands. ([Exercises
23-5](#part0035_split_000.html#page_518){.calibre5},
[23-7](#part0035_split_001.html#id_615){.calibre5},
[23-8](#part0035_split_001.html#id_617){.calibre5}, and
[23-10](#part0035_split_001.html#id_620){.calibre5}).

**Reboot the system and validate the configuration.**

[]{#part0039.html}

**[]{#part0039.html#page_547 .calibre2}Appendix D: Sample RHCSA Exam 4**

  -------------------- ----------------------------------------------------------------
  **Time Duration:**   3 hours
  **Passing Score:**   70% (210 out of 300)
  **Instructions:**    The RHCSA exam, EX200, is offered electronically on a physical
  -------------------- ----------------------------------------------------------------

computer running RHEL 8. The computer has two virtual machines with RHEL
8 running in each one of them. The exam presents two sets of tasks that
are to be completed within the stipulated time in the identified virtual
machine. Firewall and SELinux need to be considered. All settings
performed in the virtual machines must survive reboots, or you will lose
marks. Access to the Internet, printed and electronic material, and
electronic devices is prohibited during the exam.

**[Setup for Sample Exam 4:]{.underline1}**

Build two virtual machines with RHEL 8 Server with GUI (Exercises 1-1
and 1-2). Use a 10GB disk for the OS with default partitioning. Add
1x4GB disk to VM2 and a network interface to both virtual machines. Do
not configure the network interfaces or create a normal user account
during installation.

**[Instructions:]{.underline1}**

**01:** The following tasks are in addition to the exercises and labs
presented in the book. No solutions are furnished, but hints to
applicable exercises, chapters, or topics are provided in parentheses
for reference.

**02:** Please do not browse the Internet or seek help from other
sources. However, you can refer to the manual pages, and the
documentation under the *usr*share/doc directory. This rule does not
apply to the kernel download task if included.

**03:** The exam tasks should be completed in a terminal window using
only the command line interface (no GUI).

**04:** You can reboot the VM whenever you want during this exam, but
retest the configuration after each reboot for verification.

**05:** Use your knowledge and judgement for any missing configuration
in task description.

**[Tasks:]{.underline1}**

**Task 01:** On VM1, set the system hostname to
[rhcsa5.example.com](http://rhcsa5.example.com){.calibre2} and alias
rhcsa5 using the hostnamectl command. Make sure that the new hostname is
reflected in the command prompt. ([Exercises
16-1](#part0028_split_001.html#id_437){.calibre2} and
[16-5](#part0028_split_001.html#id_458){.calibre2}).

**Task 02:** On rhcsa5, configure a network connection on the primary
network device with IP address 192.168.0.245/24, gateway 192.168.0.1,
and nameserver 192.168.0.1 using the nmcli command. Use different IP
assignments based on your lab environment. ([Exercise
16-4](#part0028_split_001.html#id_455){.calibre5}).

**[]{#part0039.html#page_548 .calibre5}Task 03:** On VM2, set the system
hostname to [rhcsa6.example.com](http://rhcsa6.example.com){.calibre5}
and alias rhcsa6 using a manual method (modify file by hand). Make sure
that the new hostname is reflected in the command prompt. ([Exercises
16-1](#part0028_split_001.html#id_437){.calibre5} and
[16-5](#part0028_split_001.html#id_458){.calibre5}).

**Task 04:** On rhcsa6, configure a network connection on the primary
network device with IP address 192.168.0.246/24, gateway 192.168.0.1,
and nameserver 192.168.0.1 using a manual method (create/modify files by
hand). Use different IP assignments based on your lab environment.
([Exercise 16-3](#part0028_split_001.html#id_452){.calibre5}).

**Task 05:** Run "ping -c2 rhcsa6" on rhcsa5. Run "ping -c2 rhcsa5" on
rhcsa6. You should see 0% loss in both outputs. ([Exercise
16-5](#part0028_split_001.html#id_458){.calibre5}).

**Task 06:** On rhcsa5 and rhcsa6, attach the RHEL 8 ISO image to the VM
and mount it persistently to *mnt*sr0. Define access to both
repositories and confirm. ([Exercise
10-1](#part0022_split_001.html#id_280){.calibre5}).

**Task 07:** Export *share5 on rhcsa5 and mount it to* share6
persistently on rhcsa6. ([Exercises
17-1](#part0029_split_001.html#id_469){.calibre5} and
[17-2](#part0029_split_001.html#id_470){.calibre5}).

**Task 08:** Use NFS to export home directories for all users (u1, u2,
and u3) on rhcsa6 so that their home directories become available under
/home1 when they log on to rhcsa5. Create u1, u2, and u3. ([Exercises
17-1](#part0029_split_001.html#id_469){.calibre5} and
[17-5](#part0029_split_001.html#id_479){.calibre5}).

**Task 09:** On rhcsa5, add HTTP port 8400/udp to the public zone
persistently. ([Exercise
21-3](#part0033_split_000.html#page_474){.calibre5}).

**Task 10:** Configure password-less ssh access for u1 from rhcsa5 to
rhcsa6. Copy the directory *etc*sysconfig from rhcsa5 to rhcsa6 under
*var*tmp/remote securely. ([Exercise
19-2](#part0031_split_001.html#id_513){.calibre5}, and [Chapter
19](#part0031_split_000.html#page_433){.calibre5}, topic: Copying Files
Remotely Using scp).

**Task 11:** On rhcsa6, create VDO volume vdo2 on the 4GB disk with
logical size 16GB and mounted persistently with XFS structures on
*mnt*vdo2. ([Exercises 13-6](#part0025_split_001.html#id_370){.calibre5}
and [13-7](#part0025_split_000.html#page_316){.calibre5}).

**Task 12:** On rhcsa6, install module "container-tools" stream rhel8.
([Exercise 10-5](#part0022_split_001.html#id_297){.calibre5}).

**Task 13:** On rhcsa6, flip the value of the Boolean nfs_export_all_rw
persistently. ([Exercise
21-5](#part0033_split_001.html#id_555){.calibre5}).

**Task 14:** On rhcsa5 and rhcsa6, set the tuning profile to powersave.
([Exercise 12-2](#part0024_split_001.html#id_348){.calibre5}).

**Task 15:** On rhcsa5, create file lnfile1 under *var*tmp and create
three hard links called hard1, hard2, and hard3 for it. Identify the
inode number associated with all four files. Edit any of the files and
observe the metadata for all the files for confirmation. ([Exercise
3-2](#part0015_split_001.html#id_106){.calibre5}).

**Task 16:** On rhcsa5, members (user100 and user200) of group100 should
be able to collaborate on files under /shared but cannot delete each
other's files. ([Exercises
4-5](#part0016_split_001.html#id_130){.calibre5} and
[4-6](#part0016_split_001.html#id_132){.calibre5}).

**Task 17:** Synchronize the entire /etc directory on rhcsa5 to
*var*tmp/etc on rhcsa6. Use in-transit compression. Capture the output
and any errors in the *var*tmp/etc.transfer file on rhcsa5 during the
synchronization process. ([Chapter
19](#part0031_split_000.html#page_433){.calibre5}, topic: Synchronizing
Files Remotely Using rsync, and [Chapter
07](#part0019_split_000.html#page_151){.calibre5}, topic: Regular
Expressions).

**Task 18:** On rhcsa6, list all files that are part of the "setup"
package, and use regular expressions and I/O redirection to send the
output lines containing "hosts" to *var*tmp/setup.pkg. ([Exercise
9-2](#part0021_split_001.html#id_263){.calibre5}, and [Chapter
07](#part0019_split_000.html#page_151){.calibre5}, topics: Regular
Expressions, and Input, Output, and Error Redirections).

**Task 19:** On rhcsa5, check the current version of the Linux kernel.
Register rhcsa5 with RHSM and install the latest version of the kernel
available. Ensure that the existing kernel and its configuration remain
intact. Reboot the system and confirm the new version is loaded.
([Exercise 11-3](#part0023_split_001.html#id_322){.calibre5}, and
[Chapter 02](#part0014_split_000.html#page_35){.calibre5}, topic:
Viewing System Information).

**[]{#part0039.html#page_549 .calibre5}Task 20:** On rhcsa5, configure
journald to store messages permanently under *var*log/journal and fall
back to memory-only option if *var*log/journal directory does not exist
or has permission/access issues. ([Exercise
12-1](#part0024_split_000.html#page_294){.calibre5}).

**Task 21:** Write a bash shell script that defines an environment
variable called ENV1=book1 and creates a user account that matches the
value of the variable. ([Chapter
22](#part0034_split_000.html#page_481){.calibre5}: Script02 and
Script03).

**Task 22:** On rhcsa5, launch a named root container with host port 443
mapped to container port 443. Employ the latest version of the ubi7
image. Configure a systemd service to auto-start the container at system
reboots. Validate port mapping using an appropriate podman subcommand.
([Exercises 23-5](#part0035_split_000.html#page_518){.calibre5} and
[23-9](#part0035_split_001.html#id_619){.calibre5}).

**Task 23:** On rhcsa5, launch a named container as user100 with *data01
mapped to* data01 and two variables KERN=\$(uname -r) and SHELL defined.
Use the latest version of the ubi8 image. Configure a systemd service to
auto-start the container at system reboots without the need for user100
to log in. Create a file under the shared mount point and validate data
persistence. Verify port mapping using an appropriate podman subcommand.
([Exercises 23-7](#part0035_split_001.html#id_615){.calibre5},
[23-8](#part0035_split_001.html#id_617){.calibre5}, and
[23-10](#part0035_split_001.html#id_620){.calibre5}).

**Reboot the system and validate the configuration.**

[]{#part0040.html}

**[]{#part0040.html#page_551 .calibre2}Bibliography**

The following websites, forums, and guides were referenced in writing
this book:

1[.]{.c19} [www.virtualbox.org](http://www.virtualbox.org){.calibre5}

2[.]{.c19}
[docs.redhat.com/docs/en-US](http://docs.redhat.com/docs/en-US){.calibre5}

3[.]{.c19}
[developers.redhat.com](http://developers.redhat.com){.calibre5}

4[.]{.c19} [www.redhat.com](http://www.redhat.com){.calibre5}

5[.]{.c19} [www.opensource.org](http://www.opensource.org){.calibre5}

6[.]{.c19} [www.systemd.io](http://www.systemd.io){.calibre5}

7[.]{.c19} [www.tldp.org](http://www.tldp.org){.calibre5}

8[.]{.c19} [wiki.archlinux.org](http://wiki.archlinux.org){.calibre5}

9[.]{.c19} [www.ibm.com](http://www.ibm.com){.calibre5}

10[.]{.c23} [www.centos.org](http://www.centos.org){.calibre5}

11[.]{.c23} [www.wikipedia.org](http://www.wikipedia.org){.calibre5}

12[.]{.c23} [www.linux.org](http://www.linux.org){.calibre5}

13[.]{.c23} [www.firewalld.org](http://www.firewalld.org){.calibre5}

14[.]{.c23} [www.apache.org](http://www.apache.org){.calibre5}

15[.]{.c23} [www.gnome.org](http://www.gnome.org){.calibre5}

16[.]{.c23} [www.ietf.org](http://www.ietf.org){.calibre5}

17[.]{.c23} [www.isc.org](http://www.isc.org){.calibre5}

18[.]{.c23} [www.netfilter.org](http://www.netfilter.org){.calibre5}

19[.]{.c23} [www.nftables.org](http://www.nftables.org){.calibre5}

20[.]{.c23}
[www.nsa.gov/research/selinux](http://www.nsa.gov/research/selinux){.calibre5}

21[.]{.c23} [www.ntp.org](http://www.ntp.org){.calibre5}

22[.]{.c23}
[www.chrony.tuxfamily.org](http://www.chrony.tuxfamily.org){.calibre5}

23[.]{.c23} [www.openssh.org](http://www.openssh.org){.calibre5}

24[.]{.c23}
[www.pathname.com/fhs](http://www.pathname.com/fhs){.calibre5}

25[.]{.c23} [www.docker.io](http://www.docker.io){.calibre5}

26[.]{.c23}RHCSA Red Hat Enterprise Linux 8 book by Asghar Ghori
27[.]{.c23}Red Hat Certified System Administrator & Engineer for RHEL 7
book by Asghar Ghori 28[.]{.c23}Red Hat Certified System Administrator &
Engineer for RHEL 6 book by Asghar Ghori

[]{#part0041.html}

**[]{#part0041.html#page_553 .calibre2}Glossary**

  ---------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  . (single dot)                                             Represents current directory.
  .. (double dots)                                           Represents parent directory of the current directory.
  Absolute mode                                              A method of permission allocation to a file or directory.
  Absolute path                                              A pathname that begins with a /.
  Access ACLs                                                ACL settings applied to files.
  Access Control List                                        A method of allocating file permissions to a specific user or group. See Named user and Named group.
  Access mode                                                See Permission mode.
  Access permission                                          See File permission.
  Access right                                               See File permission.
  Access Cache Vector                                        A special cache area that SELinux uses to store its decisions.
  ACL                                                        See Access Control List.
  ACL mask                                                   Controls the maximum permissions a named user or named group can have.
  Address Resolution Protocol                                A protocol used to determine a system's Ethernet address when its IP address is known.
  Address space                                              Memory location that a process can refer.
  Administrator                                              See Superuser.
  Algorithm                                                  A set of well-defined but complex mathematical instructions used for data encryption and decryption.
  Alias                                                      A short name to refer to a lengthy command.
  Alias substitution                                         See Alias.
  Anaconda                                                   RHEL's installation program.
  Anacron                                                    A service that runs missing cron and at jobs after a system reboot.
  Apache                                                     A popular HTTP web server software.
  Application module                                         A complete set of packages to install a software application.
  Application stream                                         A method of making multiple versions of a software application available for installation from the same repository.
  AppStream                                                  One of the yum repositories in RHEL 8 that provides a number of add-on software applications along with some core operating system components.
  Archive                                                    A file that contains one or more files.
  Argument                                                   A value passed to a command or program.
  []{#part0041.html#page_554 .calibre5}ARP                   See Address Resolution Protocol.
  ASCII                                                      An acronym for American Standard Code for Information Interchange.
  Asymmetric encryption technique                            A technique that uses a combination of public/private keys to allow two network entities to communicate privately.
  Auditing                                                   System and user activity record and analysis.
  Authentication                                             The process of identifying a user to a system.
  AutoFS                                                     The NFS client-side service that automatically mounts and unmounts an NFS share on an as-needed basis.
  AutoFS maps                                                Configuration files to define the directory location to automount a remote share.
  Automounter                                                See AutoFS.
  AVC                                                        See Access Vector Cache.
  Background process                                         A process that runs in the background.
  Backup                                                     Process of saving data on an alternative media such as a tape or another disk.
  BaseOS                                                     One of the yum repositories in RHEL 8 that includes the foundational RHEL components.
  Bash shell                                                 A feature-rich default shell available in Red Hat Enterprise Linux.
  Berkeley Internet                                          A University of California at Berkeley implementation of DNS for Linux and
  Name Domain                                                UNIX platforms. See also Domain Name System.
  Binary package                                             A software package available in a format that yum/dnf/rpm can recognize and install.
  BIND                                                       See Berkeley Internet Name Domain.
  BIOS                                                       Basic I/O System. Software code that sits in the computer's non-volatile memory and is executed when the system is booted. Also see Firmware.
  Block                                                      A collection of bytes of data transmitted as a single unit.
  Block device file                                          A file associated with devices that transfer data randomly in blocks. Common examples are disk, CD, and DVD.
  Bluetooth                                                  A wireless technology for communication.
  Boolean                                                    The on/off switch to permit or deny an SELinux rule for a service.
  Boot                                                       See Boot process.
  Bootloader                                                 A small program that loads the operating system in memory.
  Boot order                                                 The sequence in which to try devices to boot the system.
  Boot process                                               The process of starting up a system to a usable state.
  Bourne Again Shell                                         See Bash shell.
  Bus                                                        Data communication path among devices in a computer system.
  Cache                                                      A temporary storage area on the system where frequently accessed information is duplicated for quick future access.
  Calling process                                            See Parent process.
  []{#part0041.html#page_555 .calibre5}CentOS                Community Enterprise Operating System. A 100% unsponsored rebuild of Red Hat Enterprise Linux OS available for free.
  Cgroup                                                     See Control group.
  Challenge-response                                         An authentication method that presents one or more arbitrary challenge
  authentication                                             questions to the user.
  Character special                                          A file associated with devices that transfer data serially, one character at a
  file                                                       time. Common examples are disk, tape, and mouse.
  Child directory                                            A directory one level below the current directory.
  Child process                                              A sub-process started by a process.
  Child shell                                                A child shell is spawned by the current shell as needed.
  Chrony                                                     An implementation of Network Time Protocol for time synchronization on network devices.
  CIDR                                                       See Classless Inter-Domain Routing.
  Classless Inter-                                           A technique for better use of IP addresses. It also results in smaller and less
  Domain Routing                                             cluttered routing tables.
  Command                                                    An instruction given to the system to perform a task.
  Command aliasing                                           See Alias.
  Command history                                            See History substitution.
  Command interpreter                                        See Shell.
  Command argument line                                      See Positional parameter.
  Command completion line                                    See Tab completion.
  Command editing line                                       A shell feature that allows editing at the command line.
  Command prompt                                             The OS prompt where you type commands.
  Command                                                    A shell feature that allows the assignment of the output of an executed
  substitution                                               command to a variable.
  Compression                                                The process of compressing data.
  Container                                                  A set of processes that runs in complete isolation from rest of the processes on the system.
  Containerized application                                  An application packaged to run inside a container.
  Container image                                            A file that contains all necessary components required by an application to run smoothly and securely.
  Container registry                                         A public or private storage location for container images.
  Context (SELinux)                                          A set of SELinux attributes applied to SELinux subjects and objects.
  Contiguous blocks data                                     A series of data blocks.
  Control group                                              A process management technique.
  []{#part0041.html#page_556 .calibre5}Core                  A core is a processor that shares the chip with other cores. Multi-core processor chips are common.
  CPU-intensive                                              A program or application that heavily uses system processors.
  Crash                                                      An abnormal system shutdown caused by electrical outage or kernel malfunction, *etc.*
  Crontable                                                  A table of cron jobs scheduled for a user. Commonly abbreviated as crontab.
  Current directory                                          The present working directory.
  Current shell                                              The shell where a program is launched. Compare with Child shell.
  DAC (SELinux)                                              See Discretionary Access Control.
  Daemon                                                     A server process that runs in the background and responds to client requests.
  Database                                                   A collection of data.
  D-bus                                                      Desktop Bus. Another communication method that allows multiple services running in parallel on a system to talk to one another on the same or remote system. Compare with Socket.
  De-duplication                                             A technique to remove redundant data blocks from storage to conserve space and improve performance.
  De-encapsulation                                           The reverse of encapsulation. See Encapsulation.
  Default                                                    Predefined values or settings that are automatically accepted by commands or programs.
  Default ACLs                                               ACL settings applied to directories.
  Default permissions                                        Permissions assigned to a file and directory at creation.
  Defunct process                                            See Zombie process.
  Desktop bus                                                See D-bus.
  Desktop environment                                        Software such as GNOME that provides graphical environment for users to interact with the system.
  Device                                                     A peripheral such as a printer, disk drive, or a CD/DVD device.
  Device driver                                              The software that controls a device.
  Device file                                                See Special file.
  DHCP                                                       See Dynamic Host Configuration Protocol.
  Directory structure                                        Inverted tree-like Linux/UNIX directory structure.
  Discretionary Access Control                               A rich set of traditional access controls in Linux.
  Disk-system based file                                     A file system created on a non-volatile storage device.
  Disk partitioning                                          Creation of partitions on a given storage device so as to access them as distinct, independent logical containers for data storage.
  Display manager                                            Application that is responsible for the presentation of graphical login screen.
  Dnf                                                        An upcoming major enhancement to yum.
  DNS                                                        See Domain Name System.
  []{#part0041.html#page_557 .calibre5}DNS name space        See Name space.
  Domain                                                     A group of computers configured to use a service such as DNS or NIS.
  Domain Name                                                The de facto hostname resolution service used on the Internet and corporate
  System                                                     networks.
  Domain (SELinux)                                           It ascertains the type of access that a process has.
  Domain transitioning                                       The ability of a process running in one SELinux domain to enter another domain to execute a task in that domain.
  Driver                                                     See Device driver.
  Dynamic Host                                               
  Configuration                                              A networking service that provides IP assignments to devices.
  Protocol                                                   
  Encapsulation                                              The process of forming a packet through the seven OSI layers.
  Encryption                                                 A method of scrambling information for privacy. See asymmetric encryption technique and symmetric encryption technique.
  Encryption keys                                            A single secret key or a pair of private/public keys that is used to encrypt and decrypt data for private communication between two network entities.
  Environment variable                                       A variable whose value is inherited by programs in sub-shells.
  EOF                                                        Marks the End OF File.
  Error redirection                                          A shell feature that allows forwarding error messages generated during a command execution to an alternative destination (file, printer, etc.).
  Ethernet                                                   A family of networking technologies designed for LANs.
  Ethernet address                                           See MAC address.
  Exit code                                                  A value returned by a command when it finishes execution.
  Exit value                                                 See Exit code.
  Export                                                     See Share.
  Exporting                                                  The process of making a directory or file system available over the network for sharing.
  Extended file system                                       A type of file system that has been around in Linux for decades and currently has the fourth generation included and widely used in recent Linux distributions.
  Extent                                                     The smallest unit of space allocation in LVM. It is always contiguous. See Logical extent and Physical extent.
  External command                                           A command external to the shell.
  Fedora                                                     Red Hat sponsored community project for collaborative enhancement of Red Hat Enterprise Linux OS.
  Fibre channel                                              A family of networking technologies designed for storage networking.
  File descriptor                                            A unique, per-process integer value used to refer to an open file.
  File globbing                                              See Filename expansion.
  Filename expansion                                         A series of characters used in matching filenames. Also see Metacharacters and Wildcard characters.
  []{#part0041.html#page_558 .calibre5}File permission       Read, write, execute or no permission assigned to a file or directory at the user, group, or public level.
  File system                                                A grouping of files stored in special data structures.
  File Protocol Transfer                                     A widely used protocol for file exchange.
  Filter                                                     A command that performs data transformation on the given input.
  Firewall                                                   A software or hardware appliance used for blocking inbound unauthorized access.
  Firewalld                                                  A dynamic firewall manager.
  Firewalld zone                                             A method of segregating incoming network traffic.
  Firmware                                                   The BIOS or the UEFI code in x86-based systems.
  FQIN                                                       See Fully Qualified Image Name.
  FTP                                                        See File Transfer Protocol.
  Full path                                                  See Absolute path.
  Fully Image Qualified Name                                 A container image name that includes all the necessary information to access it.
  Gateway                                                    A device that connects two networks.
  Gateway address                                            An IP address that allows a system to communicate with computers on a different network.
  GECOS                                                      General Electric Comprehensive Operating System. The comments field in the *etc*passwd file.
  GID                                                        See Group ID.
  Globally Identifier Unique                                 See Universally Unique Identifier.
  Globbing                                                   See Regular expression.
  GNOME                                                      GNU Object Model Environment. An intuitive graphical user environment.
  GNU                                                        GNU Not Unix. A project initiated to develop a completely free Unix-like operating system.
  GPG                                                        Gnu Privacy Guard. An open source implementation of PGP. See PGP.
  GPL                                                        General Public License that allows the use of software developed under GNU project to be available for free to the general public.
  GPT                                                        See GUID Partition Table.
  Graphical User                                             An interface that allows users to interact with the operating system or
  Interface                                                  application graphically.
  Group                                                      A collection of users that requires same permissions on files and directories.
  Group collaboration                                        A collection of users from different groups with identical rights on files for the purpose of sharing.
  Group ID                                                   A numeric identifier assigned to a group.
  GRUB2                                                      Grand Unified Bootloader version 2. The second generation of the GRUB bootloader program that loads the operating system in memory.
  []{#part0041.html#page_559 .calibre5}GSSAPI-based          An authentication method that provides a standard interface for security
  authentication                                             mechanisms to be plugged in.
  Guest                                                      An operating system instance that runs in a virtual machine.
  GUI                                                        See Graphical User Interface.
  GUID                                                       See Universally Unique Identifier.
  GUID Table Partition                                       A small disk partition on a UEFI system that stores disk partition information.
  Hard link                                                  A mapping between a filename and its inode number.
  Hardware address                                           See MAC address.
  Hardware clock                                             See Real-Time Clock.
  Hashing                                                    See Password hashing.
  History expansion                                          See History substitution.
  History substitution                                       A shell feature that enables the storage of previously executed commands.
  Home directory                                             A directory where a user lands when he logs into the system.
  Host-based firewall                                        A firewall service that runs on the Linux system.
  Host-based                                                 An authentication method that allows a single user, a group of users, or all
  authentication                                             users on the client to be authenticated on the server.
  Hostname                                                   A unique name assigned to a network node.
  Hostname resolution                                        See Name resolution.
  Host table                                                 A file that maintains IP and hostname mappings.
  HTTP                                                       See HyperText Transfer Protocol.
  HTTPS                                                      See HyperText Transfer Protocol Secure.
  HyperText Protocol Transfer                                HyperText Transfer Protocol. Allows access to web pages.
  HyperText Protocol Secure Transfer                         Secure cousin of HTTP. Allows access to secure web pages.
  Hypervisor                                                 Software loaded on a computer to virtualize its hardware.
  ICMP                                                       See Internet Control Message Protocol.
  Index node                                                 An index node number holds a file's properties including permissions, size and creation/modification time as well as contains a pointer to the data blocks that actually store the file data.
  Init                                                       An older method of system initialization. It has been replaced by systemd in newer Linux versions.
  Initialization files                                       See Shell startup files.
  Initial permissions                                        Predefined permission settings that are used to calculate default permissions for new files and directories.
  Initial Setup                                              Program that starts at first system reboot after a system has been installed to customize authentication, firewall, network, time zone and other services.
  Inode                                                      See Index node.
  []{#part0041.html#page_560 .calibre5}Inode table           A table in a file system that keeps a record of inode numbers.
  Input redirection                                          A shell feature that allows supplying input to a command from an alternative source (file, etc.).
  Installable package                                        See Binary package.
  Installer program                                          A program that is launched to install an operating system or application.
  Interface card                                             See Network device.
  Internet                                                   A complex network of computers and routers.
  Internet Control                                           A well-known networking protocol that is primarily used for testing and
  Message Protocol                                           debugging.
  Internet Protocol                                          A protocol that is responsible for relaying traffic between network entities.
  Inter-Process Communication                                Allows processes to communicate directly with each other by sharing parts of their virtual memory address space, and then reading and writing data stored in that shared virtual memory.
  I/O redirection                                            A shell feature that allows getting input from a non-default location and sending output and error messages to non-default locations.
  IP                                                         See Internet Protocol.
  IP address                                                 A unique 32-or 128-bit software address assigned to a network node.
  IPC                                                        See Inter-Process Communication.
  ISO9660                                                    A file system type used to mount optical devices.
  Job                                                        A process started in the background.
  Job control                                                The management of jobs running in the background and foreground.
  Job scheduling                                             Execution of commands, programs, or scripts in future.
  Journald                                                   A systemd-based logging service for collecting and storing logging data.
  Journaled file                                             A file system that uses the journaling mechanism for swift recovery after a
  system                                                     system crash.
                                                             A file system feature that allows it to maintain a journal (log) of its metadata
  Journaling                                                 changes to be used to fix any potential anomalies that may arise due to an
                                                             abnormal system shutdown.
  Kerberos                                                   A networking protocol used for user authentication over unsecure networks.
  Kernel                                                     Software that controls the entire system including all hardware and software.
  Kernel-Virtual based Machine                               An open source hypervisor software used for host virtualization.
  Kvdo                                                       A kernel module to support the Virtual Data Optimizer feature.
  KVM                                                        See Kernel-based Virtual Machine.
  Label (storage)                                            A unique partition identifier that may be used instead of a UUID or device file.
  Label (SELinux)                                            See Context.
  Labeling                                                   The process of mapping files with their stored SELinux contexts.
  Latency                                                    The time it takes for a data packet to travel between two network entities.
  Link                                                       An object that associates a filename to any type of file.
  []{#part0041.html#page_561 .calibre5}Link count            Number of links that refers to a file.
  Link layer address                                         See MAC address.
  Linux                                                      A UNIX-like, open source operating system.
  Load balancing                                             A technique whereby more than one server serve client requests to share the load.
  Localhost                                                  A reserved, non-networked hostname assigned to every device. It represents the device itself.
  Local variable                                             A variable whose value is private to the shell (current shell) it is defined in.
  Logical extent                                             A unit of space allocation for logical volumes in LVM.
  Logical volume                                             A logical container in LVM that holds a file system or swap.
  Login Manager                                              See Display manager.
  Logging                                                    A process of capturing desired alerts and forwarding them to preconfigured locations.
  Logical construct                                          Controls the flow of a script via test conditions.
  Logical Manager Volume                                     A widely used disk partitioning solution.
  Login                                                      A process that begins when a user enters a username and password at the login prompt.
  Login directory                                            See Home directory.
  Loopback                                                   A reserved IP address assigned to a device for testing and troubleshooting local issues.
  Looping construct                                          Performs an action on a list of elements or repeatedly until a condition becomes true or false.
  LVM                                                        See Logical Volume Manager.
  MAC address                                                A unique 48-bit hardware address of a network interface. Also called physical address, Ethernet address, and hardware address.
  MAC (SELinux)                                              See Mandatory Access Control.
  Machine                                                    A computer, system, workstation, desktop, or server.
  Major number                                               A number that points to a device driver.
  Mandatory Control Access                                   A rich set of policies for granular access controls.
  Map                                                        See AutoFS map.
  Masquerading                                               A variant of NAT.
  Master Boot Record                                         A small region on the disk that stores disk partition information.
  MBR                                                        See Master Boot Record.
  Memory-based file                                          A kernel-managed virtual file system created in memory at system boot and
  system                                                     destroyed at system shutdown.
  Memory-intensive                                           A program or application that heavily uses memory.
  Metacharacters                                             A series of characters that have special meaning to the shell and are used in pattern matching and filename globbing. Also see Wildcard characters.
  []{#part0041.html#page_562 .calibre5}Minor number          A unique number that points to an individual device controlled by a specific device driver.
  MLS                                                        See Multi-Level Security.
  Module (kernel)                                            Device drivers used to control hardware devices and software components.
  Module (package)                                           See Application module.
  Mounting                                                   Attaching a device (a file system, a CD/DVD) to the directory structure.
  Multi-Security Level                                       One of the two standard SELinux policies that controls access at deeper levels.
  Named group                                                A specific group that receives ACLs.
  Named pipe                                                 Allows two unrelated processes running on the same system or on two different systems to communicate with each other and exchange data.
  Named user                                                 A specific user that receives ACLs.
  Name resolution                                            A technique to determine IP address by providing hostname.
  Namespace                                                  A layer of isolation between process groups and the rest of the system.
  Name space                                                 A hierarchical organization of DNS domains on the Internet.
  NAT                                                        See Network Address Translation.
  NDP                                                        See Neighbor Discovery Protocol.
  Neighbor Discovery                                         A networking protocol that is used to discover Ipv6 devices and troubleshoot
  Protocol                                                   networking issues.
  Netfilter                                                  A framework that provides a set of hooks within the kernel to enable it to intercept and manipulate data packets.
  Netmask                                                    See Subnet mask.
  Network                                                    Two or more computers joined together to share resources.
  Network Address                                            Allows systems on an internal network to access external networks using a
  Translation                                                single IP address.
  Network classes                                            Ranges of IP addresses classified into five distinct categories.
  Network connection                                         A connection profile attached to a network device (interface).
  Network device                                             A physical or virtual network interface assigned to a system for network connectivity.
  Network File                                               A networking protocol that allows Linux systems to share resources (files,
  System                                                     directories, and file systems) on the network.
  Network interface                                          See Network device.
  Network card interface                                     See Network device.
  NetworkManager                                             A Linux service that is used to configure, administer, and monitor network devices and connections.
  Network mask                                               See Subnet mask.
  Network Time                                               A networking protocol that is used to synchronize the system clock with a
  Protocol                                                   reliable time source.
  NIC                                                        See Network device.
  []{#part0041.html#page_563 .calibre5}NFS                   See Network File System.
  NFS client                                                 A system that mounts an exported Linux resource.
  NFS server                                                 A system that exports (shares) a resource for mounting by an NFS client.
  Nftables                                                   A packet classification framework to monitor network traffic.
  Niceness                                                   It determines the priority of a process.
  Nice value                                                 See Niceness.
  Node                                                       A network device with a hostname and IP address.
  Node name                                                  A unique name assigned to a node.
  Nologin account (user)                                     A user without the ability to log in to the system.
  Normal account (user)                                      A user account with limited privileges on the system.
  NTP                                                        See Network Time Protocol.
  NTP client                                                 A system that receives time from a primary or secondary NTP server for its clock adjustments.
  NTP peer                                                   Two or more time servers that operate at the same stratum level.
  NTP pool                                                   A pool of time servers.
  NTP server                                                 See Primary NTP server and Secondary NTP server.
  Object (SELinux)                                           A file, directory, file system, device, network connection, network interface, network socket, network port, *etc.*
  Octal mode                                                 A method for setting permissions on a file or directory using octal numbering system.
  Octal system numbering                                     A 3 digit numbering system that represents values from 0 to 7.
  On-activation demand                                       A systemd way of activating a service when needed.
  Open source                                                Any software whose source code is published and is accessible at no cost to the public under GNU GPL for copy, modification and redistribution.
  OpenSSH                                                    A free implementation of secure shell services and utilities.
  Orphan process                                             An alive child process of a terminated parent process.
  Output redirection                                         A shell feature that allows forwarding a command output to an alternative destination (file, printer, etc.).
  Owner                                                      A user who has ownership rights on a file, directory, or process.
  Owning user                                                The owner of a file or directory.
  Owning group                                               The group of a file or directory.
  Package                                                    A set of necessary files and metadata that makes up a software application.
  Package credibility                                        The authenticity or originality of a package.
  Package database                                           A directory location that stores metadata for installed packages.
  []{#part0041.html#page_564 .calibre5}Package dependency    Additional required packages for a successful installation or functioning of another package.
  Package group                                              A group of similar applications that can be managed as a single entity.
  Package integrity                                          A state of being complete and error-free.
  Package module                                             See Application module.
  Paging                                                     The process of transferring data between memory and swap space.
  PAM                                                        See Pluggable Authentication Module.
  Parent directory                                           A directory one level above the current directory.
  Parent process                                             A process with one or more child processes spawned.
  Parent process ID                                          The ID of a process that starts a child process.
  Parallelism                                                A systemd way of starting multiple services concurrently at system boot.
  Partition                                                  A partition created on a storage device.
  Password aging                                             A mechanism that provides enhanced control on user passwords.
  Password-based authentication                              An authentication method that prompts users to enter their passwords to be signed in.
  Password hashing                                           A one-way process of converting a legible text string into a random but unique string of characters using one of the several available password hashing algorithms.
  Pattern matching                                           See Regular expression.
  Peer                                                       See NTP peer.
  Peruser startup files                                      A set of initialization files that defines custom settings for an individual user upon logging in.
  Performance-based                                          Hands-on implementation.
  Performance                                                The process of acquiring data from system components for analysis and
  monitoring                                                 decision-making purposes.
  Permission                                                 Right to read, write, or execute.
  Permission class                                           Access rights on files and directories based on an individual user, a group of users, or everyone else on the system.
  Permission type                                            Read, write, or execute permission bits set on files or directories.
  Permission mode                                            Add, revoke, or assign a permission type to a permission class.
  Persistent storage                                         A host directory mounted inside a container to store application-generated data for persistence.
  PGP                                                        Pretty Good Privacy. An encryption program to ensure data privacy and secrecy.
  Physical address                                           See MAC address.
  Physical extent                                            A unit of space allocation on physical volumes in LVM.
  Physical volume                                            A disk or a partition logically brought under LVM control.
  PID                                                        See Process ID.
  Pipe                                                       Sends output of one command as input to the second command.
  []{#part0041.html#page_565 .calibre5}Pipeline              A command construction with the pipe character used multiple times.
  Pluggable                                                  A set of library routines that allows using any authentication service available
  Authentication                                             on a system for user authentication, password modification and user account
  Module                                                     validation purposes.
  Policy (SELinux)                                           A set of rules enforced system-wide for analysis of security attributes on subjects and objects.
  Pool                                                       See Storage pool and Thin pool.
  Pool (NTP)                                                 See NTP pool.
  Port                                                       A number appended to an IP address. This number could be associated with a well-known service or is randomly generated.
  Port forwarding                                            A method of directing incoming network traffic to an alternative network port.
  Port mapping                                               Allows containerized applications to communicate with one another and with the container host.
  Positional parameter                                       An argument supplied to a script at the time of its invocation, and its position is determined by the shell based on location with reference to the calling script.
  POST                                                       Power-On-Self-Test that runs at system boot to test hardware. See BIOS, Firmware, and UEFI.
  Postfix                                                    A mail transfer application used for sending and receiving mail.
  PPID                                                       See Parent process ID.
  Primary DNS                                                A system that acts as the primary provider of DNS zones.
  Primary NTP server                                         A system that gets time from a more reliable source and provides time to secondary servers or clients.
  Primary prompt                                             The symbol where commands and programs are typed for execution.
  Priority                                                   See Process priority.
  Private key                                                A randomly generated portion of the private/public key combination that is used to decode the messages encrypted with the paired public key.
  Privilege                                                  An extra right to accomplish something.
  Process                                                    Any command, program, or daemon that runs on a system.
  Process ID                                                 A numeric identifier assigned by kernel to each process spawned.
  Process niceness                                           See Niceness.
  Process priority                                           The value at which a process is running. This value is determined based on the current niceness setting.
  Process state                                              One of multiple states in which a process is held during its lifecycle.
  Processor                                                  A CPU. It may contain more than one cores.
  Profile (module)                                           A list of recommended packages that are organized for purpose-built convenient installations.
  Prompt                                                     See Primary prompt and Secondary prompt.
  Protocol                                                   A common language that communicating nodes understand.
  Proxy                                                      A system that acts on behalf of other systems to access network services.
  []{#part0041.html#page_566 .calibre5}Public key            A randomly generated portion of the private/public key combination that is used to encode messages destined for a specific user.
  Public key-based                                           An authentication method that uses a public/private key pair for user
  authentication                                             authentication.
  Public encryption key                                      See Asymmetric encryption technique.
  Quoting                                                    Treats the specified special character as a regular character by disabling their special meaning.
  Real-Time Clock                                            A battery-backed hardware clock on the system.
  Recovery                                                   A function that recovers a crashed system to its previous normal state. It may require restoring lost data files.
  Redhat Manager Package                                     A file format used for packaging software for RHEL and its clones.
  Red Hat                                                    
  Subscription                                               A comprehensive management service provided by Red Hat to its clients.
  Management                                                 
  Redirection                                                Getting input from and sending output to non-default destinations.
  Regex                                                      See Regular expression.
  Regexp                                                     See Regular expression.
  Registry                                                   See Container registry.
  Regular expression                                         A string of characters commonly used for pattern matching and filename globbing.
  Relative path                                              A path to a file relative to the current user location in the file system hierarchy.
  Renicing                                                   Changing the niceness of a running process.
  Repository                                                 A URL location that provides access to software packages for installation.
  Rescue mode                                                A special boot mode for fixing and recovering an unbootable system.
  Resolver                                                   The client-side of DNS.
  Return code                                                See Exit code.
  RHCE                                                       Red Hat Certified Engineer. A designation that may be earned by passing a performance based RHCE exam.
  RHCSA                                                      Red Hat Certified System Administrator. A designation that may be earned by passing a performance based RHCSA exam.
  RHEL                                                       Red Hat Enterprise Linux.
  RHSM                                                       See Red Hat Subscription Management.
  Role (SELinux)                                             It controls who (SELinux subject) is allowed to access what (SELinux domains or types).
  Root (user) account                                        See Superuser.
  Router                                                     A device that routes data packets from one network to another.
  Routing                                                    The process of choosing a path over which to send a data packet.
  Root container                                             A container launched by the root user or with root privileges.
  []{#part0041.html#page_567 .calibre5}Rootless container    A container launched by a normal, unprivileged Linux user.
  Root servers                                               The thirteen most accurate root DNS servers.
  RPM                                                        See RedHat Package Manager.
  Rsyslog                                                    Essential Linux service for capturing system messages and forwarded them to various destinations for storage.
  RTC                                                        See Real-Time Clock.
  Runtime                                                    The operational state of an operating system.
  SAS                                                        Serial Attached SCSI. See Small Computer System Interface.
  SATA                                                       Serial Advanced Technology Attachment. This disk technology is a successor to the PATA drives.
  Script                                                     A text program written to perform a series of tasks.
  SCSI                                                       See Small Computer System Interface.
  Search path                                                A list of directories where the system looks for the specified command.
  Seccomp                                                    See Secure Computing Mode.
  Secondary DNS                                              A system that acts as an alternate provider of DNS zones.
  Secondary NTP                                              A system that gets time from a primary NTP server and provides time to NTP
  server                                                     clients.
  Secondary prompt                                           A prompt indicating that the entered command needs more input.
  Secret encryption key                                      See Symmetric encryption technique.
  Secure Mode Computing                                      A Linux feature that impose security constraints to protect processes.
  Secure shell                                               A set of tools that gives secure access to a system.
  Security context                                           SELinux security attributes set on files, processes, users, ports, *etc.*
  Security Enhanced                                          An implementation of Mandatory Access Control architecture for enhanced
  Linux                                                      and granular control on files, processes, users, ports, *etc.*
  SELinux                                                    See Security Enhanced Linux.
  Server (hardware)                                          Typically, a larger and more powerful system that offers services to network users.
  Server (software)                                          A process or daemon that runs on the system to serve client requests.
  Service account (user)                                     A user account that is used to control an installed application or service.
  Set Group ID                                               Sets effective group ID.
  Set User ID                                                Sets effective user ID.
  Setgid                                                     See Set group ID.
  Setuid                                                     See Set user ID.
  Shadow password                                            A mechanism to store passwords and password aging data in a secure file.
  Share                                                      A directory or file system shared over the network.
  []{#part0041.html#page_568 .calibre5}Shared memory         A portion in physical memory created by a process to share it with other processes that communicate with that process.
  Sharing                                                    See Exporting.
  Shell                                                      The Linux command interpreter that sits between a user and kernel.
  Shell parameter                                            An entity that holds a value such as a name, special character, or number.
  Shell program                                              See Script.
  Shell script                                               See Script.
  Shell scripting                                            Programming in a Linux shell to automate one or a series of tasks.
  Shell startup files                                        A set of files that are used to define the environment for a user upon logging in.
  Shell variable                                             See Local variable.
  Signal                                                     A software interrupt sent to a process.
  Simple Transfer Mail Protocol                              A networking protocol used for email transfer over the Internet.
  Single user mode                                           An operating system state in which the system cannot be accessed over the network.
  Skeleton directory                                         A directory location where user default configuration templates are stored.
  Small System Computer Interface                            A parallel interface used to connect peripheral devices to the system.
  SMTP                                                       See Simple Mail Transfer Protocol.
  Snapshot                                                   The state of a system at a certain point in time.
  Socket                                                     A communication method that allows a process to talk to another process on the same or remote system.
  Soft link                                                  See Symbolic link.
  Source package                                             A software package that can be modified and repackaged for a specific purpose.
  Special characters                                         See Metacharacters.
  Special file                                               A file that points to a specific device.
  Special file permissions                                   Additional access permission bits that may be set on files and directories, where applicable, to give extra rights to (or limit rights for) normal users on executable files and shared directories. Also see Set user ID, Set group ID, and Sticky bit.
  SSH                                                        See Secure Shell.
  Standard error                                             A standard location to forward error messages to. Also see Error redirection.
  Standard input                                             A standard location to receive input from. Also see Input redirection.
  Standard output                                            A standard location to forward output to. Also see Output redirection.
  Startup files                                              See Shell startup files.
  Stderr                                                     See Standard error.
  Stdin                                                      See Standard input.
  Stdout                                                     See Standard output.
  []{#part0041.html#page_569 .calibre5}Sticky bit            Disallows non-owners to delete files located in a directory.
  Storage pool                                               A logical storage space created with one or more disks or partitions.
  Stratis                                                    A simplified storage management solution.
  Stratum level                                              The categorization of NTP time sources based on reliability and accuracy.
  Stream (module)                                            Represents a collection of packages that are organized by version.
  String                                                     A series of characters.
  Subject (SELinux)                                          A process or user.
  Subnet                                                     One of the smaller networks formed using the process of subnetting. See Subnetting.
  Subnet mask                                                Segregates the network bits from the node bits in an IP address.
  Subnetting                                                 The process of dividing an IP address into several smaller subnetworks.
  Sub-shell                                                  See Child shell. Compare with Current shell.
  Substituting users                                         See Switching users.
  Sudo                                                       A method of delegating a portion of superuser privileges to normal users.
  Superblock                                                 A small portion in a file system that holds the file system's critical information.
  Superuser                                                  A user with full powers on the system.
  Swap                                                       Alternative disk or file system location for paging.
  Switch                                                     A network device that looks at the MAC address and switches the packet to the correct destination port based on the MAC address.
  Switching users                                            The ability to switch into a different user account provided the target user's password is known.
  Symbolic link                                              A shortcut that points to a file or directory located somewhere in the directory hierarchy. Compare with hard link.
  Symbolic mode                                              A method of setting permissions on a file using non-decimal values.
  Symlink                                                    See Symbolic link.
  Symmetric encryption technique                             A technique that employs a secret key for private communication between two network entities.
  Syslog                                                     See rsyslog.
  System                                                     A computer or a logical partition in a computer that runs an operating system.
  System Administrator                                       Person responsible for installing, configuring and managing a RHEL system.
  System call                                                A mechanism that applications use to request service from the kernel.
  System console                                             A display terminal that acts as the system console.
  Systemd                                                    System daemon. The default method of system initialization and service management in newer Linux distributions including RHEL 7 and RHEL 8.
  System recovery                                            The process of recovering an unbootable system.
  System tuning                                              A service in RHEL 8 to monitor connected devices and dynamically adjust their parameters for performance improvement.
  []{#part0041.html#page_570 .calibre5}System-wide startup   A set of initialization files that defines common settings for all users upon
  files                                                      logging in.
                                                             A shell feature that allows completing a file or command name by typing a
  Tab completion                                             partial name at the command line and then hitting the Tab key twice for
                                                             additional matching possibilities.
  Target                                                     A logical collection of systemd units. All units within a target are treated as a single entity.
  Targeted policy                                            An SELinux policy.
  TCP                                                        See Transmission Control Protocol.
  TCP/IP                                                     Transmission Control Protocol / Internet Protocol. A stacked, standard suite of protocols for computer communication.
  Terminal                                                   A window where commands are executed.
  Test condition                                             Used in logical constructs to decide what to do next.
  Thin pool                                                  A pool of storage that uses the thin provisioning technology to allow the creation of volumes much larger than their actual physical size.
  Thin provisioning                                          An economical technique of storage allocation and utilization.
  Thrashing                                                  Excessive amount of paging.
  Throughput                                                 The amount of data transferred between two network entities within a specified period of time.
  Tilde expansion                                            See Tilde substitution.
  Tilde substitution                                         A shell feature that uses the tilde character as a shortcut to navigate within the directory tree.
  Time source                                                A reference device that provides time to other devices.
  Transmission Control Protocol                              A stateful and reliable transport protocol. Compare with UDP.
  Tty                                                        Refers to a terminal.
  Tuning profile                                             A set of attributes that can be applied to a system for improving performance of certain components.
  Type enforcement                                           It controls the ability of an SELinux subject to access domains and types.
  Udevd                                                      Dynamic device management service.
  UDP                                                        See User Datagram Protocol.
  UDS                                                        See Universal De-duplication Service.
  UEFI                                                       See Unified Extensible Firmware Interface.
  UID                                                        See User ID.
  Umask                                                      See User mask.
  Unified Extensible                                         Software code used in computers for pre-boot system management. Also see
  Firmware Interface                                         Firmware.
  Universal duplication De-Service                           A kernel module to support data de-duplication.
  Universally Unique                                         A unique alphanumeric software identifier used to identify an object, such as a
  IDentifier                                                 disk or disk partition.
  []{#part0041.html#page_571 .calibre5}Unmounting            Detaching a mounted file system or a CD/DVD from the directory structure.
  Unit                                                       A systemd object used to organize service startups, socket creation, *etc.*
  Universal Time                                             The reference time used around the world to determine the local time and time
  Coordinated                                                zone.
  USB                                                        Universal Serial Bus. A bus standard to connect peripheral devices.
  User Protocol Datagram                                     A stateless and unreliable transport protocol. Compare with TCP.
  User ID                                                    A numeric identifier assigned to a user.
  User mask                                                  A value used in calculating default access rights on new files and directories.
  User Private Group                                         Referred to the GID that matches with the user's UID for safeguarding the user's private data from other users.
  UTC                                                        See Universal Time Coordinated.
  UUID                                                       See Universally Unique IDentifier.
  Variable                                                   A temporary storage of data in memory.
  Variable substitution                                      A shell feature that allows the value of a variable to be used in a command.
  VDO                                                        See Virtual Data Optimizer.
  VFAT                                                       See Virtual File Allocation Table.
  VirtualBox                                                 A type II hypervisor to virtualize an operating system.
  VirtualBox Manager                                         The management interface for VirtualBox.
  Virtual console                                            One of several console screens available for system access.
  Virtual Optimizer Data                                     A feature to conserve disk space, improve data throughput, and save cost.
  Virtual Allocation File Table                              An MSDOS-compatible file system type.
  Virtual file system                                        See memory-based file system.
  Virtual host                                               An approach to host more than one website on a single system using unique or shared IP addresses.
                                                             A technology that allows a single physical computer to run several independent
  Virtualization                                             logical computers (called virtual machines) with complete isolation from one
                                                             another.
  Virtual machine                                            A logical computer running within a virtualized environment.
  Volume group                                               A logical container in LVM that holds physical volumes, logical volumes, file systems, and swap.
  Volume-managing                                            A storage management solution that dynamically and transparently manages
  file system                                                the underlying logical volume layer for file systems.
  Wayland                                                    An innovative, superior, faster networking protocol that has replaced the X Window System protocol in RHEL 8. See X Window System protocol.
  Web                                                        A system of interlinked hypertext documents accessed over a network or the Internet via a web browser.
  Web server                                                 A system or service that provides web clients access to website pages.
  []{#part0041.html#page_572 .calibre5}Wildcard characters   A subset of metacharacters used for character matching in strings. See also Metacharacters.
  Workload                                                   Any application, database, program, or a combination that runs on the system.
  XFS                                                        eXtended File System. A high-performance journaling file system type.
  X Window System                                            A networking protocol that lays the foundation to run graphical applications.
  protocol                                                   See Wayland.
  Yum repository                                             See Repository.
  Zero-elimination block                                     A technique to remove empty (zero-byte) data blocks from storage.
  Zombie process                                             A child process that terminated abnormally and whose parent process still waits for it.
  Zone (DNS)                                                 A delegated portion of a DNS name space.
  Zone (Firewalld)                                           A firewalld zone for traffic management.
  ---------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

```{=html}
</p>
```
[]{#part0042_split_000.html}

**[]{#part0042_split_000.html#page_573 .calibre2}Index**

**.**

.bash_history file, [157](#part0019_split_000.html#page_157){.calibre5}

.bash_profile file, [172](#part0019_split_000.html#page_172){.calibre5}

.bashrc file, [172](#part0019_split_000.html#page_172){.calibre5}

**A**

Absolute path, [49](#part0014_split_000.html#page_49){.calibre5}

Access Control List, [105](#part0016_split_000.html#page_105){.calibre5}

Access ACLs, [105](#part0016_split_000.html#page_105){.calibre5}

Default ACLs, [109](#part0016_split_000.html#page_109){.calibre5}

Defined, [105](#part0016_split_000.html#page_105){.calibre5}

mask, [106](#part0016_split_000.html#page_106){.calibre5}

Named groups, [105](#part0016_split_000.html#page_105){.calibre5}

Named users, [105](#part0016_split_000.html#page_105){.calibre5}

Access permissions (See File permissions) Address Resolution Protocol
(See Networking) alias command,
[160](#part0019_split_000.html#page_160){.calibre5}

Alias substitution (See Shell)

anacron command, [192](#part0020_split_000.html#page_192){.calibre5}

anacrontab file, [191](#part0020_split_000.html#page_191){.calibre5}

Application stream (See Package)

apropos command, [55](#part0014_split_000.html#page_55){.calibre5}

Archiving, [64](#part0015_split_000.html#page_64){.calibre5}

Archiving tools, [64](#part0015_split_000.html#page_64){.calibre5}

ARP (See Networking)

at command, [187](#part0020_split_000.html#page_187){.calibre5}

at.allow file, [186](#part0020_split_000.html#page_186){.calibre5}

[at.deny](http://at.deny){.calibre5} file,
[186](#part0020_split_000.html#page_186){.calibre5}

atd service, [186](#part0020_split_000.html#page_186){.calibre5},
[187](#part0020_split_000.html#page_187){.calibre5}

audit.log file, [476](#part0033_split_000.html#page_476){.calibre5}

Auto File System (See AutoFS)

AutoFS, [404](#part0029_split_000.html#page_404){.calibre5}

Automounting user home directories,
[410](#part0029_split_000.html#page_410){.calibre5}

Benefits, [405](#part0029_split_000.html#page_405){.calibre5}

Configuration file, [405](#part0029_split_000.html#page_405){.calibre5}

Defined, [404](#part0029_split_000.html#page_404){.calibre5}

How AutoFS works, [405](#part0029_split_000.html#page_405){.calibre5}

Maps, [406](#part0029_split_000.html#page_406){.calibre5}

Direct, [406](#part0029_split_000.html#page_406){.calibre5}

Indirect, [408](#part0029_split_000.html#page_408){.calibre5}

Master, [406](#part0029_split_000.html#page_406){.calibre5}

autofs service, [404](#part0029_split_000.html#page_404){.calibre5}

autofs.conf file, [405](#part0029_split_000.html#page_405){.calibre5}

**B**

bashrc file, [170](#part0019_split_000.html#page_170){.calibre5}

Berkeley Internet Name Domain (See Domain Name System) bg command,
[170](#part0019_split_000.html#page_170){.calibre5}

BIND (See Domain Name System)

blkid command, [350](#part0027_split_000.html#page_350){.calibre5}

Block device file (See File type)

Boot process, [250](#part0023_split_000.html#page_250){.calibre5}

Bootloader phase, [251](#part0023_split_000.html#page_251){.calibre5}

Firmware phase, [250](#part0023_split_000.html#page_250){.calibre5}

BIOS, [250](#part0023_split_000.html#page_250){.calibre5}

UEFI, [250](#part0023_split_000.html#page_250){.calibre5}

Initialization phase,
[251](#part0023_split_000.html#page_251){.calibre5}

Kernel phase, [251](#part0023_split_000.html#page_251){.calibre5}

boot.log file, [289](#part0024_split_000.html#page_289){.calibre5}

btmp file, [119](#part0017_split_000.html#page_119){.calibre5}

bunzip2 command, [66](#part0015_split_000.html#page_66){.calibre5}

bzip2 command, [65](#part0015_split_000.html#page_65){.calibre5}

bzip2 vs gzip, [66](#part0015_split_000.html#page_66){.calibre5}

**C**

Calling process (See Process)

cat command, [74](#part0015_split_000.html#page_74){.calibre5},
[75](#part0015_split_000.html#page_75){.calibre5}

cd command, [49](#part0014_split_000.html#page_49){.calibre5}

chage command, [136](#part0018_split_000.html#page_136){.calibre5}

Character device file (See File type)

chcon command, [469](#part0033_split_000.html#page_469){.calibre5}

chgrp command, [146](#part0018_split_000.html#page_146){.calibre5}

Child process (See Process)

chmod command, [91](#part0016_split_000.html#page_91){.calibre5}

chown command, [146](#part0018_split_000.html#page_146){.calibre5}

Chrony (See Network Time Protocol)

chrony.conf file, [420](#part0030_split_001.html#page_420){.calibre5}

chronyc command, [421](#part0030_split_000.html#page_421){.calibre5}

[]{#part0042_split_000.html#page_574 .calibre5}chronyd service,
[421](#part0030_split_000.html#page_421){.calibre5}

CIDR (See Networking)

Classless Inter-Domain Routing (See Networking) clear command,
[51](#part0014_split_000.html#page_51){.calibre5}

Command aliasing (See Shell)

Command construction, [45](#part0014_split_000.html#page_45){.calibre5}

Command history (See Shell)

Command interpreter (See Shell)

Command line completion (See Shell)

Command line editing (See Shell)

Compression (file), [64](#part0015_split_000.html#page_64){.calibre5}

Compression tools, [64](#part0015_split_000.html#page_64){.calibre5}

config file, [469](#part0033_split_000.html#page_469){.calibre5}

Containerized applications,
[502](#part0035_split_000.html#page_502){.calibre5}

Containers

Bare-Metal or Virtual Machine,
[504](#part0035_split_000.html#page_504){.calibre5}

Benefits, [503](#part0035_split_000.html#page_503){.calibre5}

Cgroups, [503](#part0035_split_000.html#page_503){.calibre5}

Configuration file (system-wide),
[509](#part0035_split_000.html#page_509){.calibre5}

Defined, [502](#part0035_split_000.html#page_502){.calibre5}

Environment variables,
[520](#part0035_split_000.html#page_520){.calibre5}

FQIN, [505](#part0035_split_001.html#page_505){.calibre5}

Images, [505](#part0035_split_001.html#page_505){.calibre5}

Managing

Containers (advanced),
[517](#part0035_split_000.html#page_517){.calibre5}

Containers (basic), [514](#part0035_split_000.html#page_514){.calibre5}

Images, [512](#part0035_split_000.html#page_512){.calibre5}

Operational state, [525](#part0035_split_000.html#page_525){.calibre5}

Persistent storage, [521](#part0035_split_000.html#page_521){.calibre5}

Port mappings, [517](#part0035_split_000.html#page_517){.calibre5}

Variables, [520](#part0035_split_000.html#page_520){.calibre5}

Namespaces, [503](#part0035_split_000.html#page_503){.calibre5}

Persistent storage, [521](#part0035_split_000.html#page_521){.calibre5}

Port mapping, [517](#part0035_split_000.html#page_517){.calibre5}

Registry, [505](#part0035_split_001.html#page_505){.calibre5}

Root, [506](#part0035_split_000.html#page_506){.calibre5}

Rootless, [506](#part0035_split_000.html#page_506){.calibre5}

Secure Computing Mode,
[503](#part0035_split_000.html#page_503){.calibre5}

State management with systemd,
[525](#part0035_split_000.html#page_525){.calibre5}

Counting words, lines, and characters,
[78](#part0015_split_000.html#page_78){.calibre5}

cp command, [78](#part0015_split_000.html#page_78){.calibre5}

cron log file, [187](#part0020_split_000.html#page_187){.calibre5},
[189](#part0020_split_000.html#page_189){.calibre5}

cron.allow file, [186](#part0020_split_000.html#page_186){.calibre5}

crond service, [186](#part0020_split_000.html#page_186){.calibre5},
[189](#part0020_split_000.html#page_189){.calibre5}

crontab command, [189](#part0020_split_000.html#page_189){.calibre5}

crontab file, [189](#part0020_split_000.html#page_189){.calibre5}

Crontable, [189](#part0020_split_000.html#page_189){.calibre5}

**D**

DAC (See SELinux

Daemon (See Process)

date command, [425](#part0030_split_000.html#page_425){.calibre5}

D-bus (See Initialization)

De-duplication, [314](#part0025_split_000.html#page_314){.calibre5}

Desktop environment, [37](#part0014_split_001.html#page_37){.calibre5}

Desktop manager, [36](#part0014_split_000.html#page_36){.calibre5}

Device driver (See Kernel)

df command, [350](#part0027_split_000.html#page_350){.calibre5},
[354](#part0027_split_000.html#page_354){.calibre5}

dig command, [428](#part0030_split_000.html#page_428){.calibre5}

Display/Login manager, [36](#part0014_split_000.html#page_36){.calibre5}

dnf command, [217](#part0022_split_000.html#page_217){.calibre5}

dnf.conf file, [216](#part0022_split_000.html#page_216){.calibre5}

DNS (See Domain Name System)

Documentation *usr*share/doc,
[58](#part0014_split_000.html#page_58){.calibre5}

Domain Name System, [425](#part0030_split_000.html#page_425){.calibre5}

BIND, [425](#part0030_split_000.html#page_425){.calibre5}

Domain, [425](#part0030_split_000.html#page_425){.calibre5}

FQDN, [426](#part0030_split_000.html#page_426){.calibre5}

Managing

Lookup tools, [428](#part0030_split_000.html#page_428){.calibre5}

Name resolution, [425](#part0030_split_000.html#page_425){.calibre5}

Name space, [425](#part0030_split_000.html#page_425){.calibre5}

Resolver configuration file,
[426](#part0030_split_000.html#page_426){.calibre5}

Resolver sources and order,
[427](#part0030_split_000.html#page_427){.calibre5}

Roles, [426](#part0030_split_000.html#page_426){.calibre5}

Client, [426](#part0030_split_000.html#page_426){.calibre5}

Primary (Master), [426](#part0030_split_000.html#page_426){.calibre5}

Secondary (Slave), [426](#part0030_split_000.html#page_426){.calibre5}

du command, [350](#part0027_split_000.html#page_350){.calibre5},
[355](#part0027_split_000.html#page_355){.calibre5}

**E**

e2label command, [350](#part0027_split_000.html#page_350){.calibre5}

echo command, [153](#part0019_split_000.html#page_153){.calibre5},
[483](#part0034_split_000.html#page_483){.calibre5}

Editing files, [69](#part0015_split_000.html#page_69){.calibre5}

Encapsulation, [450](#part0032_split_000.html#page_450){.calibre5}

env command, [153](#part0019_split_000.html#page_153){.calibre5}

Ethernet address (See Networking)

export command, [153](#part0019_split_000.html#page_153){.calibre5}

exportfs command, [401](#part0029_split_000.html#page_401){.calibre5}

**F**

fdisk command, [303](#part0025_split_000.html#page_303){.calibre5}

fg command, [169](#part0019_split_000.html#page_169){.calibre5}

FHS (See Filesystem Hierarchy Standard)

[]{#part0042_split_000.html#page_575 .calibre5}File and directory

Copying directory, [79](#part0015_split_000.html#page_79){.calibre5}

Copying file, [78](#part0015_split_000.html#page_78){.calibre5}

Creating directory, [75](#part0015_split_000.html#page_75){.calibre5}

Creating file, [74](#part0015_split_000.html#page_74){.calibre5}

Displaying content, [75](#part0015_split_000.html#page_75){.calibre5}

Moving directory, [80](#part0015_split_000.html#page_80){.calibre5}

Moving file, [79](#part0015_split_000.html#page_79){.calibre5}

Removing file, [80](#part0015_split_000.html#page_80){.calibre5}

Renaming directory, [80](#part0015_split_000.html#page_80){.calibre5}

Renaming file, [79](#part0015_split_000.html#page_79){.calibre5}

file command, [62](#part0015_split_000.html#page_62){.calibre5}

File permissions, [90](#part0016_split_000.html#page_90){.calibre5}

Calculating default, [94](#part0016_split_000.html#page_94){.calibre5}

Classes, [90](#part0016_split_000.html#page_90){.calibre5}

Default, [93](#part0016_split_000.html#page_93){.calibre5}

Initial, [93](#part0016_split_000.html#page_93){.calibre5}

Modes, [91](#part0016_split_000.html#page_91){.calibre5}

Modifying, [91](#part0016_split_000.html#page_91){.calibre5}

Using octal notation, [91](#part0016_split_000.html#page_91){.calibre5}

Using symbolic notation,
[91](#part0016_split_000.html#page_91){.calibre5}

Special, [95](#part0016_split_000.html#page_95){.calibre5}

setgid on directories, [98](#part0016_split_000.html#page_98){.calibre5}

setgid on files, [96](#part0016_split_000.html#page_96){.calibre5}

setuid (suid), [95](#part0016_split_000.html#page_95){.calibre5}

Sticky bit, [100](#part0016_split_000.html#page_100){.calibre5}

Types, [90](#part0016_split_000.html#page_90){.calibre5}

umask, [93](#part0016_split_000.html#page_93){.calibre5}

File system

/, [40](#part0014_split_001.html#page_40){.calibre5}

/boot, [41](#part0014_split_000.html#page_41){.calibre5}

/dev, [42](#part0014_split_000.html#page_42){.calibre5}

/home, [41](#part0014_split_000.html#page_41){.calibre5}

/opt, [41](#part0014_split_000.html#page_41){.calibre5}

/proc, [42](#part0014_split_000.html#page_42){.calibre5}

/run, [43](#part0014_split_000.html#page_43){.calibre5}

/sys, [43](#part0014_split_000.html#page_43){.calibre5}

/tmp, [42](#part0014_split_000.html#page_42){.calibre5}

/usr, [41](#part0014_split_000.html#page_41){.calibre5}

/var, [41](#part0014_split_000.html#page_41){.calibre5}

Benefits, [346](#part0027_split_000.html#page_346){.calibre5}

Categories, [40](#part0014_split_001.html#page_40){.calibre5},
[346](#part0027_split_000.html#page_346){.calibre5}

Disk-based, [40](#part0014_split_001.html#page_40){.calibre5}

Memory-based, [40](#part0014_split_001.html#page_40){.calibre5}

Network-based, [40](#part0014_split_001.html#page_40){.calibre5}

Defined, [346](#part0027_split_000.html#page_346){.calibre5}

Extended file system with journal,
[348](#part0027_split_000.html#page_348){.calibre5}

fstab file, [353](#part0027_split_000.html#page_353){.calibre5}

Managing, [349](#part0027_split_000.html#page_349){.calibre5}

Commands, [349](#part0027_split_000.html#page_349){.calibre5}

Determining UUID, [351](#part0027_split_000.html#page_351){.calibre5}

Labeling, [352](#part0027_split_000.html#page_352){.calibre5}

Mount options, [351](#part0027_split_000.html#page_351){.calibre5}

Mounting, [350](#part0027_split_000.html#page_350){.calibre5}

Mounting automatically,
[353](#part0027_split_000.html#page_353){.calibre5}

Unmounting, [351](#part0027_split_000.html#page_351){.calibre5}

Monitoring, [354](#part0027_split_000.html#page_354){.calibre5}

Top-level, [39](#part0014_split_000.html#page_39){.calibre5}

Types, [346](#part0027_split_000.html#page_346){.calibre5}

Extended, [347](#part0027_split_000.html#page_347){.calibre5}

ISO9660, [349](#part0027_split_000.html#page_349){.calibre5}

VFAT, [348](#part0027_split_000.html#page_348){.calibre5}

XFS, [348](#part0027_split_000.html#page_348){.calibre5}

UUID, [351](#part0027_split_000.html#page_351){.calibre5}

File type

Block device, [42](#part0014_split_000.html#page_42){.calibre5},
[63](#part0015_split_000.html#page_63){.calibre5}

Character device, [63](#part0015_split_000.html#page_63){.calibre5}

Directory, [63](#part0015_split_000.html#page_63){.calibre5}

Raw device, [42](#part0014_split_000.html#page_42){.calibre5}

Regular, [62](#part0015_split_000.html#page_62){.calibre5}

Symbolic link, [64](#part0015_split_000.html#page_64){.calibre5}

Symlink (See Symbolic link)

Filesystem Hierarchy Standard,
[39](#part0014_split_000.html#page_39){.calibre5}

find command, [102](#part0016_split_000.html#page_102){.calibre5}

Finding files, [102](#part0016_split_000.html#page_102){.calibre5}

Firewall

Defined, [450](#part0032_split_000.html#page_450){.calibre5}

Firewalld, [450](#part0032_split_000.html#page_450){.calibre5}

Managing, [453](#part0032_split_000.html#page_453){.calibre5}

Service, [452](#part0032_split_000.html#page_452){.calibre5}

Service files, [452](#part0032_split_000.html#page_452){.calibre5}

Zone, [450](#part0032_split_000.html#page_450){.calibre5}

Zone files, [451](#part0032_split_000.html#page_451){.calibre5}

Host-based, [450](#part0032_split_000.html#page_450){.calibre5}

firewall-cmd command,
[454](#part0032_split_000.html#page_454){.calibre5}

Firewalld (See Firewall)

firewalld service, [450](#part0032_split_000.html#page_450){.calibre5}

FQDN (See Domain Name System)

FQIN (See Containers),
[505](#part0035_split_001.html#page_505){.calibre5}

free command, [367](#part0027_split_000.html#page_367){.calibre5}

fstab file, [351](#part0027_split_000.html#page_351){.calibre5},
[353](#part0027_split_000.html#page_353){.calibre5},
[401](#part0029_split_000.html#page_401){.calibre5}

Fully Qualified Domain Name (See Domain Name System) Fully Qualified
Image Name (See Containers),
[505](#part0035_split_001.html#page_505){.calibre5}

**[]{#part0042_split_000.html#page_576 .calibre2}G**

gdisk command, [310](#part0025_split_000.html#page_310){.calibre5}

getenforce command, [469](#part0033_split_000.html#page_469){.calibre5},
[470](#part0033_split_000.html#page_470){.calibre5}

getent command, [430](#part0030_split_000.html#page_430){.calibre5}

getfacl command, [105](#part0016_split_000.html#page_105){.calibre5}

getsebool command, [469](#part0033_split_000.html#page_469){.calibre5}

Getting help, [52](#part0014_split_000.html#page_52){.calibre5}

GNU, [2](#part0013_split_000.html#page_2){.calibre5}

gpasswd command, [126](#part0017_split_001.html#page_126){.calibre5}

GPL, [2](#part0013_split_000.html#page_2){.calibre5}

grep command, [166](#part0019_split_000.html#page_166){.calibre5}

Group

Creating, [141](#part0018_split_000.html#page_141){.calibre5}

Deleting, [141](#part0018_split_000.html#page_141){.calibre5}

Identifying, [120](#part0017_split_000.html#page_120){.calibre5}

Modifying, [141](#part0018_split_000.html#page_141){.calibre5}

Owning group, [146](#part0018_split_000.html#page_146){.calibre5}

group file, [124](#part0017_split_000.html#page_124){.calibre5}

groupadd command, [141](#part0018_split_000.html#page_141){.calibre5}

groupdel command, [141](#part0018_split_000.html#page_141){.calibre5}

groupmod command, [141](#part0018_split_000.html#page_141){.calibre5}

groups command, [120](#part0017_split_000.html#page_120){.calibre5}

grub file, [254](#part0023_split_000.html#page_254){.calibre5}

grub.cfg file, [253](#part0023_split_001.html#page_253){.calibre5},
[254](#part0023_split_000.html#page_254){.calibre5}

GRUB[2](#part0013_split_000.html#page_2){.calibre5},
[251](#part0023_split_000.html#page_251){.calibre5}

Configuration files

grub, [254](#part0023_split_000.html#page_254){.calibre5}

grub.cfg, [253](#part0023_split_001.html#page_253){.calibre5}

grubenv, [255](#part0023_split_000.html#page_255){.calibre5}

Managing, [251](#part0023_split_000.html#page_251){.calibre5}

Specific targets, [256](#part0023_split_000.html#page_256){.calibre5}

grub[2](#part0013_split_000.html#page_2){.calibre5}-mkconfig command,
[254](#part0023_split_000.html#page_254){.calibre5},
[255](#part0023_split_000.html#page_255){.calibre5}

grubenv file, [255](#part0023_split_000.html#page_255){.calibre5}

gshadow file, [125](#part0017_split_000.html#page_125){.calibre5}

gunzip command, [65](#part0015_split_000.html#page_65){.calibre5}

gzip command, [65](#part0015_split_000.html#page_65){.calibre5}

gzip vs bzip[2](#part0013_split_000.html#page_2){.calibre5},
[66](#part0015_split_000.html#page_66){.calibre5}

**H**

halt command, [284](#part0024_split_000.html#page_284){.calibre5}

Hardware address (See Networking)

Hardware clock, [424](#part0030_split_000.html#page_424){.calibre5}

head command, [77](#part0015_split_000.html#page_77){.calibre5}

history command, [158](#part0019_split_000.html#page_158){.calibre5}

History expansion (See Shell)

History substitution (See Shell)

host command, [429](#part0030_split_000.html#page_429){.calibre5}

Hostname, [376](#part0028_split_000.html#page_376){.calibre5}

hostname command, [376](#part0028_split_000.html#page_376){.calibre5}

hostname file, [376](#part0028_split_000.html#page_376){.calibre5}

hostnamectl command, [376](#part0028_split_000.html#page_376){.calibre5}

hosts file, [393](#part0028_split_000.html#page_393){.calibre5},
[428](#part0030_split_000.html#page_428){.calibre5}

**I**

ICMP (See Networking)

id command, [120](#part0017_split_000.html#page_120){.calibre5}

ifdown command, [388](#part0028_split_000.html#page_388){.calibre5}

ifup command, [388](#part0028_split_000.html#page_388){.calibre5}

Index node number (See Inode),
[81](#part0015_split_000.html#page_81){.calibre5}

info command, [57](#part0014_split_000.html#page_57){.calibre5}

Initialization

systemd

Control groups, [272](#part0024_split_000.html#page_272){.calibre5}

D-bus, [273](#part0024_split_000.html#page_273){.calibre5}

Listing previous system reboots,
[117](#part0017_split_000.html#page_117){.calibre5}

Managing, [276](#part0024_split_000.html#page_276){.calibre5}

Setting default target,
[283](#part0024_split_000.html#page_283){.calibre5}

Switching target, [283](#part0024_split_000.html#page_283){.calibre5}

Target, [282](#part0024_split_000.html#page_282){.calibre5}

Unit, [277](#part0024_split_000.html#page_277){.calibre5}

Viewing default target,
[283](#part0024_split_000.html#page_283){.calibre5}

Overview, [272](#part0024_split_000.html#page_272){.calibre5}

Parallelism, [272](#part0024_split_000.html#page_272){.calibre5}

Socket, [272](#part0024_split_000.html#page_272){.calibre5}

Target

Analyzing file, [276](#part0024_split_000.html#page_276){.calibre5}

Defined, [275](#part0024_split_000.html#page_275){.calibre5}

Types, [275](#part0024_split_000.html#page_275){.calibre5}

Unit

Analyzing file, [274](#part0024_split_000.html#page_274){.calibre5}

Defined, [273](#part0024_split_000.html#page_273){.calibre5}

State, [273](#part0024_split_000.html#page_273){.calibre5}

Types, [274](#part0024_split_000.html#page_274){.calibre5}

Inode, [81](#part0015_split_000.html#page_81){.calibre5}

Installation

LAB Setup, [4](#part0013_split_000.html#page_4){.calibre5}

[]{#part0042_split_001.html}


```## RHEL {#part0042_split_001.html#calibre_pb_0 .calibre11}

Adding keyboard and languages,
[20](#part0013_split_000.html#page_20){.calibre5}

Attaching image, [17](#part0013_split_001.html#page_17){.calibre5}

Configuring install destination,
[22](#part0013_split_001.html#page_22){.calibre5}

Configuring network and hostname,
[23](#part0013_split_001.html#page_23){.calibre5}

Creating user account, [26](#part0013_split_000.html#page_26){.calibre5}

Downloading, [14](#part0013_split_001.html#page_14){.calibre5}

[]{#part0042_split_001.html#page_577 .calibre5}Finishing,
[27](#part0013_split_000.html#page_27){.calibre5}

Launching installer, [17](#part0013_split_001.html#page_17){.calibre5}

Logs, [5](#part0013_split_000.html#page_5){.calibre5}

Planning, [5](#part0013_split_000.html#page_5){.calibre5}

Post-Installation, [28](#part0013_split_001.html#page_28){.calibre5}

Selecting software, [21](#part0013_split_001.html#page_21){.calibre5}

Selecting source, [20](#part0013_split_000.html#page_20){.calibre5}

Setting root password, [26](#part0013_split_000.html#page_26){.calibre5}

Virtual consoles, [5](#part0013_split_000.html#page_5){.calibre5}

VirtualBox

Creating virtual machine,
[10](#part0013_split_000.html#page_10){.calibre5}

Downloading, [6](#part0013_split_000.html#page_6){.calibre5}

Internet Control Message Protocol (See

Networking)

Internet Protocol (See Networking)

IP address (See Networking)

ip command, [378](#part0028_split_000.html#page_378){.calibre5},
[383](#part0028_split_000.html#page_383){.calibre5}

IPv6 address (See Networking)

**J**

Job control, [169](#part0019_split_000.html#page_169){.calibre5}

Job scheduling

Anacron, [191](#part0020_split_000.html#page_191){.calibre5}

Controlling access, [186](#part0020_split_000.html#page_186){.calibre5}

crontab file syntax, [189](#part0020_split_000.html#page_189){.calibre5}

Daemon atd, [187](#part0020_split_000.html#page_187){.calibre5}

crond, [189](#part0020_split_000.html#page_189){.calibre5}

Logging, [187](#part0020_split_000.html#page_187){.calibre5}

Overview, [186](#part0020_split_000.html#page_186){.calibre5}

Step values, [190](#part0020_split_001.html#page_190){.calibre5}

Using at, [187](#part0020_split_000.html#page_187){.calibre5}

Using crontab, [189](#part0020_split_000.html#page_189){.calibre5}

jobs command, [169](#part0019_split_000.html#page_169){.calibre5}

journalctl command, [291](#part0024_split_000.html#page_291){.calibre5}

journald.conf file, [291](#part0024_split_000.html#page_291){.calibre5}

**K**

Kernel

Analyzing version, [259](#part0023_split_000.html#page_259){.calibre5}

Device driver, [258](#part0023_split_000.html#page_258){.calibre5}

Directory structure, [260](#part0023_split_000.html#page_260){.calibre5}

Installing, [264](#part0023_split_000.html#page_264){.calibre5}

Module, [262](#part0023_split_000.html#page_262){.calibre5}

Modules, [258](#part0023_split_000.html#page_258){.calibre5}

Overview, [258](#part0023_split_000.html#page_258){.calibre5}

Packages, [258](#part0023_split_000.html#page_258){.calibre5}

kill command, [184](#part0020_split_000.html#page_184){.calibre5}

killall command, [185](#part0020_split_000.html#page_185){.calibre5}

kvdo module, [314](#part0025_split_000.html#page_314){.calibre5}

**L**

last command, [117](#part0017_split_000.html#page_117){.calibre5}

lastb command, [118](#part0017_split_000.html#page_118){.calibre5}

lastlog command, [119](#part0017_split_000.html#page_119){.calibre5}

less command, [76](#part0015_split_000.html#page_76){.calibre5}

let command, [496](#part0034_split_000.html#page_496){.calibre5}

Linking files, [81](#part0015_split_000.html#page_81){.calibre5}

Copying vs. linking, [84](#part0015_split_000.html#page_84){.calibre5}

Hard link, [82](#part0015_split_000.html#page_82){.calibre5}

Link, [81](#part0015_split_000.html#page_81){.calibre5}

Symbolic link, [83](#part0015_split_000.html#page_83){.calibre5}

Linux

A quick look, [2](#part0013_split_000.html#page_2){.calibre5}

Defined, [2](#part0013_split_000.html#page_2){.calibre5}

Fedora Project, [3](#part0013_split_000.html#page_3){.calibre5}

RHEL history, [3](#part0013_split_000.html#page_3){.calibre5}

Linux directory structure,
[39](#part0014_split_000.html#page_39){.calibre5}

ln command, [83](#part0015_split_000.html#page_83){.calibre5}

Log rotation, [286](#part0024_split_000.html#page_286){.calibre5}

logger command, [290](#part0024_split_000.html#page_290){.calibre5}

Logging in, [29](#part0013_split_000.html#page_29){.calibre5}

Logging out, [31](#part0013_split_000.html#page_31){.calibre5}

Logical Volume Manager (See Storage)

[login.defs](http://login.defs){.calibre5} file,
[127](#part0017_split_000.html#page_127){.calibre5}

loginctl command, [525](#part0035_split_000.html#page_525){.calibre5}

logrotate command, [287](#part0024_split_000.html#page_287){.calibre5}

logrotate.conf file, [287](#part0024_split_000.html#page_287){.calibre5}

Loopback address (See Networking)

ls command, [46](#part0014_split_000.html#page_46){.calibre5}

lsblk command, [303](#part0025_split_000.html#page_303){.calibre5},
[326](#part0026_split_000.html#page_326){.calibre5},
[349](#part0027_split_000.html#page_349){.calibre5}

lscpu command, [52](#part0014_split_000.html#page_52){.calibre5}

lvcreate command, [325](#part0026_split_000.html#page_325){.calibre5}

lvdisplay command, [325](#part0026_split_000.html#page_325){.calibre5},
[326](#part0026_split_000.html#page_326){.calibre5}

lvextend command, [325](#part0026_split_000.html#page_325){.calibre5}

LVM (See Storage)

lvreduce command, [325](#part0026_split_000.html#page_325){.calibre5}

lvremove command, [325](#part0026_split_000.html#page_325){.calibre5}

lvrename command, [325](#part0026_split_000.html#page_325){.calibre5}

lvresize command, [325](#part0026_split_000.html#page_325){.calibre5}

lvs command, [324](#part0026_split_000.html#page_324){.calibre5},
[326](#part0026_split_000.html#page_326){.calibre5}

**[]{#part0042_split_001.html#page_578 .calibre2}M**

MAC address (See Networking)

machine-id file, [294](#part0024_split_000.html#page_294){.calibre5}

Major number, [64](#part0015_split_000.html#page_64){.calibre5}

man command, [53](#part0014_split_000.html#page_53){.calibre5}

mandb command, [55](#part0014_split_000.html#page_55){.calibre5}

Manual page headings, [54](#part0014_split_000.html#page_54){.calibre5}

Manual page sections, [54](#part0014_split_000.html#page_54){.calibre5}

meminfo virtual file,
[368](#part0027_split_000.html#page_368){.calibre5}

messages log file, [289](#part0024_split_000.html#page_289){.calibre5}

Metacharacters (See Shell)

Metadata (file), [81](#part0015_split_000.html#page_81){.calibre5}

Minor number, [64](#part0015_split_000.html#page_64){.calibre5}

mkdir command, [75](#part0015_split_000.html#page_75){.calibre5}

mke2fs command, [350](#part0027_split_000.html#page_350){.calibre5}

mkfs command, [350](#part0027_split_000.html#page_350){.calibre5}

mkfs.ext3 command, [350](#part0027_split_000.html#page_350){.calibre5}

mkfs.ext4 command, [350](#part0027_split_000.html#page_350){.calibre5}

mkfs.vfat command, [350](#part0027_split_000.html#page_350){.calibre5}

mkfs.xfs command, [350](#part0027_split_000.html#page_350){.calibre5}

mkswap command, [368](#part0027_split_000.html#page_368){.calibre5}

Module (See Package)

more command, [76](#part0015_split_000.html#page_76){.calibre5}

mount command, [350](#part0027_split_000.html#page_350){.calibre5},
[401](#part0029_split_000.html#page_401){.calibre5}

mounts virtual file, [351](#part0027_split_000.html#page_351){.calibre5}

mv command, [79](#part0015_split_000.html#page_79){.calibre5}

**N**

Name resolution (See Domain Name System)

NDP (See Networking)

Neighbor Discovery Protocol (See Networking) Netfilter,
[450](#part0032_split_000.html#page_450){.calibre5}

netfilter kernel module,
[450](#part0032_split_000.html#page_450){.calibre5}

Netmask (See Networking)

Network adapter (See Networking)

Network classes (See Networking)

Network connection (See Networking)

Network connection profile (See Networking) Network device (See
Networking)

Network device naming (See Networking)

Network entity (See Networking)

Network File System

Benefits, [400](#part0029_split_000.html#page_400){.calibre5}

Configuring, [401](#part0029_split_000.html#page_401){.calibre5}

Exporting, [400](#part0029_split_000.html#page_400){.calibre5}

Mounting, [400](#part0029_split_000.html#page_400){.calibre5}

Defined, [400](#part0029_split_000.html#page_400){.calibre5}

Share, [400](#part0029_split_000.html#page_400){.calibre5}

Versions, [401](#part0029_split_000.html#page_401){.calibre5}

Network Interface Card (See Networking)

Network Time Protocol

Chrony, [418](#part0030_split_000.html#page_418){.calibre5}

Configuration file, [420](#part0030_split_001.html#page_420){.calibre5}

Displaying time, [423](#part0030_split_000.html#page_423){.calibre5}

Modifying time, [424](#part0030_split_000.html#page_424){.calibre5}

Overview, [418](#part0030_split_000.html#page_418){.calibre5}

Roles, [419](#part0030_split_000.html#page_419){.calibre5}

Client, [419](#part0030_split_000.html#page_419){.calibre5}

Peer, [419](#part0030_split_000.html#page_419){.calibre5}

Primary server, [419](#part0030_split_000.html#page_419){.calibre5}

Secondary server, [419](#part0030_split_000.html#page_419){.calibre5}

Stratum levels, [419](#part0030_split_000.html#page_419){.calibre5}

Time server, [420](#part0030_split_001.html#page_420){.calibre5}

Time source

Internet-based, [419](#part0030_split_000.html#page_419){.calibre5}

Overview, [418](#part0030_split_000.html#page_418){.calibre5}

Radio/Atomic clock, [419](#part0030_split_000.html#page_419){.calibre5}

Networking

Changing hostname, [377](#part0028_split_000.html#page_377){.calibre5}

Configuring

Commands, [387](#part0028_split_001.html#page_387){.calibre5}

Fundamentals

ARP, [383](#part0028_split_000.html#page_383){.calibre5}

CIDR notation, [380](#part0028_split_000.html#page_380){.calibre5}

Classes, [379](#part0028_split_000.html#page_379){.calibre5}

Class A, [379](#part0028_split_000.html#page_379){.calibre5}

Class B, [379](#part0028_split_000.html#page_379){.calibre5}

Class C, [379](#part0028_split_000.html#page_379){.calibre5}

Class D, [380](#part0028_split_000.html#page_380){.calibre5}

Class E, [380](#part0028_split_000.html#page_380){.calibre5}

Common protocols, [382](#part0028_split_000.html#page_382){.calibre5}

Connection profile (anatomy),
[385](#part0028_split_000.html#page_385){.calibre5}

Consistent naming, [384](#part0028_split_000.html#page_384){.calibre5}

Entity, [376](#part0028_split_000.html#page_376){.calibre5}

Hostname, [376](#part0028_split_000.html#page_376){.calibre5}

Hosts table, [393](#part0028_split_000.html#page_393){.calibre5}

ICMP, [382](#part0028_split_000.html#page_382){.calibre5}

IP address, [378](#part0028_split_000.html#page_378){.calibre5}

IPv4 vs. IPv[6](#part0013_split_000.html#page_6){.calibre5},
[384](#part0028_split_000.html#page_384){.calibre5}

IPv[6](#part0013_split_000.html#page_6){.calibre5},
[383](#part0028_split_000.html#page_383){.calibre5}

Localhost, [378](#part0028_split_000.html#page_378){.calibre5}

Loopback address, [378](#part0028_split_000.html#page_378){.calibre5}

MAC address, [383](#part0028_split_000.html#page_383){.calibre5}

[]{#part0042_split_001.html#page_579 .calibre5}NDP,
[383](#part0028_split_000.html#page_383){.calibre5}

Network connection, [385](#part0028_split_000.html#page_385){.calibre5}

Network connection profile,
[385](#part0028_split_000.html#page_385){.calibre5}

Network Interface Card,
[384](#part0028_split_000.html#page_384){.calibre5}

NetworkManager, [389](#part0028_split_000.html#page_389){.calibre5}

Node, [376](#part0028_split_000.html#page_376){.calibre5}

Port, [382](#part0028_split_000.html#page_382){.calibre5}

Protocol, [381](#part0028_split_000.html#page_381){.calibre5}

Subnet mask, [380](#part0028_split_000.html#page_380){.calibre5}

Subnetting, [380](#part0028_split_000.html#page_380){.calibre5}

TCP, [381](#part0028_split_000.html#page_381){.calibre5}

UDP, [381](#part0028_split_000.html#page_381){.calibre5}

NetworkManager service,
[389](#part0028_split_000.html#page_389){.calibre5}

network-scripts directory,
[385](#part0028_split_000.html#page_385){.calibre5}

NFS (See Network File System)

Nftables, [450](#part0032_split_000.html#page_450){.calibre5}

NIC (See Networking)

nice command, [181](#part0020_split_000.html#page_181){.calibre5}

Nice value (See Process)

Niceness (See Process)

nmcli command, [376](#part0028_split_000.html#page_376){.calibre5},
[389](#part0028_split_000.html#page_389){.calibre5}

nm-connection-editor command,
[389](#part0028_split_000.html#page_389){.calibre5}

nmtui command, [389](#part0028_split_000.html#page_389){.calibre5}

Node (See Networking)

nologin.txt file, [132](#part0017_split_000.html#page_132){.calibre5}

nslookup command, [429](#part0030_split_000.html#page_429){.calibre5}

nsswitch.conf file, [427](#part0030_split_000.html#page_427){.calibre5}

NTP (See Network Time Protocol)

**O**

Online help tools, [52](#part0014_split_000.html#page_52){.calibre5}

OpenSSH

Algorithms, [436](#part0031_split_000.html#page_436){.calibre5}

Authentication methods,
[435](#part0031_split_000.html#page_435){.calibre5}

Challenge-Response, [435](#part0031_split_000.html#page_435){.calibre5}

GSSAPI-based, [435](#part0031_split_000.html#page_435){.calibre5}

Host-based, [435](#part0031_split_000.html#page_435){.calibre5}

Password-based, [435](#part0031_split_000.html#page_435){.calibre5}

Public/private key, [435](#part0031_split_000.html#page_435){.calibre5}

Copying files, [442](#part0031_split_000.html#page_442){.calibre5}

Defined, [434](#part0031_split_000.html#page_434){.calibre5}

Encryption techniques,
[434](#part0031_split_000.html#page_434){.calibre5}

Asymmetric, [434](#part0031_split_000.html#page_434){.calibre5}

Symmetric, [434](#part0031_split_000.html#page_434){.calibre5}

Legacy unsecure commands,
[434](#part0031_split_000.html#page_434){.calibre5}

Managing

Client configuration file,
[438](#part0031_split_000.html#page_438){.calibre5}

Server configuration file,
[436](#part0031_split_000.html#page_436){.calibre5}

Packages, [436](#part0031_split_000.html#page_436){.calibre5}

Synchronizing files, [445](#part0031_split_000.html#page_445){.calibre5}

Transferring files, [444](#part0031_split_000.html#page_444){.calibre5}

Version, [436](#part0031_split_000.html#page_436){.calibre5}

**P**

Package

Application Stream, [214](#part0022_split_000.html#page_214){.calibre5}

Binary, [196](#part0021_split_000.html#page_196){.calibre5}

Database, [197](#part0021_split_000.html#page_197){.calibre5}

Dependency, [197](#part0021_split_000.html#page_197){.calibre5}

Groups, [214](#part0022_split_000.html#page_214){.calibre5}

Managing dnf/yum, [216](#part0022_split_000.html#page_216){.calibre5}

Configuration file, [216](#part0022_split_000.html#page_216){.calibre5}

Group

Installing, [230](#part0022_split_000.html#page_230){.calibre5}

Listing, [229](#part0022_split_000.html#page_229){.calibre5}

Removing, [231](#part0022_split_000.html#page_231){.calibre5}

Updating, [230](#part0022_split_000.html#page_230){.calibre5}

Individual package

Displaying, [223](#part0022_split_000.html#page_223){.calibre5}

Installing, [222](#part0022_split_000.html#page_222){.calibre5}

Listing, [220](#part0022_split_000.html#page_220){.calibre5}

Removing, [224](#part0022_split_000.html#page_224){.calibre5}

Searching metadata, [227](#part0022_split_000.html#page_227){.calibre5}

Searching provider, [228](#part0022_split_000.html#page_228){.calibre5}

Updating, [222](#part0022_split_000.html#page_222){.calibre5}

Module

Displaying, [238](#part0022_split_000.html#page_238){.calibre5}

Installing, [237](#part0022_split_000.html#page_237){.calibre5}

Listing, [236](#part0022_split_000.html#page_236){.calibre5}

Removing, [239](#part0022_split_000.html#page_239){.calibre5}

Switching stream, [243](#part0022_split_000.html#page_243){.calibre5}

Updating, [237](#part0022_split_000.html#page_237){.calibre5}

rpm

Extracting, [205](#part0021_split_000.html#page_205){.calibre5}

Freshening, [204](#part0021_split_000.html#page_204){.calibre5}

Installing, [203](#part0021_split_000.html#page_203){.calibre5}

Overwriting, [204](#part0021_split_000.html#page_204){.calibre5}

Querying, [200](#part0021_split_000.html#page_200){.calibre5}

Removing, [204](#part0021_split_000.html#page_204){.calibre5}

Upgrading, [203](#part0021_split_000.html#page_203){.calibre5}

Verifying attributes,
[207](#part0021_split_000.html#page_207){.calibre5}

Verifying signatures,
[205](#part0021_split_000.html#page_205){.calibre5}

Viewing GPG keys, [206](#part0021_split_000.html#page_206){.calibre5}

Metadata, [196](#part0021_split_000.html#page_196){.calibre5}

Module, [214](#part0022_split_000.html#page_214){.calibre5}

[]{#part0042_split_001.html#page_580 .calibre5}Profile,
[215](#part0022_split_000.html#page_215){.calibre5}

Stream, [215](#part0022_split_000.html#page_215){.calibre5}

Naming convention, [197](#part0021_split_000.html#page_197){.calibre5}

Overview, [196](#part0021_split_000.html#page_196){.calibre5}

Repository

AppStream, [215](#part0022_split_000.html#page_215){.calibre5}

BaseOS, [214](#part0022_split_000.html#page_214){.calibre5}

Overview, [215](#part0022_split_000.html#page_215){.calibre5}

Source, [196](#part0021_split_000.html#page_196){.calibre5}

Parent process (See Process)

parted command, [304](#part0025_split_000.html#page_304){.calibre5},
[308](#part0025_split_000.html#page_308){.calibre5}

partitions virtual file,
[308](#part0025_split_000.html#page_308){.calibre5}

passwd command, [138](#part0018_split_000.html#page_138){.calibre5},
[139](#part0018_split_000.html#page_139){.calibre5}

passwd file, [121](#part0017_split_000.html#page_121){.calibre5}

Path, [48](#part0014_split_000.html#page_48){.calibre5}

Pathname, [48](#part0014_split_000.html#page_48){.calibre5}

Pattern matching, [166](#part0019_split_000.html#page_166){.calibre5}

Payload, [450](#part0032_split_000.html#page_450){.calibre5}

pgrep command, [180](#part0020_split_000.html#page_180){.calibre5}

Physical address (See Networking)

pidof command, [180](#part0020_split_000.html#page_180){.calibre5}

pinfo command, [57](#part0014_split_000.html#page_57){.calibre5}

ping command, [382](#part0028_split_000.html#page_382){.calibre5},
[393](#part0028_split_000.html#page_393){.calibre5}

Pipe (See Shell)

pkg-config command, [273](#part0024_split_000.html#page_273){.calibre5}

pkill command, [185](#part0020_split_000.html#page_185){.calibre5}

podman command, [508](#part0035_split_000.html#page_508){.calibre5}

Port (See Networking)

poweroff command, [284](#part0024_split_000.html#page_284){.calibre5}

printenv command, [153](#part0019_split_000.html#page_153){.calibre5}

Process

Background job, [169](#part0019_split_000.html#page_169){.calibre5}

Calling, [176](#part0020_split_000.html#page_176){.calibre5}

Child, [176](#part0020_split_000.html#page_176){.calibre5}

Daemon, [176](#part0020_split_000.html#page_176){.calibre5}

Foreground job, [169](#part0019_split_000.html#page_169){.calibre5}

Job control, [169](#part0019_split_000.html#page_169){.calibre5}

Listing, [180](#part0020_split_000.html#page_180){.calibre5}

Listing by ownership,
[181](#part0020_split_000.html#page_181){.calibre5}

Nice value, [181](#part0020_split_000.html#page_181){.calibre5}

Niceness, [181](#part0020_split_000.html#page_181){.calibre5}

Parent, [176](#part0020_split_000.html#page_176){.calibre5}

Priority, [181](#part0020_split_000.html#page_181){.calibre5}

Process ID, [176](#part0020_split_000.html#page_176){.calibre5}

Signals, [184](#part0020_split_000.html#page_184){.calibre5}

States, [176](#part0020_split_000.html#page_176){.calibre5}

Viewing with ps, [177](#part0020_split_001.html#page_177){.calibre5}

Viewing with top, [179](#part0020_split_000.html#page_179){.calibre5}

Process file system (See File system)

Process ID (See Process)

Process priority (See Process)

Process states (See Process)

profile file, [170](#part0019_split_000.html#page_170){.calibre5}

profile.d directory, [170](#part0019_split_000.html#page_170){.calibre5}

Protocol (See Networking)

protocols file, [381](#part0028_split_000.html#page_381){.calibre5}

ps command, [177](#part0020_split_001.html#page_177){.calibre5}

Pseudo terminal, [50](#part0014_split_000.html#page_50){.calibre5}

pvcreate command, [325](#part0026_split_000.html#page_325){.calibre5}

pvdisplay command, [326](#part0026_split_000.html#page_326){.calibre5}

pvremove command, [325](#part0026_split_000.html#page_325){.calibre5}

pvs command, [323](#part0026_split_000.html#page_323){.calibre5},
[326](#part0026_split_000.html#page_326){.calibre5}

pwd command, [48](#part0014_split_000.html#page_48){.calibre5}

**Q**

Quoting mechanisms, [165](#part0019_split_000.html#page_165){.calibre5}

**R**

Real-time clock (See Hardware clock)

reboot command, [284](#part0024_split_000.html#page_284){.calibre5}

Red Hat Subscription Management,
[217](#part0022_split_000.html#page_217){.calibre5},
[506](#part0035_split_000.html#page_506){.calibre5}

redhat-release file, [156](#part0019_split_000.html#page_156){.calibre5}

Redirection

Defined, [155](#part0019_split_000.html#page_155){.calibre5}

Error, [156](#part0019_split_000.html#page_156){.calibre5}

Input, [156](#part0019_split_000.html#page_156){.calibre5}

Output, [156](#part0019_split_000.html#page_156){.calibre5}

Regex (See Pattern matching)

Regexp (See Pattern matching)

registries.conf file,
[509](#part0035_split_000.html#page_509){.calibre5}

Registry (See Containers),
[505](#part0035_split_001.html#page_505){.calibre5}

Regular expressions (See Pattern matching) Relative path,
[49](#part0014_split_000.html#page_49){.calibre5}

Repository (See Package)

resize2fs command, [350](#part0027_split_000.html#page_350){.calibre5}

resolv.conf file, [386](#part0028_split_000.html#page_386){.calibre5},
[426](#part0030_split_000.html#page_426){.calibre5}

restorecon command, [469](#part0033_split_000.html#page_469){.calibre5}

RHSM (See Red Hat Subscription Management),
[217](#part0022_split_000.html#page_217){.calibre5},
[506](#part0035_split_000.html#page_506){.calibre5}

rm command, [80](#part0015_split_000.html#page_80){.calibre5}

rmdir command, [81](#part0015_split_000.html#page_81){.calibre5}

Root containers (See Containers),
[506](#part0035_split_000.html#page_506){.calibre5}

Rootless containers (See Containers),
[506](#part0035_split_000.html#page_506){.calibre5}

rpm command, [198](#part0021_split_000.html#page_198){.calibre5}

[]{#part0042_split_001.html#page_581 .calibre5}rpm2cpio command,
[205](#part0021_split_000.html#page_205){.calibre5}

*rsync* command, [445](#part0031_split_000.html#page_445){.calibre5}

rsyslog.conf file, [285](#part0024_split_000.html#page_285){.calibre5}

rsyslogd service, [284](#part0024_split_000.html#page_284){.calibre5}

rsyslogd.pid file, [285](#part0024_split_000.html#page_285){.calibre5}

RTC (See Hardware clock)

**S**

scp command, [434](#part0031_split_000.html#page_434){.calibre5},
[436](#part0031_split_000.html#page_436){.calibre5},
[442](#part0031_split_000.html#page_442){.calibre5}

sealert command, [469](#part0033_split_000.html#page_469){.calibre5}

secure log file, [146](#part0018_split_000.html#page_146){.calibre5}

Secure shell (See OpenSSH)

seinfo command, [465](#part0033_split_000.html#page_465){.calibre5},
[469](#part0033_split_000.html#page_469){.calibre5}

SELinux

Activation modes, [469](#part0033_split_000.html#page_469){.calibre5}

Booleans, [467](#part0033_split_000.html#page_467){.calibre5}

Contexts for files, [466](#part0033_split_000.html#page_466){.calibre5}

Contexts for ports, [466](#part0033_split_000.html#page_466){.calibre5}

Contexts for processes,
[465](#part0033_split_000.html#page_465){.calibre5}

Contexts for users, [464](#part0033_split_000.html#page_464){.calibre5}

Defined, [462](#part0033_split_000.html#page_462){.calibre5}

Discretionary Access Control,
[462](#part0033_split_000.html#page_462){.calibre5}

File operations with SELinux context,
[466](#part0033_split_000.html#page_466){.calibre5}

Managing, [468](#part0033_split_000.html#page_468){.calibre5}

Analyzing alerts, [476](#part0033_split_000.html#page_476){.calibre5}

Commands, [468](#part0033_split_000.html#page_468){.calibre5}

Operational state, [469](#part0033_split_000.html#page_469){.calibre5}

Querying, [470](#part0033_split_000.html#page_470){.calibre5}

Mandatory Access Control,
[462](#part0033_split_000.html#page_462){.calibre5}

Terms

Access, [463](#part0033_split_000.html#page_463){.calibre5}

Context, [463](#part0033_split_000.html#page_463){.calibre5}

Domain, [464](#part0033_split_000.html#page_464){.calibre5}

Domain transitioning,
[467](#part0033_split_000.html#page_467){.calibre5}

Labeling, [463](#part0033_split_000.html#page_463){.calibre5}

Level, [464](#part0033_split_000.html#page_464){.calibre5}

Object, [463](#part0033_split_000.html#page_463){.calibre5}

Policy, [463](#part0033_split_000.html#page_463){.calibre5}

Role, [464](#part0033_split_000.html#page_464){.calibre5}

SELinux user, [463](#part0033_split_000.html#page_463){.calibre5}

Subject, [463](#part0033_split_000.html#page_463){.calibre5}

Type, [464](#part0033_split_000.html#page_464){.calibre5}

Type enforcement, [464](#part0033_split_000.html#page_464){.calibre5}

semanage command, [465](#part0033_split_000.html#page_465){.calibre5},
[466](#part0033_split_000.html#page_466){.calibre5},
[469](#part0033_split_000.html#page_469){.calibre5}

sesearch command, [469](#part0033_split_000.html#page_469){.calibre5}

sestatus command, [469](#part0033_split_000.html#page_469){.calibre5},
[470](#part0033_split_000.html#page_470){.calibre5}

sestatus.conf file, [471](#part0033_split_000.html#page_471){.calibre5}

setenforce command, [469](#part0033_split_000.html#page_469){.calibre5}

setfacl command, [105](#part0016_split_000.html#page_105){.calibre5}

Setgid bit, [96](#part0016_split_000.html#page_96){.calibre5},
[98](#part0016_split_000.html#page_98){.calibre5}

setroubleshootd service,
[476](#part0033_split_000.html#page_476){.calibre5}

setsebool command, [469](#part0033_split_000.html#page_469){.calibre5}

Setuid bit, [95](#part0016_split_000.html#page_95){.calibre5}

sftp command, [434](#part0031_split_000.html#page_434){.calibre5},
[436](#part0031_split_000.html#page_436){.calibre5},
[444](#part0031_split_000.html#page_444){.calibre5}

shadow file, [122](#part0017_split_000.html#page_122){.calibre5}

Shadow mechanism, [122](#part0017_split_000.html#page_122){.calibre5}

Shell

Alias substitution, [160](#part0019_split_000.html#page_160){.calibre5}

Bash, [152](#part0019_split_000.html#page_152){.calibre5}

Child shell, [152](#part0019_split_000.html#page_152){.calibre5}

Command history, [157](#part0019_split_000.html#page_157){.calibre5}

Command line completion,
[159](#part0019_split_000.html#page_159){.calibre5}

Command line editing,
[158](#part0019_split_000.html#page_158){.calibre5}

Command substitution,
[155](#part0019_split_000.html#page_155){.calibre5}

Current shell, [152](#part0019_split_000.html#page_152){.calibre5}

Features, [152](#part0019_split_000.html#page_152){.calibre5}

History expansion, [157](#part0019_split_000.html#page_157){.calibre5}

History substitution,
[157](#part0019_split_000.html#page_157){.calibre5}

Metacharacters, [162](#part0019_split_000.html#page_162){.calibre5}

Modifying command prompt,
[154](#part0019_split_000.html#page_154){.calibre5}

Overview, [152](#part0019_split_000.html#page_152){.calibre5}

Pipe, [164](#part0019_split_000.html#page_164){.calibre5}

Quoting mechanisms, [165](#part0019_split_000.html#page_165){.calibre5}

Tab completion, [159](#part0019_split_000.html#page_159){.calibre5}

Tilde substitution, [159](#part0019_split_000.html#page_159){.calibre5}

Variable substitution,
[155](#part0019_split_000.html#page_155){.calibre5}

Variables

Displaying, [153](#part0019_split_000.html#page_153){.calibre5}

Environment, [153](#part0019_split_000.html#page_153){.calibre5}

Local, [152](#part0019_split_000.html#page_152){.calibre5}

Setting, [153](#part0019_split_000.html#page_153){.calibre5}

Shell, [152](#part0019_split_000.html#page_152){.calibre5}

Unsetting, [153](#part0019_split_000.html#page_153){.calibre5}

Wildcards

Asterisk, [162](#part0019_split_000.html#page_162){.calibre5}

Exclamation point, [163](#part0019_split_000.html#page_163){.calibre5}

Question mark, [163](#part0019_split_000.html#page_163){.calibre5}

Square brackets, [163](#part0019_split_000.html#page_163){.calibre5}

Shell scripting

Command line arguments,
[486](#part0034_split_000.html#page_486){.calibre5}

Debugging, [484](#part0034_split_000.html#page_484){.calibre5}

Defined, [482](#part0034_split_000.html#page_482){.calibre5}

Displaying system info,
[482](#part0034_split_000.html#page_482){.calibre5}

Executing, [483](#part0034_split_000.html#page_483){.calibre5}

[]{#part0042_split_001.html#page_582 .calibre5}Exit codes,
[488](#part0034_split_000.html#page_488){.calibre5}

Logical statements, [488](#part0034_split_000.html#page_488){.calibre5}

Looping construct, [495](#part0034_split_000.html#page_495){.calibre5}

Parsing command output,
[486](#part0034_split_000.html#page_486){.calibre5}

Positional parameters,
[486](#part0034_split_000.html#page_486){.calibre5}

Shell parameter, [486](#part0034_split_000.html#page_486){.calibre5}

Shifting command line arguments,
[487](#part0034_split_001.html#page_487){.calibre5}

Special parameter, [486](#part0034_split_000.html#page_486){.calibre5}

Test conditions, [489](#part0034_split_000.html#page_489){.calibre5},
[496](#part0034_split_000.html#page_496){.calibre5}

Using environment variables,
[485](#part0034_split_000.html#page_485){.calibre5}

Using for-do-done, [496](#part0034_split_000.html#page_496){.calibre5},
[497](#part0034_split_000.html#page_497){.calibre5}

Using if-then-elif-fi,
[493](#part0034_split_000.html#page_493){.calibre5}

Using if-then-else-fi,
[491](#part0034_split_000.html#page_491){.calibre5}

Using if-then-fi, [490](#part0034_split_000.html#page_490){.calibre5}

Using local variables,
[485](#part0034_split_000.html#page_485){.calibre5}

shift command, [487](#part0034_split_001.html#page_487){.calibre5}

shutdown command, [284](#part0024_split_000.html#page_284){.calibre5}

Signal (See Process)

Socket, [272](#part0024_split_000.html#page_272){.calibre5}

ssh command, [434](#part0031_split_000.html#page_434){.calibre5},
[436](#part0031_split_000.html#page_436){.calibre5},
[442](#part0031_split_000.html#page_442){.calibre5}

ssh_config file, [438](#part0031_split_000.html#page_438){.calibre5}

ssh-copy-id command, [436](#part0031_split_000.html#page_436){.calibre5}

sshd service, [436](#part0031_split_000.html#page_436){.calibre5}

sshd_config file, [436](#part0031_split_000.html#page_436){.calibre5}

ssh-keygen command, [436](#part0031_split_000.html#page_436){.calibre5}

stat command, [62](#part0015_split_000.html#page_62){.calibre5}

Sticky bit, [100](#part0016_split_000.html#page_100){.calibre5}

Storage

Benefits, [322](#part0026_split_000.html#page_322){.calibre5}

LVM, [322](#part0026_split_000.html#page_322){.calibre5}

Concept, [322](#part0026_split_000.html#page_322){.calibre5}

Logical extent, [324](#part0026_split_000.html#page_324){.calibre5}

Logical volume, [324](#part0026_split_000.html#page_324){.calibre5}

Managing

Commands, [325](#part0026_split_000.html#page_325){.calibre5}

Physical extent, [324](#part0026_split_000.html#page_324){.calibre5}

Physical volume, [323](#part0026_split_000.html#page_323){.calibre5}

Volume group, [323](#part0026_split_000.html#page_323){.calibre5}

Managing

Tools, [304](#part0025_split_000.html#page_304){.calibre5}

Using gdisk, [310](#part0025_split_000.html#page_310){.calibre5}

Using parted, [308](#part0025_split_000.html#page_308){.calibre5}

Partition table

GPT, [303](#part0025_split_000.html#page_303){.calibre5}

MBR, [302](#part0025_split_000.html#page_302){.calibre5}

UEFI, [302](#part0025_split_000.html#page_302){.calibre5}

Stratis

Defined, [335](#part0026_split_000.html#page_335){.calibre5}

Dynamic expansion, [336](#part0026_split_000.html#page_336){.calibre5}

Managing, [337](#part0026_split_000.html#page_337){.calibre5}

Pool, [336](#part0026_split_000.html#page_336){.calibre5}

Thin provisioning

Defined, [304](#part0025_split_000.html#page_304){.calibre5}

pool, [304](#part0025_split_000.html#page_304){.calibre5}

[]{#part0042_split_002.html}

## VDO {#part0042_split_002.html#calibre_pb_1 .calibre11}

Compression, [314](#part0025_split_000.html#page_314){.calibre5}

De-duplication, [314](#part0025_split_000.html#page_314){.calibre5}

Defined, [314](#part0025_split_000.html#page_314){.calibre5}

How it works, [314](#part0025_split_000.html#page_314){.calibre5}

Managing, [315](#part0025_split_000.html#page_315){.calibre5}

Zero-block elimination,
[314](#part0025_split_000.html#page_314){.calibre5}

Stratis (See Storage)

stratis command, [337](#part0026_split_000.html#page_337){.calibre5}

stratisd service, [337](#part0026_split_000.html#page_337){.calibre5}

Stream (See Package)

su command, [143](#part0018_split_000.html#page_143){.calibre5}

Subnet mask (See Networking)

Subnetting (See Networking)

Substituting users, [143](#part0018_split_000.html#page_143){.calibre5}

sudo command, [144](#part0018_split_000.html#page_144){.calibre5}

sudoers file, [144](#part0018_split_000.html#page_144){.calibre5}

Swap

Commands, [368](#part0027_split_000.html#page_368){.calibre5}

Defined, [366](#part0027_split_000.html#page_366){.calibre5}

Demand paging, [367](#part0027_split_000.html#page_367){.calibre5}

Determing usage, [367](#part0027_split_000.html#page_367){.calibre5}

Prioritizing, [368](#part0027_split_000.html#page_368){.calibre5}

Thrashing, [367](#part0027_split_000.html#page_367){.calibre5}

swapoff command, [368](#part0027_split_000.html#page_368){.calibre5}

swapon command, [368](#part0027_split_000.html#page_368){.calibre5}

System logging

Configuration file, [285](#part0024_split_000.html#page_285){.calibre5}

Journal, [290](#part0024_split_000.html#page_290){.calibre5}

journald

Configuration file, [291](#part0024_split_000.html#page_291){.calibre5}

Preserving, [293](#part0024_split_000.html#page_293){.calibre5}

Viewing, [291](#part0024_split_000.html#page_291){.calibre5}

Logging, [289](#part0024_split_000.html#page_289){.calibre5}

Logging custom messages,
[290](#part0024_split_000.html#page_290){.calibre5}

Rotating log files, [286](#part0024_split_000.html#page_286){.calibre5}

rsyslogd, [284](#part0024_split_000.html#page_284){.calibre5}

systemd-journald daemon,
[291](#part0024_split_000.html#page_291){.calibre5}

System tuning (See Tuning)

systemctl command, [276](#part0024_split_000.html#page_276){.calibre5}

[]{#part0042_split_002.html#page_583 .calibre5}systemd (See
Initialization) systemd-hostnamed service,
[377](#part0028_split_000.html#page_377){.calibre5}

systemd-journald service,
[290](#part0024_split_000.html#page_290){.calibre5}

**T**

tac command, [76](#part0015_split_000.html#page_76){.calibre5}

tail command, [77](#part0015_split_000.html#page_77){.calibre5}

Tape archive (See Archiving)

tar command, [66](#part0015_split_000.html#page_66){.calibre5}

TCP (See Networking)

Thin provisioning (See Storage)

Tilde expansion (See Shell)

Tilde substitution (See Shell)

Time synchronization (See Network Time Protocol) timedatectl command,
[423](#part0030_split_000.html#page_423){.calibre5}

top command, [179](#part0020_split_000.html#page_179){.calibre5}

touch command, [74](#part0015_split_000.html#page_74){.calibre5}

traceroute command, [383](#part0028_split_000.html#page_383){.calibre5}

Transmission Control Protocol (See Networking) tree command,
[43](#part0014_split_000.html#page_43){.calibre5}

tty command, [50](#part0014_split_000.html#page_50){.calibre5}

tune2fs command, [350](#part0027_split_000.html#page_350){.calibre5}

tuned service, [294](#part0024_split_000.html#page_294){.calibre5}

tuned-adm command, [296](#part0024_split_000.html#page_296){.calibre5}

Tuning, [294](#part0024_split_000.html#page_294){.calibre5}

Daemon, [294](#part0024_split_000.html#page_294){.calibre5}

Defined, [294](#part0024_split_000.html#page_294){.calibre5}

Managing, [296](#part0024_split_000.html#page_296){.calibre5}

Profile location, [295](#part0024_split_000.html#page_295){.calibre5}

Profiles, [295](#part0024_split_000.html#page_295){.calibre5}

type command, [51](#part0014_split_000.html#page_51){.calibre5}

**U**

udevd service, [42](#part0014_split_000.html#page_42){.calibre5},
[385](#part0028_split_000.html#page_385){.calibre5}

UDP (See Networking)

UDS (See Universal De-duplication Service) UDS module,
[314](#part0025_split_000.html#page_314){.calibre5}

umask command, [93](#part0016_split_000.html#page_93){.calibre5}

umount command, [350](#part0027_split_000.html#page_350){.calibre5},
[351](#part0027_split_000.html#page_351){.calibre5}

unalias command, [162](#part0019_split_000.html#page_162){.calibre5}

uname command, [51](#part0014_split_000.html#page_51){.calibre5},
[259](#part0023_split_000.html#page_259){.calibre5}

Universal De-duplication Service,
[314](#part0025_split_000.html#page_314){.calibre5}

unset command, [153](#part0019_split_000.html#page_153){.calibre5}

UPG (See User Private Group)

uptime command, [50](#part0014_split_000.html#page_50){.calibre5}

User

Authentication file

group, [124](#part0017_split_000.html#page_124){.calibre5}

gshadow, [125](#part0017_split_000.html#page_125){.calibre5}

passwd, [121](#part0017_split_000.html#page_121){.calibre5}

shadow, [122](#part0017_split_000.html#page_122){.calibre5}

Configuring password aging,
[136](#part0018_split_000.html#page_136){.calibre5}

Creating, [128](#part0017_split_000.html#page_128){.calibre5}

Deleting, [129](#part0017_split_000.html#page_129){.calibre5}

Doing as superuser, [144](#part0018_split_000.html#page_144){.calibre5}

Identifying, [120](#part0017_split_000.html#page_120){.calibre5}

Initialization file

Peruser, [171](#part0019_split_000.html#page_171){.calibre5}

Sourcing sequence, [172](#part0019_split_000.html#page_172){.calibre5}

System-wide, [170](#part0019_split_000.html#page_170){.calibre5}

Locking and unlocking,
[139](#part0018_split_000.html#page_139){.calibre5}

[login.defs](http://login.defs){.calibre5} file,
[127](#part0017_split_000.html#page_127){.calibre5}

Managing

Listing logged-in users,
[116](#part0017_split_000.html#page_116){.calibre5}

Listing previous failed logins,
[118](#part0017_split_000.html#page_118){.calibre5}

Listing previous successful logins,
[117](#part0017_split_000.html#page_117){.calibre5}

Listing recent logins,
[119](#part0017_split_000.html#page_119){.calibre5}

Modifying, [129](#part0017_split_000.html#page_129){.calibre5}

Nologin account, [132](#part0017_split_000.html#page_132){.calibre5}

Owning user, [146](#part0018_split_000.html#page_146){.calibre5}

Password aging, [136](#part0018_split_000.html#page_136){.calibre5}

Skeleton directory, [129](#part0017_split_000.html#page_129){.calibre5}

Substituting, [143](#part0018_split_000.html#page_143){.calibre5}

Type

Normal, [120](#part0017_split_000.html#page_120){.calibre5}

Service, [120](#part0017_split_000.html#page_120){.calibre5}

Superuser, [120](#part0017_split_000.html#page_120){.calibre5}

User Private Group, [124](#part0017_split_000.html#page_124){.calibre5}

useradd file, [127](#part0017_split_000.html#page_127){.calibre5}

User Datagram Protocol (See Networking)

User Private Group, [124](#part0017_split_000.html#page_124){.calibre5}

useradd command, [128](#part0017_split_000.html#page_128){.calibre5}

useradd file, [127](#part0017_split_000.html#page_127){.calibre5}

usermod command, [129](#part0017_split_000.html#page_129){.calibre5}

UUID (See File system)

**V**

vdo command, [315](#part0025_split_000.html#page_315){.calibre5}

vdostats command, [315](#part0025_split_000.html#page_315){.calibre5}

vgcreate command, [325](#part0026_split_000.html#page_325){.calibre5}

vgdisplay command, [324](#part0026_split_000.html#page_324){.calibre5}

vgextend command, [325](#part0026_split_000.html#page_325){.calibre5}

vgreduce command, [325](#part0026_split_000.html#page_325){.calibre5}

[]{#part0042_split_002.html#page_584 .calibre5}vgremove command,
[325](#part0026_split_000.html#page_325){.calibre5}

vgrename command, [325](#part0026_split_000.html#page_325){.calibre5}

vgs command, [323](#part0026_split_000.html#page_323){.calibre5},
[326](#part0026_split_000.html#page_326){.calibre5}

vi editor (See vim editor)

vim editor, [69](#part0015_split_000.html#page_69){.calibre5}

Changing, [72](#part0015_split_000.html#page_72){.calibre5}

Copying, [72](#part0015_split_000.html#page_72){.calibre5}

Deleting, [71](#part0015_split_000.html#page_71){.calibre5}

Inserting, [70](#part0015_split_000.html#page_70){.calibre5}

Modes, [69](#part0015_split_000.html#page_69){.calibre5}

Command mode, [69](#part0015_split_000.html#page_69){.calibre5}

Extended mode, [69](#part0015_split_000.html#page_69){.calibre5}

Input mode, [69](#part0015_split_000.html#page_69){.calibre5}

Last line mode, [69](#part0015_split_000.html#page_69){.calibre5}

Moving, [72](#part0015_split_000.html#page_72){.calibre5}

Navigating, [70](#part0015_split_000.html#page_70){.calibre5}

Pasting, [72](#part0015_split_000.html#page_72){.calibre5}

Replacing, [72](#part0015_split_000.html#page_72){.calibre5}

Saving and quitting, [73](#part0015_split_000.html#page_73){.calibre5}

Searching, [71](#part0015_split_000.html#page_71){.calibre5}

Starting, [69](#part0015_split_000.html#page_69){.calibre5}

Undoing and repeating, [71](#part0015_split_000.html#page_71){.calibre5}

VirtualBox

Changing boot order, [27](#part0013_split_000.html#page_27){.calibre5}

Creating virtual machine,
[10](#part0013_split_000.html#page_10){.calibre5}

Downloading, [6](#part0013_split_000.html#page_6){.calibre5}

Volume-Managing file system,
[336](#part0026_split_000.html#page_336){.calibre5}

**W**

w command, [117](#part0017_split_000.html#page_117){.calibre5}

Wayland, [36](#part0014_split_000.html#page_36){.calibre5}

Desktop environment, [37](#part0014_split_001.html#page_37){.calibre5}

Desktop manager, [36](#part0014_split_000.html#page_36){.calibre5}

Display/Login manager, [36](#part0014_split_000.html#page_36){.calibre5}

wc command, [78](#part0015_split_000.html#page_78){.calibre5}

whatis command, [56](#part0014_split_000.html#page_56){.calibre5}

whereis command, [51](#part0014_split_000.html#page_51){.calibre5}

which command, [51](#part0014_split_000.html#page_51){.calibre5}

who command, [116](#part0017_split_000.html#page_116){.calibre5}

Wildcard characters (See Wildcards under Shell) wtmp file,
[118](#part0017_split_000.html#page_118){.calibre5}

**X**

xfs_admin command, [350](#part0027_split_000.html#page_350){.calibre5}

xfs_growfs command, [350](#part0027_split_000.html#page_350){.calibre5}

xfs_info command, [350](#part0027_split_000.html#page_350){.calibre5}

xfs_repair command, [348](#part0027_split_000.html#page_348){.calibre5}

**Z**

Zero-block elimination,
[314](#part0025_split_000.html#page_314){.calibre5}
```
